-- Logs begin at Fri 2023-01-20 12:41:02 EST, end at Fri 2023-01-20 12:49:02 EST. --
Jan 20 12:41:02 zcy-Z390-AORUS-MASTER systemd-journald[328]: System Journal (/var/log/journal/0124d7fcec8e4fe2b725ac76f1bce4df) is 1.5G, max 4.0G, 2.4G free.
Jan 20 12:41:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:02.483390  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:02.483461  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:02.491406  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[f47a25d3-21f9-42a3-be2d-b8930f83eff9] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:02 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0020fdda0 2 [] false false map[] 0xc002141a00 0xc0016ead10}
Jan 20 12:41:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:02.491437  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:02.689454  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:41:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:02.689514  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:02.690604  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:02 GMT]] 0xc0017028a0 2 [] true false map[] 0xc002141d00 <nil>}
Jan 20 12:41:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:02.690715  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:41:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:02.695891  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:41:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:02.695951  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:02.696916  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:02 GMT]] 0xc0020fde20 2 [] true false map[] 0xc001a4bf00 <nil>}
Jan 20 12:41:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:02.697013  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:41:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:02.710224  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:41:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:02.710276  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:41:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:02.710283  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:02.710331  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:02.711368  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:02 GMT]] 0xc0008e3ce0 2 [] true false map[] 0xc0002c2d00 <nil>}
Jan 20 12:41:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:02.711473  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:41:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:02.711454  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:02 GMT]] 0xc0017029c0 2 [] true false map[] 0xc0016aa100 <nil>}
Jan 20 12:41:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:02.711560  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:41:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:02.787178  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:41:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:03.483491  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:03.483563  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:03.491325  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[cc738f57-c20d-4fba-8048-dd85ababe235] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:03 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008dc8a0 2 [] false false map[] 0xc0016aa400 0xc0014b7b80}
Jan 20 12:41:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:03.491357  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:03.581506  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:41:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:03.587089  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:41:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:03.587100  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:41:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:03.587943  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:41:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:03.588391  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:41:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:03.588432  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:41:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:03.588438  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:41:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:03.588446  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:41:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:04.484153  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:04.484224  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:04.492461  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[e0cd2e27-5095-4e2f-871d-2775da4c3a12] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:04 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0017183a0 2 [] false false map[] 0xc0016ab100 0xc0014b7d90}
Jan 20 12:41:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:04.492491  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:05.483890  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:05.483964  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:05.492334  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[d6ade832-726c-467e-9d60-3fc033575a65] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:05 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0002bed00 2 [] false false map[] 0xc000339500 0xc001da1d90}
Jan 20 12:41:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:05.492365  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:05.582237  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:41:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:05.588081  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:41:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:05.588092  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:41:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:05.588753  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:41:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:05.589158  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:41:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:05.589203  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:41:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:05.589210  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:41:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:05.589218  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:41:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:06.164440  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:41:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:06.164515  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:06.175518  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:06 GMT] X-Content-Type-Options:[nosniff]] 0xc001719140 2 [] false false map[] 0xc0016ab600 0xc001fa6790}
Jan 20 12:41:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:06.175648  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:41:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:06.271627  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:41:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:06.271691  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:06.272949  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:06 GMT]] 0xc000707d80 29 [] true false map[] 0xc0016abb00 <nil>}
Jan 20 12:41:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:06.273050  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:41:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:06.484076  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:06.484135  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:06.498091  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[c41cbe22-84ca-4842-a14e-e1deebe995a8] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:06 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008dd360 2 [] false false map[] 0xc000cf6300 0xc002008840}
Jan 20 12:41:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:06.498226  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:06.895475  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:41:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:06.895539  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:06.902771  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:06 GMT] X-Content-Type-Options:[nosniff]] 0xc00129c000 2 [] false false map[] 0xc001847f00 0xc001ecd970}
Jan 20 12:41:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:06.902800  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:41:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:07.028161  199956 handler.go:293] error while reading "/proc/199956/fd/34" link: readlink /proc/199956/fd/34: no such file or directory
Jan 20 12:41:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:07.483214  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:07.483276  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:07.491200  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[e6b1c947-c1e8-4644-b378-6b5cc86951be] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:07 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001718000 2 [] false false map[] 0xc000fcc100 0xc001983d90}
Jan 20 12:41:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:07.491231  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:07.525607  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/etcd.yaml"
Jan 20 12:41:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:07.526063  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver.yaml"
Jan 20 12:41:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:07.526624  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Jan 20 12:41:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:07.527130  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-scheduler.yaml"
Jan 20 12:41:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:07.527447  199956 config.go:279] "Setting pods for source" source="file"
Jan 20 12:41:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:07.582031  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:41:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:07.583467  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:41:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:07.583479  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:41:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:07.583485  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:41:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:07.584305  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:41:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:07.584350  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:41:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:07.584356  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:41:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:07.584365  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:41:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:07.788670  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.483197  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.483239  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.489489  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[2b5c11fa-f4e6-4dbb-bab9-59df8da8d416] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:08 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e85c0 2 [] false false map[] 0xc001f36100 0xc001983ef0}
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.489558  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.647248  199956 config.go:279] "Setting pods for source" source="api"
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.647604  199956 config.go:384] "Receiving a new pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.647665  199956 kubelet.go:2060] "SyncLoop ADD" source="api" pods=[default/unsigned-unencrypted-cc-1]
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.647684  199956 topology_manager.go:200] "Topology Admit Handler"
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.647694  199956 manager.go:914] "Looking for needed resources" needed=1 resourceName="cpu"
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.647699  199956 manager.go:914] "Looking for needed resources" needed=2147483648 resourceName="memory"
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.647768  199956 pod_workers.go:571] "Pod is being synced for the first time" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.647794  199956 pod_workers.go:888] "Processing pod event" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.647808  199956 kubelet.go:1501] "syncPod enter" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.647819  199956 kubelet_pods.go:1435] "Generating pod status" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.647840  199956 kubelet_pods.go:1447] "Got phase for pod" pod="default/unsigned-unencrypted-cc-1" oldPhase=Pending phase=Pending
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.647918  199956 reflector.go:219] Starting reflector *v1.ConfigMap (0s) from object-"default"/"kube-root-ca.crt"
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.647932  199956 reflector.go:255] Listing and watching *v1.ConfigMap from object-"default"/"kube-root-ca.crt"
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.648348  199956 qos_container_manager_linux.go:379] "Updated QoS cgroup configuration"
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.652956  199956 config.go:279] "Setting pods for source" source="api"
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.653335  199956 kubelet.go:2073] "SyncLoop RECONCILE" source="api" pods=[default/unsigned-unencrypted-cc-1]
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.653451  199956 status_manager.go:685] "Patch status for pod" pod="default/unsigned-unencrypted-cc-1" patch="{\"metadata\":{\"uid\":\"4f97314d-815b-4787-b174-5c158cd28c9d\"},\"status\":{\"$setElementOrder/conditions\":[{\"type\":\"Initialized\"},{\"type\":\"Ready\"},{\"type\":\"ContainersReady\"},{\"type\":\"PodScheduled\"}],\"conditions\":[{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-01-20T17:41:08Z\",\"status\":\"True\",\"type\":\"Initialized\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-01-20T17:41:08Z\",\"message\":\"containers with unready status: [ci-example256m]\",\"reason\":\"ContainersNotReady\",\"status\":\"False\",\"type\":\"Ready\"},{\"lastProbeTime\":null,\"lastTransitionTime\":\"2023-01-20T17:41:08Z\",\"message\":\"containers with unready status: [ci-example256m]\",\"reason\":\"ContainersNotReady\",\"status\":\"False\",\"type\":\"ContainersReady\"}],\"containerStatuses\":[{\"image\":\"ngcn-registry.sh.intel.com:443/ci-example256m:latest\",\"imageID\":\"\",\"lastState\":{},\"name\":\"ci-example256m\",\"ready\":false,\"restartCount\":0,\"started\":false,\"state\":{\"waiting\":{\"reason\":\"ContainerCreating\"}}}],\"hostIP\":\"10.239.159.53\",\"startTime\":\"2023-01-20T17:41:08Z\"}}"
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.653491  199956 status_manager.go:694] "Status for pod updated successfully" pod="default/unsigned-unencrypted-cc-1" statusVersion=1 status={Phase:Pending Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP: PodIPs:[] StartTime:2023-01-20 12:41:08 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:ci-example256m State:{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest ImageID: ContainerID: Started:0xc00114a9a9}] QOSClass:Guaranteed EphemeralContainerStatuses:[]}
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.669408  199956 factory.go:262] Factory "containerd" was unable to handle container "/kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d"
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.669453  199956 factory.go:262] Factory "systemd" was unable to handle container "/kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d"
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.669475  199956 factory.go:258] Using factory "raw" for container "/kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d"
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.670551  199956 manager.go:988] Added container: "/kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d" (aliases: [], namespace: "")
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.671182  199956 handler.go:325] Added event &{/kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d 2023-01-20 12:41:08.665876487 -0500 EST containerCreation {<nil>}}
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.671262  199956 container.go:530] Start housekeeping for container "/kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d"
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.671313  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.685154  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="default/unsigned-unencrypted-cc-1" volumeName="kube-api-access-qcb8g" volumeSpecName="kube-api-access-qcb8g"
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.699346  199956 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") " pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.800151  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") " pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.800258  199956 reconciler.go:245] "operationExecutor.MountVolume started for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") " pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.800393  199956 projected.go:183] Setting up volume kube-api-access-qcb8g for pod 4f97314d-815b-4787-b174-5c158cd28c9d at /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.815334  199956 empty_dir_linux.go:99] Statfs_t of /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g: {Type:61267 Bsize:4096 Blocks:245327514 Bfree:237983561 Bavail:225503190 Files:62382080 Ffree:61945639 Fsid:{Val:[-1860784916 147171724]} Namelen:255 Frsize:4096 Flags:4128 Spare:[0 0 0 0]}
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.815400  199956 empty_dir.go:334] pod 4f97314d-815b-4787-b174-5c158cd28c9d: mounting tmpfs for volume wrapped_kube-api-access-qcb8g
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.815432  199956 mount_linux.go:183] Mounting cmd (mount) with arguments (-t tmpfs -o size=2147483648 tmpfs /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g)
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.820541  199956 atomic_writer.go:181] pod default/unsigned-unencrypted-cc-1 volume kube-api-access-qcb8g: performed write of new data to ts data directory: /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g/..2023_01_20_17_41_08.3469924095
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.820796  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") " pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.879290  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.879356  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.880528  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:08 GMT] X-Content-Type-Options:[nosniff]] 0xc0009a0960 2 [] true false map[] 0xc000fcdc00 <nil>}
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.880634  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.972419  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.972466  199956 kuberuntime_manager.go:469] "No sandbox for pod can be found. Need to start a new one" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.972517  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:true CreateSandbox:true SandboxID: Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.972552  199956 kuberuntime_manager.go:723] "SyncPod received new pod, will create a sandbox for it" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.972576  199956 kuberuntime_manager.go:730] "Stopping PodSandbox for pod, will start new one" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.972617  199956 kuberuntime_manager.go:785] "Creating PodSandbox for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:08.972950  199956 kuberuntime_sandbox.go:63] "Running pod with runtime handler" pod="default/unsigned-unencrypted-cc-1" runtimeHandler="kata-qemu"
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:41:08.973636292-05:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:unsigned-unencrypted-cc-1,Uid:4f97314d-815b-4787-b174-5c158cd28c9d,Namespace:default,Attempt:0,}"
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER NetworkManager[905]: <info>  [1674236468.9824] manager: (vethfbf8f23a): new Veth device (/org/freedesktop/NetworkManager/Devices/122)
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER systemd-udevd[206885]: ethtool: autonegotiation is unset or enabled, the speed and duplex are not writable.
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER systemd-udevd[206885]: Using default interface naming scheme 'v245'.
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER systemd-udevd[206885]: vethfbf8f23a: Could not generate persistent MAC: No data available
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER NetworkManager[905]: <info>  [1674236468.9844] device (vethfbf8f23a): carrier: link connected
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kernel: cni0: port 6(vethfbf8f23a) entered blocking state
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kernel: cni0: port 6(vethfbf8f23a) entered disabled state
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kernel: device vethfbf8f23a entered promiscuous mode
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kernel: IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kernel: IPv6: ADDRCONF(NETDEV_CHANGE): vethfbf8f23a: link becomes ready
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kernel: cni0: port 6(vethfbf8f23a) entered blocking state
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER kernel: cni0: port 6(vethfbf8f23a) entered forwarding state
Jan 20 12:41:08 zcy-Z390-AORUS-MASTER containerd[201983]: map[string]interface {}{"cniVersion":"0.3.1", "hairpinMode":true, "ipMasq":false, "ipam":map[string]interface {}{"ranges":[][]map[string]interface {}{[]map[string]interface {}{map[string]interface {}{"subnet":"10.244.0.0/24"}}}, "routes":[]types.Route{types.Route{Dst:net.IPNet{IP:net.IP{0xa, 0xf4, 0x0, 0x0}, Mask:net.IPMask{0xff, 0xff, 0x0, 0x0}}, GW:net.IP(nil)}}, "type":"host-local"}, "isDefaultGateway":true, "isGateway":true, "mtu":(*uint)(0xc000186618), "name":"cbr0", "type":"bridge"}
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:09.018817  199956 factory.go:258] Using factory "containerd" for container "/kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b"
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER kernel: eth0: Caught tx_queue_len zero misconfig
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:09.133605  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER virtiofsd[206930]: zcy-Z390-AORUS-MASTER virtiofsd[206930]: Use of deprecated flag '-f': This flag has no effect, please remove it
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER virtiofsd[206930]: zcy-Z390-AORUS-MASTER virtiofsd[206930]: Use of deprecated option format '-o': Please specify options without it (e.g., '--cache auto' instead of '-o cache=auto')
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER virtiofsd[206933]: zcy-Z390-AORUS-MASTER virtiofsd[206930]: Waiting for vhost-user socket connection...
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER virtiofsd[206933]: zcy-Z390-AORUS-MASTER virtiofsd[206930]: Client connected, servicing requests
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:09.483690  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:09.483709  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:09.486716  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[5d40d193-8e62-45c7-9688-67ab72a3f720] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:09 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123eca0 2 [] false false map[] 0xc00105b700 0xc00151bad0}
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:09.486747  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:09.581387  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:09.582893  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:09.582909  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:09.583398  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:09.583753  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:09.583803  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:09.583811  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:09.583821  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:09.741469  199956 manager.go:988] Added container: "/kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b" (aliases: [e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b /kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b], namespace: "containerd")
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:09.741560  199956 handler.go:325] Added event &{/kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b 2023-01-20 12:41:09.013876492 -0500 EST containerCreation {<nil>}}
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:09.741587  199956 container.go:530] Start housekeeping for container "/kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b"
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:09.741830  199956 factory.go:258] Using factory "containerd" for container "/kata_overhead/e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b"
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER containerd[201983]: {"cniVersion":"0.3.1","hairpinMode":true,"ipMasq":false,"ipam":{"ranges":[[{"subnet":"10.244.0.0/24"}]],"routes":[{"dst":"10.244.0.0/16"}],"type":"host-local"},"isDefaultGateway":true,"isGateway":true,"mtu":1450,"name":"cbr0","type":"bridge"}time="2023-01-20T12:41:09.742635636-05:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:unsigned-unencrypted-cc-1,Uid:4f97314d-815b-4787-b174-5c158cd28c9d,Namespace:default,Attempt:0,} returns sandbox id \"e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b\""
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:09.742751  199956 kuberuntime_manager.go:823] "Created PodSandbox for pod" podSandboxID="e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:09.742765  199956 manager.go:988] Added container: "/kata_overhead/e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b" (aliases: [e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b /kata_overhead/e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b], namespace: "containerd")
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:09.742852  199956 handler.go:325] Added event &{/kata_overhead/e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b 2023-01-20 12:41:09.033876492 -0500 EST containerCreation {<nil>}}
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:09.742870  199956 container.go:530] Start housekeeping for container "/kata_overhead/e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b"
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:09.742923  199956 kuberuntime_manager.go:846] "Determined the ip for pod after sandbox changed" IPs=[10.244.0.12] pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:09.742997  199956 kuberuntime_manager.go:889] "Creating container in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:09.743179  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Normal" reason="Pulling" message="Pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\""
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:09.743184  199956 kuberuntime_image.go:47] "Pulling image without credentials" image="ngcn-registry.sh.intel.com:443/ci-example256m:latest"
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:41:09.743288448-05:00" level=info msg="PullImage \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\""
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:41:09.743307703-05:00" level=info msg="TaskManager get ImageService succeed." id=e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:09.745424  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:09.755538  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59505228Ki" capacity="65586124Ki" time="2023-01-20 12:41:09.74212522 -0500 EST m=+582.297900043"
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:09.755551  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64395188Ki" capacity="65061836Ki" time="2023-01-20 12:41:09.755487656 -0500 EST m=+582.311262478"
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:09.755557  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902012656Ki" capacity="981310056Ki" time="2023-01-20 12:41:09.74212522 -0500 EST m=+582.297900043"
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:09.755563  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945617" capacity="62382080" time="2023-01-20 12:41:09.74212522 -0500 EST m=+582.297900043"
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:09.755568  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902012656Ki" capacity="981310056Ki" time="2023-01-20 12:41:01.260966386 -0500 EST"
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:09.755572  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945617" capacity="62382080" time="2023-01-20 12:41:01.260966386 -0500 EST"
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:09.755577  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510763" capacity="511757" time="2023-01-20 12:41:09.755234103 -0500 EST m=+582.311008926"
Jan 20 12:41:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:09.755600  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:41:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:10.002557  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:41:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:10.002566  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:41:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:10.002721  199956 interface.go:209] Interface eno2 is up
Jan 20 12:41:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:10.002748  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:41:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:10.002756  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:41:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:10.002761  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:41:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:10.002764  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:41:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:10.002767  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:41:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:10.003118  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:41:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:10.003141  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:41:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:10.003145  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:41:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:10.003172  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:41:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:10.231280  199956 generic.go:155] "GenericPLEG" podUID=4f97314d-815b-4787-b174-5c158cd28c9d containerID="e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b" oldState=non-existent newState=running
Jan 20 12:41:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:10.232184  199956 kuberuntime_manager.go:1037] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b] pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:41:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:10.233405  199956 generic.go:421] "PLEG: Write status" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:41:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:10.233482  199956 kubelet.go:2098] "SyncLoop (PLEG): event for pod" pod="default/unsigned-unencrypted-cc-1" event=&{ID:4f97314d-815b-4787-b174-5c158cd28c9d Type:ContainerStarted Data:e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b}
Jan 20 12:41:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:10.483897  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:10.483928  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:10.490445  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[32bb751f-8e09-44da-a5c4-68d8760ffdb7] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:10 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e9e00 2 [] false false map[] 0xc001f36100 0xc000aff290}
Jan 20 12:41:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:10.490512  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:10 zcy-Z390-AORUS-MASTER avahi-daemon[896]: Joining mDNS multicast group on interface vethfbf8f23a.IPv6 with address fe80::3c4a:a0ff:fe3b:4bc7.
Jan 20 12:41:10 zcy-Z390-AORUS-MASTER avahi-daemon[896]: New relevant interface vethfbf8f23a.IPv6 for mDNS.
Jan 20 12:41:10 zcy-Z390-AORUS-MASTER avahi-daemon[896]: Registering new address record for fe80::3c4a:a0ff:fe3b:4bc7 on vethfbf8f23a.*.
Jan 20 12:41:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:11.166571  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:41:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:11.166643  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:11.174480  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[4bac734a-6b83-4c89-9e48-6220e5fdf57c] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:11 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001820b60 2 [] false false map[] 0xc001716100 0xc000d42d10}
Jan 20 12:41:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:11.174509  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:11.483431  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:11.483501  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:11.491233  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[b4bc43c2-672b-41b1-98fc-39aa4f898e91] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:11 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008e2a20 2 [] false false map[] 0xc001716400 0xc001982c60}
Jan 20 12:41:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:11.491265  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:11.582177  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:41:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:11.589632  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:41:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:11.589701  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:41:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:11.591943  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:41:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:11.594027  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:41:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:11.594250  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:41:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:11.594282  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:41:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:11.594325  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:41:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:12.483595  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:12.483665  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:12.491531  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[f7e82e75-feb9-4449-af88-e16c2b8c234c] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:12 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008e2ea0 2 [] false false map[] 0xc0002c3300 0xc0013e16b0}
Jan 20 12:41:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:12.491562  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:12.689361  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:41:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:12.689426  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:12.690509  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:12 GMT]] 0xc0008e2f20 2 [] true false map[] 0xc0002c3600 <nil>}
Jan 20 12:41:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:12.690612  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:41:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:12.695843  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:41:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:12.695903  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:12.696919  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:12 GMT]] 0xc0008e2f60 2 [] true false map[] 0xc0002c3800 <nil>}
Jan 20 12:41:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:12.697021  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:41:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:12.710248  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:41:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:12.710307  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:12.710320  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:41:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:12.710379  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:12.711305  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:12 GMT]] 0xc001702040 2 [] true false map[] 0xc001716700 <nil>}
Jan 20 12:41:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:12.711351  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:12 GMT]] 0xc00123e920 2 [] true false map[] 0xc0002c3a00 <nil>}
Jan 20 12:41:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:12.711414  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:41:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:12.711456  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:41:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:12.790089  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.483150  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.483241  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.496085  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[15297cdd-e01f-464d-8d5d-27679562633d] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:13 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001719860 2 [] false false map[] 0xc0002c3e00 0xc0014d1970}
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.496229  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.581894  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=2 pods=[kube-system/kube-apiserver-zcy-z390-aorus-master confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr]
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.581998  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.582047  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd updateType=0
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.582119  199956 pod_workers.go:888] "Processing pod event" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d updateType=0
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.582135  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.582185  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/kube-apiserver-zcy-z390-aorus-master"
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.582199  199956 kubelet.go:1501] "syncPod enter" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.582236  199956 kubelet_pods.go:1435] "Generating pod status" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.582285  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" oldPhase=Running phase=Running
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.582337  199956 kubelet_pods.go:1447] "Got phase for pod" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" oldPhase=Running phase=Running
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.582572  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:28 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:41 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:41 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:28 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.239.159.53 PodIPs:[{IP:10.239.159.53}] StartTime:2023-01-20 12:31:28 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:kube-apiserver State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:22 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:6 Image:k8s.gcr.io/kube-apiserver:v1.24.0 ImageID:k8s.gcr.io/kube-apiserver@sha256:a04522b882e919de6141b47d72393fb01226c78e7388400f966198222558c955 ContainerID:containerd://5900ac5c1383a321a6ae837b57d6d29d7a660a102c0806210a9ea57290673062 Started:0xc0015d24de}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.582935  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-apiserver-zcy-z390-aorus-master"
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.583007  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/kube-apiserver-zcy-z390-aorus-master"
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.583053  199956 status_manager.go:535] "Ignoring same status for pod" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:58 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:08 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:08 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:58 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.4 PodIPs:[{IP:10.244.0.4}] StartTime:2023-01-20 12:31:58 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:kube-rbac-proxy State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:59 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:gcr.io/kubebuilder/kube-rbac-proxy:v0.13.0 ImageID:gcr.io/kubebuilder/kube-rbac-proxy@sha256:d99a8d144816b951a67648c12c0b988936ccd25cf3754f3cd85ab8c01592248f ContainerID:containerd://1cb44bdbde0b41852e382d7dab7c184af8dd4e5b25d5cfe6a10a9c8ab601659d Started:0xc00103ce8f} {Name:manager State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:59 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:quay.io/confidential-containers/operator:v0.2.0 ImageID:quay.io/confidential-containers/operator@sha256:c965b55253a9abe4c2f7596c42467fa59f2cc741bfafeed1d25629ed6f8df12d ContainerID:containerd://186424b47c7b9022ecfe8996dc73cd9101f7539259cd65914279c84c9a02a288 Started:0xc00103cee0}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.583420  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.583498  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.584142  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.584148  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/kube-apiserver-zcy-z390-aorus-master"
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.584320  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd isTerminal=false
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.584322  199956 kubelet.go:1503] "syncPod exit" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d isTerminal=false
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.584376  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd updateType=0
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.584395  199956 pod_workers.go:988] "Processing pod event done" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d updateType=0
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.588066  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.588077  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.588084  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.588609  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.588653  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.588660  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.588669  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.622110  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" volumeName="ca-certs" volumeSpecName="ca-certs"
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.622231  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.622302  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" volumeName="etc-pki" volumeSpecName="etc-pki"
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.622368  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.622435  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.622498  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.622657  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" volumeName="kube-api-access-4pnfq" volumeSpecName="kube-api-access-4pnfq"
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.634938  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-4pnfq\" (UniqueName: \"kubernetes.io/projected/d2688d45-2487-46e7-aecb-e3479626909d-kube-api-access-4pnfq\") pod \"cc-operator-controller-manager-79797456f6-m6znr\" (UID: \"d2688d45-2487-46e7-aecb-e3479626909d\") Volume is already mounted to pod, but remount was requested." pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.635114  199956 projected.go:183] Setting up volume kube-api-access-4pnfq for pod d2688d45-2487-46e7-aecb-e3479626909d at /var/lib/kubelet/pods/d2688d45-2487-46e7-aecb-e3479626909d/volumes/kubernetes.io~projected/kube-api-access-4pnfq
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.635520  199956 atomic_writer.go:161] pod confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr volume kube-api-access-4pnfq: no update required for target directory /var/lib/kubelet/pods/d2688d45-2487-46e7-aecb-e3479626909d/volumes/kubernetes.io~projected/kube-api-access-4pnfq
Jan 20 12:41:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:13.635598  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-4pnfq\" (UniqueName: \"kubernetes.io/projected/d2688d45-2487-46e7-aecb-e3479626909d-kube-api-access-4pnfq\") pod \"cc-operator-controller-manager-79797456f6-m6znr\" (UID: \"d2688d45-2487-46e7-aecb-e3479626909d\") " pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:41:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:14.483414  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:14.483483  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:14.491506  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[1d8654f2-3bcb-4d00-a250-d22a8db93b18] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:14 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001702960 2 [] false false map[] 0xc0011c4300 0xc001c0bb80}
Jan 20 12:41:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:14.491537  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:15.483696  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:15.483745  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:15.490140  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[867f0d8b-a883-434c-bccd-5f20c55c2484] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:15 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123eb00 2 [] false false map[] 0xc001775300 0xc0005a1e40}
Jan 20 12:41:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:15.490212  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:15.582086  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:41:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:15.589483  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:41:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:15.589544  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:41:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:15.591736  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:41:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:15.593587  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:41:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:15.593812  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:41:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:15.593846  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:41:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:15.593890  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:41:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:16.164949  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:41:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:16.164972  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:16.168103  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:16 GMT] X-Content-Type-Options:[nosniff]] 0xc0020fc580 2 [] false false map[] 0xc0014c6b00 0xc001e01e40}
Jan 20 12:41:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:16.168143  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:41:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:16.272422  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:41:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:16.272486  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:16.273762  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:16 GMT]] 0xc0008dd4e0 29 [] true false map[] 0xc0011c4600 <nil>}
Jan 20 12:41:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:16.273868  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:41:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:16.483116  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:16.483176  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:16.491507  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[1ea88f9c-5712-4c8a-b8b5-d9e069f9cb76] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:16 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0020fc940 2 [] false false map[] 0xc002140400 0xc001e200b0}
Jan 20 12:41:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:16.491535  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:16.895923  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:41:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:16.895990  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:16.906933  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:16 GMT] X-Content-Type-Options:[nosniff]] 0xc00013cce0 2 [] false false map[] 0xc0011c4800 0xc001fae4d0}
Jan 20 12:41:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:16.907069  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:41:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:17.484078  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:17.484112  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:17.490458  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[699c16a6-26dc-4176-b48b-23ae22fc6983] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:17 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00013df60 2 [] false false map[] 0xc0014c6f00 0xc00210e2c0}
Jan 20 12:41:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:17.490535  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:17.581837  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:41:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:17.587230  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:41:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:17.587242  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:41:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:17.587249  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:41:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:17.588049  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:41:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:17.588096  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:41:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:17.588102  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:41:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:17.588111  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:41:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:17.791429  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:41:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:18.483163  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:18.483235  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:18.496816  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[188e2480-d56e-4be0-a7bd-b9bf81aa5898] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:18 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001820260 2 [] false false map[] 0xc001f36100 0xc0006a2790}
Jan 20 12:41:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:18.496947  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:18.821549  199956 handler.go:293] error while reading "/proc/199956/fd/34" link: readlink /proc/199956/fd/34: no such file or directory
Jan 20 12:41:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:18.878631  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/healthz" timeout="1s"
Jan 20 12:41:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:18.878700  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:18.878719  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:41:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:18.878782  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:18.880032  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:18 GMT] X-Content-Type-Options:[nosniff]] 0xc000706020 2 [] true false map[] 0xc001f36700 <nil>}
Jan 20 12:41:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:18.880063  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:18 GMT] X-Content-Type-Options:[nosniff]] 0xc001820a40 2 [] true false map[] 0xc0002c2f00 <nil>}
Jan 20 12:41:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:18.880154  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:41:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:18.880183  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:41:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:19.483707  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:19.483781  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:19.491300  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[334df614-e536-4c4c-99ac-9f536fdbe27b] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:19 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000cb24c0 2 [] false false map[] 0xc0011c4400 0xc001360160}
Jan 20 12:41:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:19.491333  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:19.582256  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:41:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:19.588098  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:41:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:19.588111  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:41:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:19.588790  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:41:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:19.589110  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:41:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:19.589156  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:41:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:19.589163  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:41:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:19.589171  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:41:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:19.756044  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:41:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:19.764156  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:41:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:19.774185  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945617" capacity="62382080" time="2023-01-20 12:41:11.261799428 -0500 EST"
Jan 20 12:41:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:19.774198  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510782" capacity="511757" time="2023-01-20 12:41:19.773883297 -0500 EST m=+592.329658122"
Jan 20 12:41:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:19.774206  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59490480Ki" capacity="65586124Ki" time="2023-01-20 12:41:19.75835052 -0500 EST m=+592.314125406"
Jan 20 12:41:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:19.774212  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64394988Ki" capacity="65061836Ki" time="2023-01-20 12:41:19.774132578 -0500 EST m=+592.329907402"
Jan 20 12:41:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:19.774218  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902012604Ki" capacity="981310056Ki" time="2023-01-20 12:41:19.75835052 -0500 EST m=+592.314125406"
Jan 20 12:41:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:19.774225  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945617" capacity="62382080" time="2023-01-20 12:41:19.75835052 -0500 EST m=+592.314125406"
Jan 20 12:41:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:19.774230  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902012604Ki" capacity="981310056Ki" time="2023-01-20 12:41:11.261799428 -0500 EST"
Jan 20 12:41:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:19.774256  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:41:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:20.179768  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:41:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:20.179776  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:41:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:20.179969  199956 interface.go:209] Interface eno2 is up
Jan 20 12:41:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:20.180010  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:41:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:20.180041  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:41:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:20.180046  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:41:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:20.180049  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:41:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:20.180053  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:41:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:20.180469  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:41:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:20.180490  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:41:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:20.180493  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:41:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:20.180515  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:41:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:20.483172  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:20.483201  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:20.489385  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[19d86340-a25c-4032-9cb0-1204bf2eea22] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:20 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001820a60 2 [] false false map[] 0xc0011c4700 0xc000be86e0}
Jan 20 12:41:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:20.489454  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.166780  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.166804  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.170997  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[5e704736-d67a-4b5a-8123-eac28550287f] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:21 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001821080 2 [] false false map[] 0xc001f36900 0xc000be8840}
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.171044  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.483381  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.483456  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.496101  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[8a2a0087-2fca-44e4-add2-24147e071175] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:21 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001486480 2 [] false false map[] 0xc0011c5c00 0xc0010288f0}
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.496238  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.581473  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[kube-flannel/kube-flannel-ds-hprn4]
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.581567  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.581711  199956 pod_workers.go:888] "Processing pod event" pod="kube-flannel/kube-flannel-ds-hprn4" podUID=04e472cb-b6f9-4065-b509-d7e1579fc48d updateType=0
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.581791  199956 kubelet.go:1501] "syncPod enter" pod="kube-flannel/kube-flannel-ds-hprn4" podUID=04e472cb-b6f9-4065-b509-d7e1579fc48d
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.581822  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.581940  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-flannel/kube-flannel-ds-hprn4" oldPhase=Running phase=Running
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.582330  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-flannel/kube-flannel-ds-hprn4" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:44 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:45 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:45 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.239.159.53 PodIPs:[{IP:10.239.159.53}] StartTime:2023-01-20 12:31:42 -0500 EST InitContainerStatuses:[{Name:install-cni-plugin State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2023-01-20 12:31:43 -0500 EST,FinishedAt:2023-01-20 12:31:43 -0500 EST,ContainerID:containerd://349eee1bf5a2d95206979822c956ea6f555dbb786434c7b9fe46524872f1a18d,}} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:docker.io/rancher/mirrored-flannelcni-flannel-cni-plugin:v1.1.0 ImageID:docker.io/rancher/mirrored-flannelcni-flannel-cni-plugin@sha256:28d3a6be9f450282bf42e4dad143d41da23e3d91f66f19c01ee7fd21fd17cb2b ContainerID:containerd://349eee1bf5a2d95206979822c956ea6f555dbb786434c7b9fe46524872f1a18d Started:<nil>} {Name:install-cni State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2023-01-20 12:31:43 -0500 EST,FinishedAt:2023-01-20 12:31:43 -0500 EST,ContainerID:containerd://85a4ede91a47b8d7df192dc39004a2a7fa45e095191a7335eaed8ec826cd9f36,}} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:docker.io/rancher/mirrored-flannelcni-flannel:v0.19.1 ImageID:docker.io/rancher/mirrored-flannelcni-flannel@sha256:e3b0f06121b0f7a11a684e910bba89b3ab49cd6e73aeb6c719f83b2456946366 ContainerID:containerd://85a4ede91a47b8d7df192dc39004a2a7fa45e095191a7335eaed8ec826cd9f36 Started:<nil>}] ContainerStatuses:[{Name:kube-flannel State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:44 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:docker.io/rancher/mirrored-flannelcni-flannel:v0.19.1 ImageID:docker.io/rancher/mirrored-flannelcni-flannel@sha256:e3b0f06121b0f7a11a684e910bba89b3ab49cd6e73aeb6c719f83b2456946366 ContainerID:containerd://0f9ae677fe935afcf43e145281deae0d663ba95fda6759ff24f0dd3e1cee17a9 Started:0xc00114b469}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.582711  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.582833  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.583300  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.583488  199956 kubelet.go:1503] "syncPod exit" pod="kube-flannel/kube-flannel-ds-hprn4" podUID=04e472cb-b6f9-4065-b509-d7e1579fc48d isTerminal=false
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.583546  199956 pod_workers.go:988] "Processing pod event done" pod="kube-flannel/kube-flannel-ds-hprn4" podUID=04e472cb-b6f9-4065-b509-d7e1579fc48d updateType=0
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.587161  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.587173  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.587179  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.587707  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.587750  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.587757  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.587765  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.683947  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="run" volumeSpecName="run"
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.684054  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="cni-plugin" volumeSpecName="cni-plugin"
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.684125  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="cni" volumeSpecName="cni"
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.684205  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="flannel-cfg" volumeSpecName="flannel-cfg"
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.684276  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="xtables-lock" volumeSpecName="xtables-lock"
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.684392  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="kube-api-access-hqj8d" volumeSpecName="kube-api-access-hqj8d"
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.697128  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-hqj8d\" (UniqueName: \"kubernetes.io/projected/04e472cb-b6f9-4065-b509-d7e1579fc48d-kube-api-access-hqj8d\") pod \"kube-flannel-ds-hprn4\" (UID: \"04e472cb-b6f9-4065-b509-d7e1579fc48d\") Volume is already mounted to pod, but remount was requested." pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.697262  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"flannel-cfg\" (UniqueName: \"kubernetes.io/configmap/04e472cb-b6f9-4065-b509-d7e1579fc48d-flannel-cfg\") pod \"kube-flannel-ds-hprn4\" (UID: \"04e472cb-b6f9-4065-b509-d7e1579fc48d\") Volume is already mounted to pod, but remount was requested." pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.697406  199956 configmap.go:181] Setting up volume flannel-cfg for pod 04e472cb-b6f9-4065-b509-d7e1579fc48d at /var/lib/kubelet/pods/04e472cb-b6f9-4065-b509-d7e1579fc48d/volumes/kubernetes.io~configmap/flannel-cfg
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.697446  199956 projected.go:183] Setting up volume kube-api-access-hqj8d for pod 04e472cb-b6f9-4065-b509-d7e1579fc48d at /var/lib/kubelet/pods/04e472cb-b6f9-4065-b509-d7e1579fc48d/volumes/kubernetes.io~projected/kube-api-access-hqj8d
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.697465  199956 configmap.go:205] Received configMap kube-flannel/kube-flannel-cfg containing (2) pieces of data, 365 total bytes
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.697553  199956 quota_linux.go:271] SupportsQuotas called, but quotas disabled
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.697832  199956 atomic_writer.go:161] pod kube-flannel/kube-flannel-ds-hprn4 volume flannel-cfg: no update required for target directory /var/lib/kubelet/pods/04e472cb-b6f9-4065-b509-d7e1579fc48d/volumes/kubernetes.io~configmap/flannel-cfg
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.697897  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"flannel-cfg\" (UniqueName: \"kubernetes.io/configmap/04e472cb-b6f9-4065-b509-d7e1579fc48d-flannel-cfg\") pod \"kube-flannel-ds-hprn4\" (UID: \"04e472cb-b6f9-4065-b509-d7e1579fc48d\") " pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.697904  199956 atomic_writer.go:161] pod kube-flannel/kube-flannel-ds-hprn4 volume kube-api-access-hqj8d: no update required for target directory /var/lib/kubelet/pods/04e472cb-b6f9-4065-b509-d7e1579fc48d/volumes/kubernetes.io~projected/kube-api-access-hqj8d
Jan 20 12:41:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:21.697982  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-hqj8d\" (UniqueName: \"kubernetes.io/projected/04e472cb-b6f9-4065-b509-d7e1579fc48d-kube-api-access-hqj8d\") pod \"kube-flannel-ds-hprn4\" (UID: \"04e472cb-b6f9-4065-b509-d7e1579fc48d\") " pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:41:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:22.483413  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:22.483487  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:22.491525  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[a8fbd583-fdd7-47ca-a187-3d515c58a39f] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:22 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008e2460 2 [] false false map[] 0xc000cf7a00 0xc00166e370}
Jan 20 12:41:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:22.491556  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:22.689994  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:41:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:22.690062  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:22.691159  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:22 GMT]] 0xc0014872a0 2 [] true false map[] 0xc001716300 <nil>}
Jan 20 12:41:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:22.691273  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:41:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:22.695655  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:41:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:22.695714  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:22.696752  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:22 GMT]] 0xc0014872e0 2 [] true false map[] 0xc001716500 <nil>}
Jan 20 12:41:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:22.696852  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:41:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:22.710371  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:41:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:22.710433  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:22.710447  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:41:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:22.710509  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:22.711456  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:22 GMT]] 0xc001487320 2 [] true false map[] 0xc001716700 <nil>}
Jan 20 12:41:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:22.711560  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:41:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:22.711528  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:22 GMT]] 0xc0009a0940 2 [] true false map[] 0xc001775a00 <nil>}
Jan 20 12:41:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:22.711640  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:41:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:22.792961  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:41:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:23.483404  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:23.483475  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:23.496096  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[24d98732-05dc-4b6a-9e0f-50c18df836d8] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:23 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0009a1920 2 [] false false map[] 0xc001775c00 0xc001765d90}
Jan 20 12:41:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:23.496250  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:23.582045  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:41:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:23.588331  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:41:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:23.588355  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:41:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:23.589077  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:41:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:23.589431  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:41:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:23.589479  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:41:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:23.589487  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:41:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:23.589496  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:41:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:24.484193  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:24.484260  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:24.492453  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[5723bd09-0a08-4316-9da4-c382ec60859c] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:24 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0009a07c0 2 [] false false map[] 0xc001774b00 0xc0015f3c30}
Jan 20 12:41:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:24.492499  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:25.483225  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:25.483291  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:25.491165  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[b9eb85ad-6bf8-4f29-b439-4e39c09911b4] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:25 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008e3920 2 [] false false map[] 0xc001716100 0xc001018370}
Jan 20 12:41:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:25.491198  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:25.582121  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:41:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:25.588080  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:41:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:25.588094  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:41:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:25.588102  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:41:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:25.588676  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:41:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:25.588729  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:41:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:25.588736  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:41:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:25.588744  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.165132  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.165206  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.172674  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:26 GMT] X-Content-Type-Options:[nosniff]] 0xc0009a1fc0 2 [] false false map[] 0xc002140600 0xc0010184d0}
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.172711  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.272526  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.272592  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.273846  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:26 GMT]] 0xc0012e8040 29 [] true false map[] 0xc0002c3e00 <nil>}
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.273949  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.483888  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.483956  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.492197  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[cb02a4d5-4af5-4ddd-b5f9-ef4ef013549e] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:26 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e8240 2 [] false false map[] 0xc001716500 0xc0010186e0}
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.492255  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.581970  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[kube-system/coredns-6d4b75cb6d-zdl2m]
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.582048  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e updateType=0
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.582110  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.582141  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.582231  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/coredns-6d4b75cb6d-zdl2m" oldPhase=Running phase=Running
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.582518  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/coredns-6d4b75cb6d-zdl2m" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:44 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:44 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.3 PodIPs:[{IP:10.244.0.3}] StartTime:2023-01-20 12:31:42 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:coredns State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:44 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:k8s.gcr.io/coredns/coredns:v1.8.6 ImageID:k8s.gcr.io/coredns/coredns@sha256:5b6ec0d6de9baaf3e92d0f66cd96a25b9edbce8716f5f15dcd1a616b3abd590e ContainerID:containerd://e489181a7f95431b6d92f037dbe3f788b46a5e024df7aaf29e0115dab1168ab1 Started:0xc001f849de}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.582841  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.582912  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.583412  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.583580  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e isTerminal=false
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.583637  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e updateType=0
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.620856  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/coredns-6d4b75cb6d-zdl2m" volumeName="config-volume" volumeSpecName="config-volume"
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.621010  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/coredns-6d4b75cb6d-zdl2m" volumeName="kube-api-access-tqzsm" volumeSpecName="kube-api-access-tqzsm"
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.634886  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/cf25b9ea-6823-4eff-b30d-36a09f9d897e-config-volume\") pod \"coredns-6d4b75cb6d-zdl2m\" (UID: \"cf25b9ea-6823-4eff-b30d-36a09f9d897e\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.635010  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-tqzsm\" (UniqueName: \"kubernetes.io/projected/cf25b9ea-6823-4eff-b30d-36a09f9d897e-kube-api-access-tqzsm\") pod \"coredns-6d4b75cb6d-zdl2m\" (UID: \"cf25b9ea-6823-4eff-b30d-36a09f9d897e\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.635163  199956 projected.go:183] Setting up volume kube-api-access-tqzsm for pod cf25b9ea-6823-4eff-b30d-36a09f9d897e at /var/lib/kubelet/pods/cf25b9ea-6823-4eff-b30d-36a09f9d897e/volumes/kubernetes.io~projected/kube-api-access-tqzsm
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.635207  199956 configmap.go:181] Setting up volume config-volume for pod cf25b9ea-6823-4eff-b30d-36a09f9d897e at /var/lib/kubelet/pods/cf25b9ea-6823-4eff-b30d-36a09f9d897e/volumes/kubernetes.io~configmap/config-volume
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.635292  199956 configmap.go:205] Received configMap kube-system/coredns containing (1) pieces of data, 339 total bytes
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.635366  199956 quota_linux.go:271] SupportsQuotas called, but quotas disabled
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.635575  199956 atomic_writer.go:161] pod kube-system/coredns-6d4b75cb6d-zdl2m volume kube-api-access-tqzsm: no update required for target directory /var/lib/kubelet/pods/cf25b9ea-6823-4eff-b30d-36a09f9d897e/volumes/kubernetes.io~projected/kube-api-access-tqzsm
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.635593  199956 atomic_writer.go:161] pod kube-system/coredns-6d4b75cb6d-zdl2m volume config-volume: no update required for target directory /var/lib/kubelet/pods/cf25b9ea-6823-4eff-b30d-36a09f9d897e/volumes/kubernetes.io~configmap/config-volume
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.635641  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-tqzsm\" (UniqueName: \"kubernetes.io/projected/cf25b9ea-6823-4eff-b30d-36a09f9d897e-kube-api-access-tqzsm\") pod \"coredns-6d4b75cb6d-zdl2m\" (UID: \"cf25b9ea-6823-4eff-b30d-36a09f9d897e\") " pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.635672  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/cf25b9ea-6823-4eff-b30d-36a09f9d897e-config-volume\") pod \"coredns-6d4b75cb6d-zdl2m\" (UID: \"cf25b9ea-6823-4eff-b30d-36a09f9d897e\") " pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.896135  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.896205  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.903610  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:26 GMT] X-Content-Type-Options:[nosniff]] 0xc0012e8ca0 2 [] false false map[] 0xc001716900 0xc0016ea790}
Jan 20 12:41:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:26.903638  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.483847  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.483924  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.496529  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[528f773b-b16a-4878-9bba-198ff02a476a] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:27 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e9260 2 [] false false map[] 0xc000338500 0xc001899810}
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.496655  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.526046  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/etcd.yaml"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.528174  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver.yaml"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.531105  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.531977  199956 kubelet.go:1308] "Image garbage collection succeeded"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.532159  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-scheduler.yaml"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.532449  199956 config.go:279] "Setting pods for source" source="file"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.537149  199956 kubelet_getters.go:176] "Pod status updated" pod="kube-system/etcd-zcy-z390-aorus-master" status=Running
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.537167  199956 kubelet_getters.go:176] "Pod status updated" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" status=Running
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.537173  199956 kubelet_getters.go:176] "Pod status updated" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" status=Running
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.537177  199956 kubelet_getters.go:176] "Pod status updated" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" status=Running
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.582211  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.589522  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.589581  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.591546  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.593483  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.593713  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.593749  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.593793  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.628003  199956 oom_linux.go:66] attempting to set "/proc/199956/oom_score_adj" to "-999"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635406  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/alsa-restore.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635416  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/alsa-restore.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635421  199956 factory.go:255] Factory "raw" can handle container "/system.slice/alsa-restore.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635427  199956 manager.go:925] ignoring container "/system.slice/alsa-restore.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635431  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/blk-availability.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635434  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/blk-availability.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635437  199956 factory.go:255] Factory "raw" can handle container "/system.slice/blk-availability.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635441  199956 manager.go:925] ignoring container "/system.slice/blk-availability.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635444  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dbus.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635447  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/dbus.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635451  199956 factory.go:255] Factory "raw" can handle container "/system.slice/dbus.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635455  199956 manager.go:925] ignoring container "/system.slice/dbus.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635457  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-1000.slice/session-1.scope"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635460  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-1000.slice/session-1.scope"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635463  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-1000.slice/session-1.scope", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635468  199956 manager.go:925] ignoring container "/user.slice/user-1000.slice/session-1.scope"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635470  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-fsckd.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635473  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-fsckd.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635476  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-fsckd.socket", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635480  199956 manager.go:925] ignoring container "/system.slice/systemd-fsckd.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635483  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/udisks2.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635486  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/udisks2.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635489  199956 factory.go:255] Factory "raw" can handle container "/system.slice/udisks2.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635493  199956 manager.go:925] ignoring container "/system.slice/udisks2.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635496  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-udevd-kernel.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635499  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-udevd-kernel.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635502  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-udevd-kernel.socket", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635506  199956 manager.go:925] ignoring container "/system.slice/systemd-udevd-kernel.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635509  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/proc-sys-fs-binfmt_misc.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635512  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/proc-sys-fs-binfmt_misc.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635516  199956 manager.go:925] ignoring container "/system.slice/proc-sys-fs-binfmt_misc.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635519  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dev-hugepages.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635522  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/dev-hugepages.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635526  199956 manager.go:925] ignoring container "/system.slice/dev-hugepages.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635529  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-remount-fs.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635532  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-remount-fs.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635535  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-remount-fs.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635539  199956 manager.go:925] ignoring container "/system.slice/systemd-remount-fs.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635542  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-core20-1611.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635546  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-core20-1611.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635550  199956 manager.go:925] ignoring container "/system.slice/snap-core20-1611.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635553  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/keyboard-setup.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635556  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/keyboard-setup.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635559  199956 factory.go:255] Factory "raw" can handle container "/system.slice/keyboard-setup.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635563  199956 manager.go:925] ignoring container "/system.slice/keyboard-setup.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635566  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-snapd-ns-snap\\x2dstore.mnt.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635570  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-snapd-ns-snap\\x2dstore.mnt.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635574  199956 manager.go:925] ignoring container "/system.slice/run-snapd-ns-snap\\x2dstore.mnt.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635577  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/cron.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635579  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/cron.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635582  199956 factory.go:255] Factory "raw" can handle container "/system.slice/cron.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635586  199956 manager.go:925] ignoring container "/system.slice/cron.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635589  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/upower.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635592  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/upower.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635595  199956 factory.go:255] Factory "raw" can handle container "/system.slice/upower.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635599  199956 manager.go:925] ignoring container "/system.slice/upower.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635602  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-186424b47c7b9022ecfe8996dc73cd9101f7539259cd65914279c84c9a02a288-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635606  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-186424b47c7b9022ecfe8996dc73cd9101f7539259cd65914279c84c9a02a288-rootfs.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635612  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-186424b47c7b9022ecfe8996dc73cd9101f7539259cd65914279c84c9a02a288-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635616  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/cups-browsed.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635618  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/cups-browsed.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635622  199956 factory.go:255] Factory "raw" can handle container "/system.slice/cups-browsed.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635626  199956 manager.go:925] ignoring container "/system.slice/cups-browsed.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635628  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/apport.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635631  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/apport.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635634  199956 factory.go:255] Factory "raw" can handle container "/system.slice/apport.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635638  199956 manager.go:925] ignoring container "/system.slice/apport.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635641  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635645  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-rootfs.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635650  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635653  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-b0713fbc\\x2defc5\\x2d4044\\x2d9d08\\x2d2326a0752f87-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dgcgm6.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635658  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-b0713fbc\\x2defc5\\x2d4044\\x2d9d08\\x2d2326a0752f87-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dgcgm6.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635664  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-b0713fbc\\x2defc5\\x2d4044\\x2d9d08\\x2d2326a0752f87-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dgcgm6.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635668  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journal-flush.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635671  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journal-flush.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635674  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journal-flush.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635678  199956 manager.go:925] ignoring container "/system.slice/systemd-journal-flush.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635681  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-kernel-config.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635685  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-kernel-config.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635689  199956 manager.go:925] ignoring container "/system.slice/sys-kernel-config.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635691  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635696  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-rootfs.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635700  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635704  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-machine-id-commit.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635707  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-machine-id-commit.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635710  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-machine-id-commit.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635714  199956 manager.go:925] ignoring container "/system.slice/systemd-machine-id-commit.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635717  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-tmpfiles-setup.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635720  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-tmpfiles-setup.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635725  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-tmpfiles-setup.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635729  199956 manager.go:925] ignoring container "/system.slice/systemd-tmpfiles-setup.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635732  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/user@0.service/dbus.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635735  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/user@0.service/dbus.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635738  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/user@0.service/dbus.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635742  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/user@0.service/dbus.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635745  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/acpid.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635748  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/acpid.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635751  199956 factory.go:255] Factory "raw" can handle container "/system.slice/acpid.socket", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635755  199956 manager.go:925] ignoring container "/system.slice/acpid.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635758  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b3af3b285c950aba4fcd0176473261f7f4700aeaafe9c73ae15b15a7d6304092-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635762  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b3af3b285c950aba4fcd0176473261f7f4700aeaafe9c73ae15b15a7d6304092-rootfs.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635767  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b3af3b285c950aba4fcd0176473261f7f4700aeaafe9c73ae15b15a7d6304092-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635770  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-shm.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635775  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-shm.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635785  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-shm.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635789  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journald-dev-log.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635792  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journald-dev-log.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635795  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journald-dev-log.socket", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635799  199956 manager.go:925] ignoring container "/system.slice/systemd-journald-dev-log.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635802  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-1cb44bdbde0b41852e382d7dab7c184af8dd4e5b25d5cfe6a10a9c8ab601659d-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635806  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-1cb44bdbde0b41852e382d7dab7c184af8dd4e5b25d5cfe6a10a9c8ab601659d-rootfs.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635811  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-1cb44bdbde0b41852e382d7dab7c184af8dd4e5b25d5cfe6a10a9c8ab601659d-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635814  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-lvm2\\x2dpvscan.slice/lvm2-pvscan@8:10.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635818  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-lvm2\\x2dpvscan.slice/lvm2-pvscan@8:10.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635821  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-lvm2\\x2dpvscan.slice/lvm2-pvscan@8:10.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635826  199956 manager.go:925] ignoring container "/system.slice/system-lvm2\\x2dpvscan.slice/lvm2-pvscan@8:10.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635829  199956 factory.go:262] Factory "containerd" was unable to handle container "/kata_overhead"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635831  199956 factory.go:262] Factory "systemd" was unable to handle container "/kata_overhead"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635834  199956 factory.go:255] Factory "raw" can handle container "/kata_overhead", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635838  199956 manager.go:925] ignoring container "/kata_overhead"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635840  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2d58469e7a\\x2dc331\\x2d785c\\x2dc760\\x2d93c7232334bd.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635845  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2d58469e7a\\x2dc331\\x2d785c\\x2dc760\\x2d93c7232334bd.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635850  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2d58469e7a\\x2dc331\\x2d785c\\x2dc760\\x2d93c7232334bd.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635854  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-user-1000-gvfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635857  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-user-1000-gvfs.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635861  199956 manager.go:925] ignoring container "/system.slice/run-user-1000-gvfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635864  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/boot-efi.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635867  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/boot-efi.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635871  199956 manager.go:925] ignoring container "/system.slice/boot-efi.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635874  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-7af065b7\\x2d9095\\x2d4d91\\x2d9b9e\\x2d2644e7b1f4bf-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dx6vjr.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635878  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-7af065b7\\x2d9095\\x2d4d91\\x2d9b9e\\x2d2644e7b1f4bf-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dx6vjr.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635884  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-7af065b7\\x2d9095\\x2d4d91\\x2d9b9e\\x2d2644e7b1f4bf-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dx6vjr.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635888  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/rtkit-daemon.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635891  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/rtkit-daemon.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635894  199956 factory.go:255] Factory "raw" can handle container "/system.slice/rtkit-daemon.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635899  199956 manager.go:925] ignoring container "/system.slice/rtkit-daemon.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635901  199956 factory.go:262] Factory "containerd" was unable to handle container "/init.scope"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635904  199956 factory.go:262] Factory "systemd" was unable to handle container "/init.scope"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635907  199956 factory.go:255] Factory "raw" can handle container "/init.scope", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635911  199956 manager.go:925] ignoring container "/init.scope"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635913  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-shm.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635917  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-shm.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635922  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-shm.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635926  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-snapd-16292.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635929  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-snapd-16292.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635933  199956 manager.go:925] ignoring container "/system.slice/snap-snapd-16292.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635939  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/syslog.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635942  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/syslog.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635945  199956 factory.go:255] Factory "raw" can handle container "/system.slice/syslog.socket", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635948  199956 manager.go:925] ignoring container "/system.slice/syslog.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635951  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635956  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-rootfs.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635961  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635965  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/lvm2-lvmpolld.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635968  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/lvm2-lvmpolld.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635971  199956 factory.go:255] Factory "raw" can handle container "/system.slice/lvm2-lvmpolld.socket", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635975  199956 manager.go:925] ignoring container "/system.slice/lvm2-lvmpolld.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635978  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-shm.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635982  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-shm.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635987  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-shm.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635991  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/gdm.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635993  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/gdm.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.635996  199956 factory.go:255] Factory "raw" can handle container "/system.slice/gdm.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636000  199956 manager.go:925] ignoring container "/system.slice/gdm.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636003  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-fs-fuse-connections.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636006  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-fs-fuse-connections.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636010  199956 manager.go:925] ignoring container "/system.slice/sys-fs-fuse-connections.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636013  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:41:27.637058874-05:00" level=info msg="StopPodSandbox for \"c1cda931f1357d6910e39cd5c0376b33fd1d266eee14025b4fdf233198fed924\""
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636017  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-rootfs.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636022  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636026  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636030  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-rootfs.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636035  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636039  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-sysusers.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636042  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-sysusers.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636045  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-sysusers.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636050  199956 manager.go:925] ignoring container "/system.slice/systemd-sysusers.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636052  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/-.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636055  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/-.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636059  199956 manager.go:925] ignoring container "/system.slice/-.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636062  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-cf25b9ea\\x2d6823\\x2d4eff\\x2db30d\\x2d36a09f9d897e-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dtqzsm.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636066  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-cf25b9ea\\x2d6823\\x2d4eff\\x2db30d\\x2d36a09f9d897e-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dtqzsm.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636072  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-cf25b9ea\\x2d6823\\x2d4eff\\x2db30d\\x2d36a09f9d897e-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dtqzsm.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636076  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636080  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-rootfs.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636086  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636089  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636093  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-rootfs.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636098  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636102  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f8443d2868215ec42d3fa335663976e33df166c5e98369aa3f9d7ddb9235bead-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636106  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f8443d2868215ec42d3fa335663976e33df166c5e98369aa3f9d7ddb9235bead-rootfs.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636111  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f8443d2868215ec42d3fa335663976e33df166c5e98369aa3f9d7ddb9235bead-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636114  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/ModemManager.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636117  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/ModemManager.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636120  199956 factory.go:255] Factory "raw" can handle container "/system.slice/ModemManager.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636124  199956 manager.go:925] ignoring container "/system.slice/ModemManager.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636127  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dbus.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636129  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/dbus.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636133  199956 factory.go:255] Factory "raw" can handle container "/system.slice/dbus.socket", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636136  199956 manager.go:925] ignoring container "/system.slice/dbus.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636139  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-snap\\x2dstore-558.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636142  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-snap\\x2dstore-558.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636148  199956 manager.go:925] ignoring container "/system.slice/snap-snap\\x2dstore-558.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636150  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-kernel-debug.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636154  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-kernel-debug.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636158  199956 manager.go:925] ignoring container "/system.slice/sys-kernel-debug.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636161  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-13149330f3752d0ff65bcac86c7b522b2040811e815fbc87e56b40b612875d5a-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636165  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-13149330f3752d0ff65bcac86c7b522b2040811e815fbc87e56b40b612875d5a-rootfs.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636169  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-13149330f3752d0ff65bcac86c7b522b2040811e815fbc87e56b40b612875d5a-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636173  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/session-42.scope"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636176  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/session-42.scope"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636180  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/session-42.scope", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636184  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/session-42.scope"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636186  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-systemd\\x2dfsck.slice"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636189  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-systemd\\x2dfsck.slice"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636192  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-systemd\\x2dfsck.slice", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636196  199956 manager.go:925] ignoring container "/system.slice/system-systemd\\x2dfsck.slice"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636200  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/ufw.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636203  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/ufw.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636206  199956 factory.go:255] Factory "raw" can handle container "/system.slice/ufw.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636210  199956 manager.go:925] ignoring container "/system.slice/ufw.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636213  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/uuidd.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636216  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/uuidd.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636219  199956 factory.go:255] Factory "raw" can handle container "/system.slice/uuidd.socket", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636223  199956 manager.go:925] ignoring container "/system.slice/uuidd.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636226  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-modules-load.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636228  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-modules-load.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636232  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-modules-load.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636236  199956 manager.go:925] ignoring container "/system.slice/systemd-modules-load.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636238  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journald.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636241  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journald.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636244  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journald.socket", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636248  199956 manager.go:925] ignoring container "/system.slice/systemd-journald.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636251  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/openvpn.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636254  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/openvpn.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636257  199956 factory.go:255] Factory "raw" can handle container "/system.slice/openvpn.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636261  199956 manager.go:925] ignoring container "/system.slice/openvpn.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636265  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/ssh.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636267  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/ssh.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636270  199956 factory.go:255] Factory "raw" can handle container "/system.slice/ssh.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636275  199956 manager.go:925] ignoring container "/system.slice/ssh.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636277  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-shm.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636281  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-shm.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636286  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-shm.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636290  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-update-utmp.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636293  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-update-utmp.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636296  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-update-utmp.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636300  199956 manager.go:925] ignoring container "/system.slice/systemd-update-utmp.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636303  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636307  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-rootfs.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636312  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636316  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/kmod-static-nodes.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636319  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/kmod-static-nodes.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636322  199956 factory.go:255] Factory "raw" can handle container "/system.slice/kmod-static-nodes.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636326  199956 manager.go:925] ignoring container "/system.slice/kmod-static-nodes.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636329  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-9b11acac1b891eafc7ff2b244f1c42938f71a575ce81ff917e3b7ebf61d2e045-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636333  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-9b11acac1b891eafc7ff2b244f1c42938f71a575ce81ff917e3b7ebf61d2e045-rootfs.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636338  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-9b11acac1b891eafc7ff2b244f1c42938f71a575ce81ff917e3b7ebf61d2e045-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636342  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-user-sessions.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636344  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-user-sessions.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636347  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-user-sessions.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636352  199956 manager.go:925] ignoring container "/system.slice/systemd-user-sessions.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636354  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2d562f9e70\\x2db2b7\\x2d3ac4\\x2d2311\\x2db91510a5a9ac.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636358  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2d562f9e70\\x2db2b7\\x2d3ac4\\x2d2311\\x2db91510a5a9ac.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636363  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2d562f9e70\\x2db2b7\\x2d3ac4\\x2d2311\\x2db91510a5a9ac.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636366  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-1c057a0e\\x2d3ee3\\x2d44f3\\x2db294\\x2d434acc8f9491-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d2sbqp.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636370  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-1c057a0e\\x2d3ee3\\x2d44f3\\x2db294\\x2d434acc8f9491-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d2sbqp.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636376  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-1c057a0e\\x2d3ee3\\x2d44f3\\x2db294\\x2d434acc8f9491-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d2sbqp.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636380  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/thermald.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636382  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/thermald.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636385  199956 factory.go:255] Factory "raw" can handle container "/system.slice/thermald.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636389  199956 manager.go:925] ignoring container "/system.slice/thermald.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636393  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-initctl.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636396  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-initctl.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636399  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-initctl.socket", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636403  199956 manager.go:925] ignoring container "/system.slice/systemd-initctl.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636406  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-user-1000.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636409  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-user-1000.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636413  199956 manager.go:925] ignoring container "/system.slice/run-user-1000.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636416  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-shm.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636420  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-shm.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636427  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-shm.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636431  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-0f9ae677fe935afcf43e145281deae0d663ba95fda6759ff24f0dd3e1cee17a9-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636435  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-0f9ae677fe935afcf43e145281deae0d663ba95fda6759ff24f0dd3e1cee17a9-rootfs.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636440  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-0f9ae677fe935afcf43e145281deae0d663ba95fda6759ff24f0dd3e1cee17a9-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636444  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snapd.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636446  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/snapd.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636450  199956 factory.go:255] Factory "raw" can handle container "/system.slice/snapd.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636455  199956 manager.go:925] ignoring container "/system.slice/snapd.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636458  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-random-seed.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636461  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-random-seed.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636464  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-random-seed.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636468  199956 manager.go:925] ignoring container "/system.slice/systemd-random-seed.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636471  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-snapd-ns.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636474  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-snapd-ns.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636478  199956 manager.go:925] ignoring container "/system.slice/run-snapd-ns.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636481  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-bare-5.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636484  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-bare-5.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636488  199956 manager.go:925] ignoring container "/system.slice/snap-bare-5.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636491  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2d8b7da23a\\x2d3511\\x2dfe06\\x2d72d7\\x2d26a549eb607d.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636495  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2d8b7da23a\\x2d3511\\x2dfe06\\x2d72d7\\x2d26a549eb607d.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636499  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2d8b7da23a\\x2d3511\\x2dfe06\\x2d72d7\\x2d26a549eb607d.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636502  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-gtk\\x2dcommon\\x2dthemes-1535.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636506  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-gtk\\x2dcommon\\x2dthemes-1535.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636510  199956 manager.go:925] ignoring container "/system.slice/snap-gtk\\x2dcommon\\x2dthemes-1535.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636513  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-d94e1927\\x2dd22d\\x2d4831\\x2da764\\x2d0170eae346a1-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d9qh7j.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636517  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-d94e1927\\x2dd22d\\x2d4831\\x2da764\\x2d0170eae346a1-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d9qh7j.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636523  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-d94e1927\\x2dd22d\\x2d4831\\x2da764\\x2d0170eae346a1-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d9qh7j.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636527  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/irqbalance.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636530  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/irqbalance.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636533  199956 factory.go:255] Factory "raw" can handle container "/system.slice/irqbalance.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636537  199956 manager.go:925] ignoring container "/system.slice/irqbalance.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636540  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/docker.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636544  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/docker.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636547  199956 factory.go:255] Factory "raw" can handle container "/system.slice/docker.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636551  199956 manager.go:925] ignoring container "/system.slice/docker.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636553  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f78ed441ff85d669a403a76c0987f2d86913431bd96d454b1a3605bdcf0474f2-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636557  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f78ed441ff85d669a403a76c0987f2d86913431bd96d454b1a3605bdcf0474f2-rootfs.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636562  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f78ed441ff85d669a403a76c0987f2d86913431bd96d454b1a3605bdcf0474f2-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636566  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e489181a7f95431b6d92f037dbe3f788b46a5e024df7aaf29e0115dab1168ab1-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636570  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e489181a7f95431b6d92f037dbe3f788b46a5e024df7aaf29e0115dab1168ab1-rootfs.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636575  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e489181a7f95431b6d92f037dbe3f788b46a5e024df7aaf29e0115dab1168ab1-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636579  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/wpa_supplicant.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636581  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/wpa_supplicant.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636585  199956 factory.go:255] Factory "raw" can handle container "/system.slice/wpa_supplicant.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636589  199956 manager.go:925] ignoring container "/system.slice/wpa_supplicant.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636592  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2dd678d9bc\\x2d8fcb\\x2d9fbe\\x2d1142\\x2ddede54862bab.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636596  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2dd678d9bc\\x2d8fcb\\x2d9fbe\\x2d1142\\x2ddede54862bab.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636600  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2dd678d9bc\\x2d8fcb\\x2d9fbe\\x2d1142\\x2ddede54862bab.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636603  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/apparmor.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636606  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/apparmor.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636610  199956 factory.go:255] Factory "raw" can handle container "/system.slice/apparmor.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636615  199956 manager.go:925] ignoring container "/system.slice/apparmor.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636617  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-shm.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636621  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-shm.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636627  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-shm.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636630  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/docker.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636633  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/docker.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636637  199956 factory.go:255] Factory "raw" can handle container "/system.slice/docker.socket", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636640  199956 manager.go:925] ignoring container "/system.slice/docker.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636643  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journald.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636646  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journald.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636649  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journald.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636653  199956 manager.go:925] ignoring container "/system.slice/systemd-journald.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636656  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/unattended-upgrades.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636660  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/unattended-upgrades.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636663  199956 factory.go:255] Factory "raw" can handle container "/system.slice/unattended-upgrades.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636667  199956 manager.go:925] ignoring container "/system.slice/unattended-upgrades.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636670  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-resolved.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636673  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-resolved.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636676  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-resolved.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636680  199956 manager.go:925] ignoring container "/system.slice/systemd-resolved.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636683  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/NetworkManager-wait-online.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636689  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/NetworkManager-wait-online.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636693  199956 factory.go:255] Factory "raw" can handle container "/system.slice/NetworkManager-wait-online.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636697  199956 manager.go:925] ignoring container "/system.slice/NetworkManager-wait-online.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636700  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636702  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636705  199956 factory.go:255] Factory "raw" can handle container "/user.slice", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636709  199956 manager.go:925] ignoring container "/user.slice"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636712  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-sysctl.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636714  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-sysctl.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636718  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-sysctl.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:41:27.639538430-05:00" level=info msg="TearDown network for sandbox \"c1cda931f1357d6910e39cd5c0376b33fd1d266eee14025b4fdf233198fed924\" successfully"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:41:27.639553211-05:00" level=info msg="StopPodSandbox for \"c1cda931f1357d6910e39cd5c0376b33fd1d266eee14025b4fdf233198fed924\" returns successfully"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:41:27.639715425-05:00" level=info msg="RemovePodSandbox for \"c1cda931f1357d6910e39cd5c0376b33fd1d266eee14025b4fdf233198fed924\""
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:41:27.639735632-05:00" level=info msg="Forcibly stopping sandbox \"c1cda931f1357d6910e39cd5c0376b33fd1d266eee14025b4fdf233198fed924\""
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636722  199956 manager.go:925] ignoring container "/system.slice/systemd-sysctl.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636725  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/avahi-daemon.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636728  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/avahi-daemon.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636732  199956 factory.go:255] Factory "raw" can handle container "/system.slice/avahi-daemon.socket", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636736  199956 manager.go:925] ignoring container "/system.slice/avahi-daemon.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636739  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-shm.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636743  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-shm.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636748  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-shm.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636752  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-timesyncd.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636754  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-timesyncd.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636758  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-timesyncd.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636762  199956 manager.go:925] ignoring container "/system.slice/systemd-timesyncd.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636765  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636767  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636771  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636775  199956 manager.go:925] ignoring container "/user.slice/user-0.slice"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636778  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/user@0.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636781  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/user@0.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636784  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/user@0.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636788  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/user@0.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636791  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636795  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-rootfs.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636800  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636804  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636809  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-rootfs.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636813  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636817  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-kernel-debug-tracing.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636821  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-kernel-debug-tracing.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636825  199956 manager.go:925] ignoring container "/system.slice/sys-kernel-debug-tracing.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636828  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-udevd-control.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636830  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-udevd-control.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636833  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-udevd-control.socket", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636838  199956 manager.go:925] ignoring container "/system.slice/systemd-udevd-control.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636840  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snapd.apparmor.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636843  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/snapd.apparmor.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636846  199956 factory.go:255] Factory "raw" can handle container "/system.slice/snapd.apparmor.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636851  199956 manager.go:925] ignoring container "/system.slice/snapd.apparmor.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636854  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/whoopsie.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636857  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/whoopsie.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636860  199956 factory.go:255] Factory "raw" can handle container "/system.slice/whoopsie.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636864  199956 manager.go:925] ignoring container "/system.slice/whoopsie.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636867  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e10efaa459a32161059fda75d4bcaf3bbdb896781f772b508e43fe00bc7c9003-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636871  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e10efaa459a32161059fda75d4bcaf3bbdb896781f772b508e43fe00bc7c9003-rootfs.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636876  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e10efaa459a32161059fda75d4bcaf3bbdb896781f772b508e43fe00bc7c9003-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636880  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/switcheroo-control.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636882  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/switcheroo-control.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636887  199956 factory.go:255] Factory "raw" can handle container "/system.slice/switcheroo-control.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636892  199956 manager.go:925] ignoring container "/system.slice/switcheroo-control.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636894  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/user-runtime-dir@0.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636897  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/user-runtime-dir@0.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636901  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/user-runtime-dir@0.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636905  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/user-runtime-dir@0.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636908  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/bluetooth.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636910  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/bluetooth.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636913  199956 factory.go:255] Factory "raw" can handle container "/system.slice/bluetooth.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636917  199956 manager.go:925] ignoring container "/system.slice/bluetooth.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636920  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-04e472cb\\x2db6f9\\x2d4065\\x2db509\\x2dd7e1579fc48d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dhqj8d.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636925  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-04e472cb\\x2db6f9\\x2d4065\\x2db509\\x2dd7e1579fc48d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dhqj8d.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636930  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-04e472cb\\x2db6f9\\x2d4065\\x2db509\\x2dd7e1579fc48d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dhqj8d.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636934  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/colord.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636937  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/colord.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636940  199956 factory.go:255] Factory "raw" can handle container "/system.slice/colord.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636944  199956 manager.go:925] ignoring container "/system.slice/colord.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636947  199956 factory.go:262] Factory "containerd" was unable to handle container "/docker"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636949  199956 factory.go:262] Factory "systemd" was unable to handle container "/docker"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636953  199956 factory.go:255] Factory "raw" can handle container "/docker", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636957  199956 manager.go:925] ignoring container "/docker"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636959  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636962  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636965  199956 factory.go:255] Factory "raw" can handle container "/system.slice", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636969  199956 manager.go:925] ignoring container "/system.slice"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636971  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-udevd.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636974  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-udevd.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636977  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-udevd.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636981  199956 manager.go:925] ignoring container "/system.slice/systemd-udevd.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636984  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-rfkill.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636985  199956 kuberuntime_gc.go:171] "Removing sandbox" sandboxID="c1cda931f1357d6910e39cd5c0376b33fd1d266eee14025b4fdf233198fed924"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636987  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-rfkill.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636993  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-rfkill.socket", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.636998  199956 manager.go:925] ignoring container "/system.slice/systemd-rfkill.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637000  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/uuidd.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637003  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/uuidd.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637006  199956 factory.go:255] Factory "raw" can handle container "/system.slice/uuidd.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637011  199956 manager.go:925] ignoring container "/system.slice/uuidd.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637014  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dev-mqueue.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637017  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/dev-mqueue.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637022  199956 manager.go:925] ignoring container "/system.slice/dev-mqueue.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637024  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snapd.seeded.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637027  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/snapd.seeded.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637030  199956 factory.go:255] Factory "raw" can handle container "/system.slice/snapd.seeded.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637034  199956 manager.go:925] ignoring container "/system.slice/snapd.seeded.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637037  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/console-setup.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637040  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/console-setup.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637043  199956 factory.go:255] Factory "raw" can handle container "/system.slice/console-setup.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637047  199956 manager.go:925] ignoring container "/system.slice/console-setup.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637050  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-udev-trigger.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637053  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-udev-trigger.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637056  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-udev-trigger.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637060  199956 manager.go:925] ignoring container "/system.slice/systemd-udev-trigger.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637063  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-gnome\\x2d3\\x2d38\\x2d2004-115.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637066  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-gnome\\x2d3\\x2d38\\x2d2004-115.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637070  199956 manager.go:925] ignoring container "/system.slice/snap-gnome\\x2d3\\x2d38\\x2d2004-115.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637073  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-5900ac5c1383a321a6ae837b57d6d29d7a660a102c0806210a9ea57290673062-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637078  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-5900ac5c1383a321a6ae837b57d6d29d7a660a102c0806210a9ea57290673062-rootfs.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637083  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-5900ac5c1383a321a6ae837b57d6d29d7a660a102c0806210a9ea57290673062-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637086  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637090  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-rootfs.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637095  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637099  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-1000.slice"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637102  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-1000.slice"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637105  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-1000.slice", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637110  199956 manager.go:925] ignoring container "/user.slice/user-1000.slice"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637113  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-systemd\\x2dfsck.slice/systemd-fsck@dev-disk-by\\x2duuid-4B5A\\x2dDC89.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637117  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-systemd\\x2dfsck.slice/systemd-fsck@dev-disk-by\\x2duuid-4B5A\\x2dDC89.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637120  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-systemd\\x2dfsck.slice/systemd-fsck@dev-disk-by\\x2duuid-4B5A\\x2dDC89.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637125  199956 manager.go:925] ignoring container "/system.slice/system-systemd\\x2dfsck.slice/systemd-fsck@dev-disk-by\\x2duuid-4B5A\\x2dDC89.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637128  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/accounts-daemon.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637131  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/accounts-daemon.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637134  199956 factory.go:255] Factory "raw" can handle container "/system.slice/accounts-daemon.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637138  199956 manager.go:925] ignoring container "/system.slice/accounts-daemon.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637141  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-shm.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637145  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-shm.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637150  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-shm.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637153  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/cups.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637156  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/cups.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637159  199956 factory.go:255] Factory "raw" can handle container "/system.slice/cups.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637163  199956 manager.go:925] ignoring container "/system.slice/cups.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637167  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/session-44.scope"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637170  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/session-44.scope"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637173  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/session-44.scope", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637178  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/session-44.scope"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637181  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dm-event.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637183  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/dm-event.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637186  199956 factory.go:255] Factory "raw" can handle container "/system.slice/dm-event.socket", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637190  199956 manager.go:925] ignoring container "/system.slice/dm-event.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637194  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-shm.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637198  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-shm.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637203  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-shm.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637206  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/NetworkManager.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637209  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/NetworkManager.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637213  199956 factory.go:255] Factory "raw" can handle container "/system.slice/NetworkManager.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637217  199956 manager.go:925] ignoring container "/system.slice/NetworkManager.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637220  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-logind.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637223  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-logind.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637227  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-logind.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637231  199956 manager.go:925] ignoring container "/system.slice/systemd-logind.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637233  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/polkit.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637236  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/polkit.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637239  199956 factory.go:255] Factory "raw" can handle container "/system.slice/polkit.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637243  199956 manager.go:925] ignoring container "/system.slice/polkit.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637246  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/kerneloops.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637249  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/kerneloops.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637252  199956 factory.go:255] Factory "raw" can handle container "/system.slice/kerneloops.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637256  199956 manager.go:925] ignoring container "/system.slice/kerneloops.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637258  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snapd.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637261  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/snapd.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637264  199956 factory.go:255] Factory "raw" can handle container "/system.slice/snapd.socket", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637268  199956 manager.go:925] ignoring container "/system.slice/snapd.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637271  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-user-0.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637274  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-user-0.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637278  199956 manager.go:925] ignoring container "/system.slice/run-user-0.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637281  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-lvm2\\x2dpvscan.slice"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637285  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-lvm2\\x2dpvscan.slice"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637288  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-lvm2\\x2dpvscan.slice", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637292  199956 manager.go:925] ignoring container "/system.slice/system-lvm2\\x2dpvscan.slice"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637295  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-d2688d45\\x2d2487\\x2d46e7\\x2daecb\\x2de3479626909d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d4pnfq.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637300  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-d2688d45\\x2d2487\\x2d46e7\\x2daecb\\x2de3479626909d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d4pnfq.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:41:27.641953817-05:00" level=info msg="TearDown network for sandbox \"c1cda931f1357d6910e39cd5c0376b33fd1d266eee14025b4fdf233198fed924\" successfully"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637305  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-d2688d45\\x2d2487\\x2d46e7\\x2daecb\\x2de3479626909d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d4pnfq.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637310  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/lvm2-monitor.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637313  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/lvm2-monitor.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637316  199956 factory.go:255] Factory "raw" can handle container "/system.slice/lvm2-monitor.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637320  199956 manager.go:925] ignoring container "/system.slice/lvm2-monitor.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637323  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/acpid.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637326  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/acpid.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637329  199956 factory.go:255] Factory "raw" can handle container "/system.slice/acpid.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637334  199956 manager.go:925] ignoring container "/system.slice/acpid.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637337  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/setvtrgb.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637339  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/setvtrgb.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637342  199956 factory.go:255] Factory "raw" can handle container "/system.slice/setvtrgb.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637346  199956 manager.go:925] ignoring container "/system.slice/setvtrgb.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637349  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/user@0.service/dbus.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637352  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/user@0.service/dbus.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637355  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/user@0.service/dbus.socket", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637359  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/user@0.service/dbus.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637362  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-1000.slice/user@1000.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637365  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-1000.slice/user@1000.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637368  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-1000.slice/user@1000.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637372  199956 manager.go:925] ignoring container "/user.slice/user-1000.slice/user@1000.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637375  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-1000.slice/user-runtime-dir@1000.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637378  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-1000.slice/user-runtime-dir@1000.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637381  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-1000.slice/user-runtime-dir@1000.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637385  199956 manager.go:925] ignoring container "/user.slice/user-1000.slice/user-runtime-dir@1000.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637388  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e0c5b991f81d5128566686552e45a9bbc8c584e4f6c94909edb3a42720dacc90-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637392  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e0c5b991f81d5128566686552e45a9bbc8c584e4f6c94909edb3a42720dacc90-rootfs.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637397  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e0c5b991f81d5128566686552e45a9bbc8c584e4f6c94909edb3a42720dacc90-rootfs.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637401  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/containerd.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637404  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/containerd.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637407  199956 factory.go:255] Factory "raw" can handle container "/system.slice/containerd.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637411  199956 manager.go:925] ignoring container "/system.slice/containerd.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637414  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-shm.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637418  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-shm.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637423  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-shm.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637426  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-getty.slice"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637430  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-getty.slice"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637433  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-getty.slice", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637438  199956 manager.go:925] ignoring container "/system.slice/system-getty.slice"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637440  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-kernel-tracing.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637444  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-kernel-tracing.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637448  199956 manager.go:925] ignoring container "/system.slice/sys-kernel-tracing.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637451  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/cups.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637454  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/cups.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637457  199956 factory.go:255] Factory "raw" can handle container "/system.slice/cups.socket", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637461  199956 manager.go:925] ignoring container "/system.slice/cups.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637463  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/avahi-daemon.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637466  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/avahi-daemon.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637469  199956 factory.go:255] Factory "raw" can handle container "/system.slice/avahi-daemon.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637473  199956 manager.go:925] ignoring container "/system.slice/avahi-daemon.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637476  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-shm.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637480  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-shm.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637485  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-shm.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637489  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2d719b045b\\x2df019\\x2d025c\\x2d185d\\x2da976163f375a.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637493  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2d719b045b\\x2df019\\x2d025c\\x2d185d\\x2da976163f375a.mount", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637498  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2d719b045b\\x2df019\\x2d025c\\x2d185d\\x2da976163f375a.mount"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637501  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/session-40.scope"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637504  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/session-40.scope"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637507  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/session-40.scope", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637511  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/session-40.scope"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637514  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/networkd-dispatcher.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637517  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/networkd-dispatcher.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637520  199956 factory.go:255] Factory "raw" can handle container "/system.slice/networkd-dispatcher.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637524  199956 manager.go:925] ignoring container "/system.slice/networkd-dispatcher.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637527  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-modprobe.slice"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637529  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-modprobe.slice"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637533  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-modprobe.slice", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637537  199956 manager.go:925] ignoring container "/system.slice/system-modprobe.slice"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637539  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-tmpfiles-setup-dev.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637542  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-tmpfiles-setup-dev.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637545  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-tmpfiles-setup-dev.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637549  199956 manager.go:925] ignoring container "/system.slice/systemd-tmpfiles-setup-dev.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637555  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journald-audit.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637558  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journald-audit.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637561  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journald-audit.socket", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637565  199956 manager.go:925] ignoring container "/system.slice/systemd-journald-audit.socket"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637568  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/rsyslog.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637571  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/rsyslog.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637574  199956 factory.go:255] Factory "raw" can handle container "/system.slice/rsyslog.service", but ignoring.
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.637578  199956 manager.go:925] ignoring container "/system.slice/rsyslog.service"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.641047  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.643591  199956 qos_container_manager_linux.go:379] "Updated QoS cgroup configuration"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:41:27.644484400-05:00" level=info msg="RemovePodSandbox \"c1cda931f1357d6910e39cd5c0376b33fd1d266eee14025b4fdf233198fed924\" returns successfully"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.644576  199956 kuberuntime_gc.go:171] "Removing sandbox" sandboxID="5655e090fc268963cfeaccefa7a6cc076cad2e493254730cf3df7011dab14334"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:41:27.644680409-05:00" level=info msg="StopPodSandbox for \"5655e090fc268963cfeaccefa7a6cc076cad2e493254730cf3df7011dab14334\""
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:41:27.646874650-05:00" level=info msg="TearDown network for sandbox \"5655e090fc268963cfeaccefa7a6cc076cad2e493254730cf3df7011dab14334\" successfully"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:41:27.646889695-05:00" level=info msg="StopPodSandbox for \"5655e090fc268963cfeaccefa7a6cc076cad2e493254730cf3df7011dab14334\" returns successfully"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:41:27.647051060-05:00" level=info msg="RemovePodSandbox for \"5655e090fc268963cfeaccefa7a6cc076cad2e493254730cf3df7011dab14334\""
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:41:27.647068059-05:00" level=info msg="Forcibly stopping sandbox \"5655e090fc268963cfeaccefa7a6cc076cad2e493254730cf3df7011dab14334\""
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:41:27.649212775-05:00" level=info msg="TearDown network for sandbox \"5655e090fc268963cfeaccefa7a6cc076cad2e493254730cf3df7011dab14334\" successfully"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:41:27.651643363-05:00" level=info msg="RemovePodSandbox \"5655e090fc268963cfeaccefa7a6cc076cad2e493254730cf3df7011dab14334\" returns successfully"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.651788  199956 kuberuntime_gc.go:343] "Removing pod logs" podUID=3152a9ab-0783-4c02-a348-14eaa946402c
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.651993  199956 kubelet.go:1280] "Container garbage collection succeeded"
Jan 20 12:41:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:27.794046  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:41:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:28.285516  199956 generic.go:155] "GenericPLEG" podUID=3152a9ab-0783-4c02-a348-14eaa946402c containerID="5655e090fc268963cfeaccefa7a6cc076cad2e493254730cf3df7011dab14334" oldState=exited newState=non-existent
Jan 20 12:41:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:28.285570  199956 generic.go:155] "GenericPLEG" podUID=3152a9ab-0783-4c02-a348-14eaa946402c containerID="c1cda931f1357d6910e39cd5c0376b33fd1d266eee14025b4fdf233198fed924" oldState=exited newState=non-existent
Jan 20 12:41:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:28.285609  199956 generic.go:399] "PLEG: Delete status for pod" podUID="3152a9ab-0783-4c02-a348-14eaa946402c"
Jan 20 12:41:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:28.484214  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:28.484280  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:28.492649  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[ce67ad9f-998a-470f-9dca-e593d005ca57] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:28 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001300ce0 2 [] false false map[] 0xc0014c6200 0xc000de6160}
Jan 20 12:41:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:28.492679  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:28.879645  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:41:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:28.879714  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:28.880935  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:28 GMT] X-Content-Type-Options:[nosniff]] 0xc000320940 2 [] true false map[] 0xc001716100 <nil>}
Jan 20 12:41:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:28.881047  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.355423  199956 reflector.go:536] object-"kube-system"/"coredns": Watch close - *v1.ConfigMap total 0 items received
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.483773  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.483836  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.491189  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[515a8899-cbfd-4d50-8006-859a116e3e89] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:29 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001820840 2 [] false false map[] 0xc001716300 0xc0013b0160}
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.491220  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.581716  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[confidential-containers-system/cc-operator-pre-install-daemon-qjplj]
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.581807  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.581991  199956 pod_workers.go:888] "Processing pod event" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" podUID=b0713fbc-efc5-4044-9d08-2326a0752f87 updateType=0
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.582075  199956 kubelet.go:1501] "syncPod enter" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" podUID=b0713fbc-efc5-4044-9d08-2326a0752f87
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.582113  199956 kubelet_pods.go:1435] "Generating pod status" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj"
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.582203  199956 kubelet_pods.go:1447] "Got phase for pod" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" oldPhase=Running phase=Running
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.582493  199956 status_manager.go:535] "Ignoring same status for pod" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:01 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:05 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:05 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:01 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.5 PodIPs:[{IP:10.244.0.5}] StartTime:2023-01-20 12:32:01 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:cc-runtime-pre-install-pod State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:32:03 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:quay.io/confidential-containers/container-engine-for-cc-payload:1034f9fcf947b22eea080a6f77d8e164e2369849 ImageID:quay.io/confidential-containers/container-engine-for-cc-payload@sha256:f86f078b3a47026a066e65c7d836d9b9a43bf177555c276624d90f42e50279a1 ContainerID:containerd://e0c5b991f81d5128566686552e45a9bbc8c584e4f6c94909edb3a42720dacc90 Started:0xc00108f08e}] QOSClass:BestEffort EphemeralContainerStatuses:[]}
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.582869  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj"
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.582946  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj"
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.583345  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj"
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.583513  199956 kubelet.go:1503] "syncPod exit" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" podUID=b0713fbc-efc5-4044-9d08-2326a0752f87 isTerminal=false
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.583570  199956 pod_workers.go:988] "Processing pod event done" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" podUID=b0713fbc-efc5-4044-9d08-2326a0752f87 updateType=0
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.589186  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.589247  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.589279  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.591512  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.591733  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.591768  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.591813  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.642947  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" volumeName="confidential-containers-artifacts" volumeSpecName="confidential-containers-artifacts"
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.643058  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" volumeName="etc-systemd-system" volumeSpecName="etc-systemd-system"
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.643134  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" volumeName="dbus" volumeSpecName="dbus"
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.643208  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" volumeName="systemd" volumeSpecName="systemd"
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.643332  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" volumeName="kube-api-access-gcgm6" volumeSpecName="kube-api-access-gcgm6"
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.654116  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-gcgm6\" (UniqueName: \"kubernetes.io/projected/b0713fbc-efc5-4044-9d08-2326a0752f87-kube-api-access-gcgm6\") pod \"cc-operator-pre-install-daemon-qjplj\" (UID: \"b0713fbc-efc5-4044-9d08-2326a0752f87\") Volume is already mounted to pod, but remount was requested." pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj"
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.654302  199956 projected.go:183] Setting up volume kube-api-access-gcgm6 for pod b0713fbc-efc5-4044-9d08-2326a0752f87 at /var/lib/kubelet/pods/b0713fbc-efc5-4044-9d08-2326a0752f87/volumes/kubernetes.io~projected/kube-api-access-gcgm6
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.654726  199956 atomic_writer.go:161] pod confidential-containers-system/cc-operator-pre-install-daemon-qjplj volume kube-api-access-gcgm6: no update required for target directory /var/lib/kubelet/pods/b0713fbc-efc5-4044-9d08-2326a0752f87/volumes/kubernetes.io~projected/kube-api-access-gcgm6
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.654799  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-gcgm6\" (UniqueName: \"kubernetes.io/projected/b0713fbc-efc5-4044-9d08-2326a0752f87-kube-api-access-gcgm6\") pod \"cc-operator-pre-install-daemon-qjplj\" (UID: \"b0713fbc-efc5-4044-9d08-2326a0752f87\") " pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj"
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.774864  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.783177  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.793244  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510782" capacity="511757" time="2023-01-20 12:41:29.792921221 -0500 EST m=+602.348696045"
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.793257  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59486760Ki" capacity="65586124Ki" time="2023-01-20 12:41:29.77707604 -0500 EST m=+602.332850928"
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.793264  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64394392Ki" capacity="65061836Ki" time="2023-01-20 12:41:29.793184773 -0500 EST m=+602.348959598"
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.793270  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902012192Ki" capacity="981310056Ki" time="2023-01-20 12:41:29.77707604 -0500 EST m=+602.332850928"
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.793275  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:41:29.77707604 -0500 EST m=+602.332850928"
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.793281  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902012192Ki" capacity="981310056Ki" time="2023-01-20 12:41:21.262545966 -0500 EST"
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.793287  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:41:21.262545966 -0500 EST"
Jan 20 12:41:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:29.793313  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:41:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:30.213327  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:41:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:30.213364  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:41:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:30.213942  199956 interface.go:209] Interface eno2 is up
Jan 20 12:41:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:30.214062  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:41:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:30.214098  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:41:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:30.214122  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:41:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:30.214138  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:41:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:30.214157  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:41:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:30.215202  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:41:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:30.215249  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:41:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:30.215270  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:41:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:30.215289  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:41:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:30.483837  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:30.483912  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:30.491496  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[2c8d3a6f-3051-49b5-83c1-81af29087c94] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:30 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001487e60 2 [] false false map[] 0xc00105af00 0xc0012ccdc0}
Jan 20 12:41:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:30.491526  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:31.166163  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:41:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:31.166227  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:31.174677  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[213b5329-e324-4a9e-ac79-7cdf343482c9] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:31 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001301c80 2 [] false false map[] 0xc002141b00 0xc001580370}
Jan 20 12:41:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:31.174730  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:31.483363  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:31.483437  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:31.495744  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[d4d94de6-bf96-47e2-8d14-f4a0d28c7e69] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:31 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008e21a0 2 [] false false map[] 0xc001716900 0xc0014b2d10}
Jan 20 12:41:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:31.495882  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:31.581719  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[confidential-containers-system/cc-operator-daemon-install-t6mp7]
Jan 20 12:41:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:31.581810  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:41:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:31.581944  199956 pod_workers.go:888] "Processing pod event" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" podUID=7af065b7-9095-4d91-9b9e-2644e7b1f4bf updateType=0
Jan 20 12:41:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:31.582031  199956 kubelet.go:1501] "syncPod enter" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" podUID=7af065b7-9095-4d91-9b9e-2644e7b1f4bf
Jan 20 12:41:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:31.582070  199956 kubelet_pods.go:1435] "Generating pod status" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7"
Jan 20 12:41:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:31.582164  199956 kubelet_pods.go:1447] "Got phase for pod" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" oldPhase=Running phase=Running
Jan 20 12:41:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:31.582446  199956 status_manager.go:535] "Ignoring same status for pod" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:04 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:20 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:20 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:04 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.6 PodIPs:[{IP:10.244.0.6}] StartTime:2023-01-20 12:32:04 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:cc-runtime-install-pod State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:32:20 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:quay.io/confidential-containers/runtime-payload-ci:kata-containers-amd64 ImageID:quay.io/confidential-containers/runtime-payload-ci@sha256:4736ba274765c889404fb98f01de0a997e68d2d7e5acca2440488f0e1337032b ContainerID:containerd://e10efaa459a32161059fda75d4bcaf3bbdb896781f772b508e43fe00bc7c9003 Started:0xc0015d38f9}] QOSClass:BestEffort EphemeralContainerStatuses:[]}
Jan 20 12:41:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:31.582809  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7"
Jan 20 12:41:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:31.582882  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7"
Jan 20 12:41:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:31.583308  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="confidential-containers-system/cc-operator-daemon-install-t6mp7"
Jan 20 12:41:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:31.583471  199956 kubelet.go:1503] "syncPod exit" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" podUID=7af065b7-9095-4d91-9b9e-2644e7b1f4bf isTerminal=false
Jan 20 12:41:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:31.583528  199956 pod_workers.go:988] "Processing pod event done" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" podUID=7af065b7-9095-4d91-9b9e-2644e7b1f4bf updateType=0
Jan 20 12:41:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:31.587236  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:41:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:31.587247  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:41:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:31.587760  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:41:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:31.588130  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:41:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:31.588177  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:41:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:31.588183  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:41:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:31.588192  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:41:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:31.656526  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" volumeName="containerd-conf" volumeSpecName="containerd-conf"
Jan 20 12:41:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:31.656632  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" volumeName="kata-artifacts" volumeSpecName="kata-artifacts"
Jan 20 12:41:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:31.656734  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" volumeName="dbus" volumeSpecName="dbus"
Jan 20 12:41:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:31.656805  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" volumeName="systemd" volumeSpecName="systemd"
Jan 20 12:41:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:31.656869  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" volumeName="local-bin" volumeSpecName="local-bin"
Jan 20 12:41:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:31.656984  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" volumeName="kube-api-access-x6vjr" volumeSpecName="kube-api-access-x6vjr"
Jan 20 12:41:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:31.668157  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-x6vjr\" (UniqueName: \"kubernetes.io/projected/7af065b7-9095-4d91-9b9e-2644e7b1f4bf-kube-api-access-x6vjr\") pod \"cc-operator-daemon-install-t6mp7\" (UID: \"7af065b7-9095-4d91-9b9e-2644e7b1f4bf\") Volume is already mounted to pod, but remount was requested." pod="confidential-containers-system/cc-operator-daemon-install-t6mp7"
Jan 20 12:41:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:31.668341  199956 projected.go:183] Setting up volume kube-api-access-x6vjr for pod 7af065b7-9095-4d91-9b9e-2644e7b1f4bf at /var/lib/kubelet/pods/7af065b7-9095-4d91-9b9e-2644e7b1f4bf/volumes/kubernetes.io~projected/kube-api-access-x6vjr
Jan 20 12:41:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:31.668769  199956 atomic_writer.go:161] pod confidential-containers-system/cc-operator-daemon-install-t6mp7 volume kube-api-access-x6vjr: no update required for target directory /var/lib/kubelet/pods/7af065b7-9095-4d91-9b9e-2644e7b1f4bf/volumes/kubernetes.io~projected/kube-api-access-x6vjr
Jan 20 12:41:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:31.668841  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-x6vjr\" (UniqueName: \"kubernetes.io/projected/7af065b7-9095-4d91-9b9e-2644e7b1f4bf-kube-api-access-x6vjr\") pod \"cc-operator-daemon-install-t6mp7\" (UID: \"7af065b7-9095-4d91-9b9e-2644e7b1f4bf\") " pod="confidential-containers-system/cc-operator-daemon-install-t6mp7"
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.483717  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.483786  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.491522  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[baef89ec-c94c-4449-ab65-5dcd9abaad58] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:32 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008e2ee0 2 [] false false map[] 0xc0011c4900 0xc0016e8630}
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.491554  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.581482  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[kube-system/kube-controller-manager-zcy-z390-aorus-master]
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.581559  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce updateType=0
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.581617  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.581649  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master"
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.581731  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" oldPhase=Running phase=Running
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.582000  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:28 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:37 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:37 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:28 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.239.159.53 PodIPs:[{IP:10.239.159.53}] StartTime:2023-01-20 12:31:28 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:kube-controller-manager State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:22 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:6 Image:k8s.gcr.io/kube-controller-manager:v1.24.0 ImageID:k8s.gcr.io/kube-controller-manager@sha256:df044a154e79a18f749d3cd9d958c3edde2b6a00c815176472002b7bbf956637 ContainerID:containerd://f8443d2868215ec42d3fa335663976e33df166c5e98369aa3f9d7ddb9235bead Started:0xc0018f7509}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.582350  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master"
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.582423  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master"
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.583338  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/kube-controller-manager-zcy-z390-aorus-master"
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.583498  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce isTerminal=false
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.583553  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce updateType=0
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.664840  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="ca-certs" volumeSpecName="ca-certs"
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.664868  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.664889  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="etc-pki" volumeSpecName="etc-pki"
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.664906  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="flexvolume-dir" volumeSpecName="flexvolume-dir"
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.664923  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.664939  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.664957  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.664973  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.689390  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.689420  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.690003  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:32 GMT]] 0xc0008e2f60 2 [] true false map[] 0xc001717600 <nil>}
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.690059  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.696331  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.696389  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.697475  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:32 GMT]] 0xc00129c440 2 [] true false map[] 0xc001717800 <nil>}
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.697579  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.709811  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.709873  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.709885  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.709944  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.711045  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:32 GMT]] 0xc000ffa0a0 2 [] true false map[] 0xc001f36a00 <nil>}
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.711147  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.711092  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:32 GMT]] 0xc001719500 2 [] true false map[] 0xc001717a00 <nil>}
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.711244  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:41:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:32.795906  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:41:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:33.483770  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:33.483841  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:33.492169  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[cd171cbd-1f8c-44ad-986a-46cb6e41d219] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:33 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000cb24c0 2 [] false false map[] 0xc0011c4500 0xc0002b6160}
Jan 20 12:41:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:33.492199  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:33.582127  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:41:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:33.589499  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:41:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:33.589562  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:41:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:33.591813  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:41:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:33.593959  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:41:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:33.594181  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:41:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:33.594214  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:41:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:33.594255  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:41:34 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:34.483643  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:34 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:34.483675  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:34 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:34.489965  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[97e16c89-5c24-4459-8f0a-2a0e7a701017] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:34 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008e2300 2 [] false false map[] 0xc001716100 0xc00112e4d0}
Jan 20 12:41:34 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:34.490038  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:35.483617  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:35.483636  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:35.486247  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[9246ded5-2951-4af8-9350-1b1bda9b2ecb] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:35 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008e27e0 2 [] false false map[] 0xc001716300 0xc000d4a0b0}
Jan 20 12:41:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:35.486303  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:35.581548  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:41:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:35.588904  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:41:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:35.588967  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:41:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:35.588999  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:41:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:35.591122  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:41:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:35.591343  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:41:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:35.591379  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:41:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:35.591423  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:41:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:36.164499  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:41:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:36.164573  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:36.171727  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:36 GMT] X-Content-Type-Options:[nosniff]] 0xc0017198a0 2 [] false false map[] 0xc001716700 0xc000f0a210}
Jan 20 12:41:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:36.171754  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:41:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:36.271561  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:41:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:36.271625  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:36.272785  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:36 GMT]] 0xc000ffbce0 29 [] true false map[] 0xc001716900 <nil>}
Jan 20 12:41:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:36.272884  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:41:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:36.483778  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:36.483808  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:36.490390  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[4af8947d-47b9-4294-a72c-f061de514ccf] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:36 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ffbd60 2 [] false false map[] 0xc001f37100 0xc0011b9c30}
Jan 20 12:41:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:36.490467  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:36.895652  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:41:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:36.895719  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:36.902938  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:36 GMT] X-Content-Type-Options:[nosniff]] 0xc001719d20 2 [] false false map[] 0xc001717400 0xc001345a20}
Jan 20 12:41:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:36.902962  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:41:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:37.483981  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:37.484013  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:37.488905  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[2f1d63ed-b6c9-477a-9524-611b44fbba9a] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:37 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0003211a0 2 [] false false map[] 0xc001f37300 0xc001547d90}
Jan 20 12:41:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:37.488939  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:37.581444  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:41:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:37.587178  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:41:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:37.587192  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:41:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:37.587727  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:41:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:37.588103  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:41:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:37.588150  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:41:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:37.588156  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:41:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:37.588165  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:41:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:37.796997  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:41:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:37.820949  199956 handler.go:293] error while reading "/proc/199956/fd/34" link: readlink /proc/199956/fd/34: no such file or directory
Jan 20 12:41:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:38.483819  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:38.483884  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:38.491507  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[b710a1ca-b249-4bd6-af5e-3b21542b1a04] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:38 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000321d40 2 [] false false map[] 0xc000339a00 0xc00180b550}
Jan 20 12:41:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:38.491539  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:38.878922  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:41:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:38.878986  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:38.879031  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/healthz" timeout="1s"
Jan 20 12:41:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:38.879095  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:38.880120  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:38 GMT] X-Content-Type-Options:[nosniff]] 0xc0009a0200 2 [] true false map[] 0xc0014c6900 <nil>}
Jan 20 12:41:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:38.880153  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:38 GMT] X-Content-Type-Options:[nosniff]] 0xc000cb2580 2 [] true false map[] 0xc001f37a00 <nil>}
Jan 20 12:41:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:38.880239  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:41:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:38.880260  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:41:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:39.483569  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:39.483599  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:39.490248  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[3a526af0-5b35-4acd-9a59-9dbb51c6e2c5] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:39 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000707c40 2 [] false false map[] 0xc001f37c00 0xc001893ad0}
Jan 20 12:41:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:39.490317  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:39.582078  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:41:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:39.588146  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:41:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:39.588159  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:41:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:39.588755  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:41:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:39.589096  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:41:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:39.589143  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:41:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:39.589150  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:41:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:39.589159  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:41:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:39.794344  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:41:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:39.812383  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:41:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:39.824301  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902012140Ki" capacity="981310056Ki" time="2023-01-20 12:41:31.262429988 -0500 EST"
Jan 20 12:41:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:39.824313  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:41:31.262429988 -0500 EST"
Jan 20 12:41:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:39.824319  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510782" capacity="511757" time="2023-01-20 12:41:39.824016208 -0500 EST m=+612.379791031"
Jan 20 12:41:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:39.824324  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59486196Ki" capacity="65586124Ki" time="2023-01-20 12:41:39.796452327 -0500 EST m=+612.352227218"
Jan 20 12:41:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:39.824329  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64394276Ki" capacity="65061836Ki" time="2023-01-20 12:41:39.824250689 -0500 EST m=+612.380025513"
Jan 20 12:41:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:39.824334  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902012140Ki" capacity="981310056Ki" time="2023-01-20 12:41:39.796452327 -0500 EST m=+612.352227218"
Jan 20 12:41:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:39.824339  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:41:39.796452327 -0500 EST m=+612.352227218"
Jan 20 12:41:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:39.824362  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.483793  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.483859  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.497294  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[36591a30-a581-475b-83e8-5d1958f8b9aa] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:40 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000cb3020 2 [] false false map[] 0xc0011c4800 0xc0002b6370}
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.497440  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.581986  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[kube-system/coredns-6d4b75cb6d-48zl2]
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.582063  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 updateType=0
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.582120  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.582163  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.582289  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/coredns-6d4b75cb6d-48zl2" oldPhase=Running phase=Running
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.582592  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/coredns-6d4b75cb6d-48zl2" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:45 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:45 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.2 PodIPs:[{IP:10.244.0.2}] StartTime:2023-01-20 12:31:42 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:coredns State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:43 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:k8s.gcr.io/coredns/coredns:v1.8.6 ImageID:k8s.gcr.io/coredns/coredns@sha256:5b6ec0d6de9baaf3e92d0f66cd96a25b9edbce8716f5f15dcd1a616b3abd590e ContainerID:containerd://b3af3b285c950aba4fcd0176473261f7f4700aeaafe9c73ae15b15a7d6304092 Started:0xc00125e539}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.582918  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.582998  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.583492  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.583669  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 isTerminal=false
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.583723  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 updateType=0
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.619791  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.619799  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.620011  199956 interface.go:209] Interface eno2 is up
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.620037  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.620045  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.620050  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.620053  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.620057  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.620470  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.620479  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.620483  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.620486  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.624919  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/coredns-6d4b75cb6d-48zl2" volumeName="config-volume" volumeSpecName="config-volume"
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.624954  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/coredns-6d4b75cb6d-48zl2" volumeName="kube-api-access-9qh7j" volumeSpecName="kube-api-access-9qh7j"
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.632847  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-9qh7j\" (UniqueName: \"kubernetes.io/projected/d94e1927-d22d-4831-a764-0170eae346a1-kube-api-access-9qh7j\") pod \"coredns-6d4b75cb6d-48zl2\" (UID: \"d94e1927-d22d-4831-a764-0170eae346a1\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.632878  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/d94e1927-d22d-4831-a764-0170eae346a1-config-volume\") pod \"coredns-6d4b75cb6d-48zl2\" (UID: \"d94e1927-d22d-4831-a764-0170eae346a1\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.632914  199956 configmap.go:181] Setting up volume config-volume for pod d94e1927-d22d-4831-a764-0170eae346a1 at /var/lib/kubelet/pods/d94e1927-d22d-4831-a764-0170eae346a1/volumes/kubernetes.io~configmap/config-volume
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.632924  199956 projected.go:183] Setting up volume kube-api-access-9qh7j for pod d94e1927-d22d-4831-a764-0170eae346a1 at /var/lib/kubelet/pods/d94e1927-d22d-4831-a764-0170eae346a1/volumes/kubernetes.io~projected/kube-api-access-9qh7j
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.632929  199956 configmap.go:205] Received configMap kube-system/coredns containing (1) pieces of data, 339 total bytes
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.632953  199956 quota_linux.go:271] SupportsQuotas called, but quotas disabled
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.633013  199956 atomic_writer.go:161] pod kube-system/coredns-6d4b75cb6d-48zl2 volume config-volume: no update required for target directory /var/lib/kubelet/pods/d94e1927-d22d-4831-a764-0170eae346a1/volumes/kubernetes.io~configmap/config-volume
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.633029  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/d94e1927-d22d-4831-a764-0170eae346a1-config-volume\") pod \"coredns-6d4b75cb6d-48zl2\" (UID: \"d94e1927-d22d-4831-a764-0170eae346a1\") " pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.633034  199956 atomic_writer.go:161] pod kube-system/coredns-6d4b75cb6d-48zl2 volume kube-api-access-9qh7j: no update required for target directory /var/lib/kubelet/pods/d94e1927-d22d-4831-a764-0170eae346a1/volumes/kubernetes.io~projected/kube-api-access-9qh7j
Jan 20 12:41:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:40.633051  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-9qh7j\" (UniqueName: \"kubernetes.io/projected/d94e1927-d22d-4831-a764-0170eae346a1-kube-api-access-9qh7j\") pod \"coredns-6d4b75cb6d-48zl2\" (UID: \"d94e1927-d22d-4831-a764-0170eae346a1\") " pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:41:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:41.166393  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:41:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:41.166462  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:41.174590  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[fca9ee36-449d-403c-84d8-0a3e27ed5b53] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:41 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123edc0 2 [] false false map[] 0xc0014c7600 0xc000fd8e70}
Jan 20 12:41:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:41.174651  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:41.483666  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:41.483731  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:41.491363  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[954c81a7-9a67-4e9b-ab1c-fd121099e7b8] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:41 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008dd320 2 [] false false map[] 0xc0011c4800 0xc001982630}
Jan 20 12:41:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:41.491393  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:41.582222  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:41:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:41.589672  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:41:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:41.589736  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:41:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:41.592080  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:41:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:41.594138  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:41:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:41.594356  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:41:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:41.594390  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:41:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:41.594433  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:41:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:42.483125  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:42.483200  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:42.491283  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[50735af4-075e-44b1-8c49-62ba457c5f26] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:42 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00129ca20 2 [] false false map[] 0xc0014c7a00 0xc0002b6840}
Jan 20 12:41:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:42.491315  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:42.688967  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:41:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:42.689027  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:42.690185  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:42 GMT]] 0xc0008ddc20 2 [] true false map[] 0xc0000dd000 <nil>}
Jan 20 12:41:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:42.690293  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:41:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:42.695417  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:41:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:42.695477  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:42.696544  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:42 GMT]] 0xc00129cb80 2 [] true false map[] 0xc001846e00 <nil>}
Jan 20 12:41:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:42.696649  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:41:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:42.709610  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:41:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:42.709651  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:41:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:42.709669  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:42.709710  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:42.710781  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:42 GMT]] 0xc00123fee0 2 [] true false map[] 0xc0000dd400 <nil>}
Jan 20 12:41:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:42.710837  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:42 GMT]] 0xc0008ddd40 2 [] true false map[] 0xc001847000 <nil>}
Jan 20 12:41:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:42.710886  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:41:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:42.710933  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:41:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:42.798437  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:41:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:43.483919  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:43.483989  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:43.492416  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[1310a6b2-5d9b-4d89-b56d-82792dc0fc94] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:43 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008e2740 2 [] false false map[] 0xc001847200 0xc0022c1b80}
Jan 20 12:41:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:43.492443  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:43.581208  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:41:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:43.587156  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:41:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:43.587168  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:41:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:43.587175  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:41:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:43.587718  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:41:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:43.587765  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:41:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:43.587771  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:41:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:43.587779  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:41:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:44.483809  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:44.483880  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:44.497038  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[8b11dfea-8020-4efd-8645-c3866f43ebef] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:44 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0017195a0 2 [] false false map[] 0xc001775e00 0xc00110dc30}
Jan 20 12:41:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:44.497171  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:45.484116  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:45.484183  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:45.492656  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[e001b082-39ea-4cc0-a696-df4ca219e931] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:45 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001702040 2 [] false false map[] 0xc001716200 0xc00153b550}
Jan 20 12:41:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:45.492696  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:45.581918  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:41:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:45.587287  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:41:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:45.587300  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:41:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:45.588083  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:41:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:45.588433  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:41:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:45.588478  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:41:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:45.588485  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:41:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:45.588494  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:41:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:46.164210  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:41:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:46.164231  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:46.166733  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:46 GMT] X-Content-Type-Options:[nosniff]] 0xc001702180 2 [] false false map[] 0xc000cf7c00 0xc001801550}
Jan 20 12:41:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:46.166769  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:41:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:46.272511  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:41:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:46.272575  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:46.273804  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:46 GMT]] 0xc001702200 29 [] true false map[] 0xc000cf7e00 <nil>}
Jan 20 12:41:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:46.273910  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:41:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:46.483068  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:46.483098  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:46.489576  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[31185eb6-9f5a-4d1f-bf55-73331b59bfe8] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:46 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001702640 2 [] false false map[] 0xc00105a500 0xc0018016b0}
Jan 20 12:41:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:46.489645  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:46.895189  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:41:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:46.895230  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:46.903952  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:46 GMT] X-Content-Type-Options:[nosniff]] 0xc0017029c0 2 [] false false map[] 0xc000338d00 0xc0017631e0}
Jan 20 12:41:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:46.904079  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:41:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:47.483534  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:47.483601  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:47.491561  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[d983a7e7-ffe8-4339-8a4c-ab6566910e07] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:47 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000320fe0 2 [] false false map[] 0xc000339000 0xc0019c93f0}
Jan 20 12:41:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:47.491591  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:47.526178  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/etcd.yaml"
Jan 20 12:41:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:47.528280  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver.yaml"
Jan 20 12:41:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:47.530902  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Jan 20 12:41:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:47.531434  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-scheduler.yaml"
Jan 20 12:41:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:47.531761  199956 config.go:279] "Setting pods for source" source="file"
Jan 20 12:41:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:47.581733  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:41:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:47.588497  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:41:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:47.588516  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:41:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:47.589074  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:41:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:47.589441  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:41:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:47.589489  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:41:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:47.589496  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:41:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:47.589505  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:41:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:47.799505  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:41:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:48.484046  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:48.484114  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:48.492378  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[5a4f053c-22bb-4563-8289-60c31a939ac1] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:48 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001820280 2 [] false false map[] 0xc002141c00 0xc0019824d0}
Jan 20 12:41:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:48.492407  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:48.879385  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:41:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:48.879449  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:48.880680  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:48 GMT] X-Content-Type-Options:[nosniff]] 0xc0008dcf80 2 [] true false map[] 0xc002141f00 <nil>}
Jan 20 12:41:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:48.880829  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:41:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:49.483154  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:49.483220  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:49.491681  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[14acb8e1-4009-41c6-8fb3-a2f90c5a4b50] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:49 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008dd280 2 [] false false map[] 0xc0000dd300 0xc0006d4000}
Jan 20 12:41:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:49.491710  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:49.582034  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:41:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:49.588148  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:41:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:49.588161  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:41:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:49.588168  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:41:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:49.588909  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:41:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:49.588955  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:41:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:49.588961  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:41:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:49.588970  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:41:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:49.825241  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:41:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:49.838098  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:41:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:49.847525  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64393888Ki" capacity="65061836Ki" time="2023-01-20 12:41:49.847473884 -0500 EST m=+622.403248708"
Jan 20 12:41:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:49.847538  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902012104Ki" capacity="981310056Ki" time="2023-01-20 12:41:49.827265838 -0500 EST m=+622.383040729"
Jan 20 12:41:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:49.847545  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:41:49.827265838 -0500 EST m=+622.383040729"
Jan 20 12:41:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:49.847552  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902012104Ki" capacity="981310056Ki" time="2023-01-20 12:41:41.260501586 -0500 EST"
Jan 20 12:41:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:49.847557  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:41:41.260501586 -0500 EST"
Jan 20 12:41:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:49.847562  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510782" capacity="511757" time="2023-01-20 12:41:49.847212579 -0500 EST m=+622.402987404"
Jan 20 12:41:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:49.847568  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59477468Ki" capacity="65586124Ki" time="2023-01-20 12:41:49.827265838 -0500 EST m=+622.383040729"
Jan 20 12:41:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:49.847594  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:41:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:50.483729  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:50.483799  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:50.491466  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[b582d28c-ff86-4073-a17a-cec5ab96daf0] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:50 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001820f20 2 [] false false map[] 0xc0014c7200 0xc0014fc790}
Jan 20 12:41:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:50.491497  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:51.019168  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:41:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:51.019177  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:41:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:51.019345  199956 interface.go:209] Interface eno2 is up
Jan 20 12:41:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:51.019371  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:41:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:51.019378  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:41:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:51.019383  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:41:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:51.019386  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:41:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:51.019405  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:41:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:51.019669  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:41:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:51.019679  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:41:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:51.019684  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:41:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:51.019688  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:41:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:51.166085  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:41:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:51.166148  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:51.178955  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[ba36ce1d-2f6a-4a30-9473-fdd8d8144e62] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:51 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001718000 2 [] false false map[] 0xc0014c7900 0xc0013f16b0}
Jan 20 12:41:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:51.179096  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:51.483911  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:51.483976  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:51.497187  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[825055b9-a0fc-43ca-87e5-da8b31f8502e] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:51 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0018219c0 2 [] false false map[] 0xc001774000 0xc00187b600}
Jan 20 12:41:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:51.497318  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:51.581911  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:41:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:51.584155  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:41:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:51.584175  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:41:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:51.584890  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:41:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:51.585400  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:41:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:51.585474  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:41:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:51.585484  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:41:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:51.585498  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:41:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:52.042456  199956 handler.go:293] error while reading "/proc/199956/fd/34" link: readlink /proc/199956/fd/34: no such file or directory
Jan 20 12:41:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:52.484145  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:52.484213  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:52.492380  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[98159a0f-143c-46b9-b0a0-1ed2b436e3f3] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:52 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00129d3a0 2 [] false false map[] 0xc001774300 0xc00189d6b0}
Jan 20 12:41:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:52.492410  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:52.689834  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:41:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:52.689904  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:52.690997  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:52 GMT]] 0xc0008e36c0 2 [] true false map[] 0xc001774600 <nil>}
Jan 20 12:41:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:52.691101  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:41:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:52.696332  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:41:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:52.696399  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:52.697493  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:52 GMT]] 0xc00129d6e0 2 [] true false map[] 0xc0011c4800 <nil>}
Jan 20 12:41:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:52.697595  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:41:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:52.709903  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:41:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:52.709968  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:52.709981  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:41:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:52.710042  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:52.711260  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:52 GMT]] 0xc00123e6e0 2 [] true false map[] 0xc0002c3600 <nil>}
Jan 20 12:41:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:52.711300  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:52 GMT]] 0xc00129d7e0 2 [] true false map[] 0xc001774800 <nil>}
Jan 20 12:41:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:52.711368  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:41:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:52.711397  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:41:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:52.800771  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:41:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:53.484009  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:53.484076  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:53.492634  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[181c30ea-6069-4611-b489-6e97d2378309] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:53 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000320200 2 [] false false map[] 0xc0002c3800 0xc001e87600}
Jan 20 12:41:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:53.492663  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:53.581497  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:41:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:53.587257  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:41:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:53.587271  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:41:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:53.588029  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:41:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:53.588546  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:41:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:53.588594  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:41:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:53.588602  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:41:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:53.588610  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:41:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:54.484211  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:54.484281  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:54.492499  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[f147039f-0c3b-41ed-89f0-a987db8387e5] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:54 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000320ee0 2 [] false false map[] 0xc0016aa400 0xc001ed3290}
Jan 20 12:41:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:54.492530  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:55.484069  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:55.484139  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:55.498654  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[93793317-c2f8-4eb5-80dc-45790403891d] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:55 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000706280 2 [] false false map[] 0xc000338600 0xc000d42160}
Jan 20 12:41:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:55.498808  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:55.581678  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:41:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:55.589011  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:41:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:55.589079  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:41:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:55.591298  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:41:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:55.593366  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:41:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:55.593590  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:41:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:55.593627  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:41:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:55.593671  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.164568  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.164642  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.172122  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:56 GMT] X-Content-Type-Options:[nosniff]] 0xc000707580 2 [] false false map[] 0xc000338e00 0xc001982630}
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.172148  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.272303  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.272366  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.273610  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:56 GMT]] 0xc0003206c0 29 [] true false map[] 0xc0016aa100 <nil>}
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.273726  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.484214  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.484283  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.492532  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[9cd7d9c0-cf14-4468-b5b1-accee22bc68a] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:56 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000707ae0 2 [] false false map[] 0xc0016aa400 0xc0002b6210}
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.492560  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.581514  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[kube-system/kube-proxy-prhfv]
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.581595  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/kube-proxy-prhfv" podUID=1c057a0e-3ee3-44f3-b294-434acc8f9491 updateType=0
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.581654  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/kube-proxy-prhfv" podUID=1c057a0e-3ee3-44f3-b294-434acc8f9491
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.581683  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/kube-proxy-prhfv"
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.581766  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/kube-proxy-prhfv" oldPhase=Running phase=Running
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.582051  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/kube-proxy-prhfv" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:43 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:43 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.239.159.53 PodIPs:[{IP:10.239.159.53}] StartTime:2023-01-20 12:31:42 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:kube-proxy State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:43 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:k8s.gcr.io/kube-proxy:v1.24.0 ImageID:k8s.gcr.io/kube-proxy@sha256:c957d602267fa61082ab8847914b2118955d0739d592cc7b01e278513478d6a8 ContainerID:containerd://f78ed441ff85d669a403a76c0987f2d86913431bd96d454b1a3605bdcf0474f2 Started:0xc001d3b159}] QOSClass:BestEffort EphemeralContainerStatuses:[]}
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.582367  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-proxy-prhfv"
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.582439  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/kube-proxy-prhfv"
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.582862  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/kube-proxy-prhfv"
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.583045  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/kube-proxy-prhfv" podUID=1c057a0e-3ee3-44f3-b294-434acc8f9491 isTerminal=false
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.583099  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/kube-proxy-prhfv" podUID=1c057a0e-3ee3-44f3-b294-434acc8f9491 updateType=0
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.637989  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-proxy-prhfv" volumeName="kube-proxy" volumeSpecName="kube-proxy"
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.638102  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-proxy-prhfv" volumeName="xtables-lock" volumeSpecName="xtables-lock"
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.638175  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-proxy-prhfv" volumeName="lib-modules" volumeSpecName="lib-modules"
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.638292  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-proxy-prhfv" volumeName="kube-api-access-2sbqp" volumeSpecName="kube-api-access-2sbqp"
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.660199  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-2sbqp\" (UniqueName: \"kubernetes.io/projected/1c057a0e-3ee3-44f3-b294-434acc8f9491-kube-api-access-2sbqp\") pod \"kube-proxy-prhfv\" (UID: \"1c057a0e-3ee3-44f3-b294-434acc8f9491\") Volume is already mounted to pod, but remount was requested." pod="kube-system/kube-proxy-prhfv"
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.660318  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/1c057a0e-3ee3-44f3-b294-434acc8f9491-kube-proxy\") pod \"kube-proxy-prhfv\" (UID: \"1c057a0e-3ee3-44f3-b294-434acc8f9491\") Volume is already mounted to pod, but remount was requested." pod="kube-system/kube-proxy-prhfv"
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.660450  199956 configmap.go:181] Setting up volume kube-proxy for pod 1c057a0e-3ee3-44f3-b294-434acc8f9491 at /var/lib/kubelet/pods/1c057a0e-3ee3-44f3-b294-434acc8f9491/volumes/kubernetes.io~configmap/kube-proxy
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.660507  199956 configmap.go:205] Received configMap kube-system/kube-proxy containing (2) pieces of data, 1458 total bytes
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.660547  199956 projected.go:183] Setting up volume kube-api-access-2sbqp for pod 1c057a0e-3ee3-44f3-b294-434acc8f9491 at /var/lib/kubelet/pods/1c057a0e-3ee3-44f3-b294-434acc8f9491/volumes/kubernetes.io~projected/kube-api-access-2sbqp
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.660582  199956 quota_linux.go:271] SupportsQuotas called, but quotas disabled
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.660876  199956 atomic_writer.go:161] pod kube-system/kube-proxy-prhfv volume kube-proxy: no update required for target directory /var/lib/kubelet/pods/1c057a0e-3ee3-44f3-b294-434acc8f9491/volumes/kubernetes.io~configmap/kube-proxy
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.660938  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/1c057a0e-3ee3-44f3-b294-434acc8f9491-kube-proxy\") pod \"kube-proxy-prhfv\" (UID: \"1c057a0e-3ee3-44f3-b294-434acc8f9491\") " pod="kube-system/kube-proxy-prhfv"
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.661008  199956 atomic_writer.go:161] pod kube-system/kube-proxy-prhfv volume kube-api-access-2sbqp: no update required for target directory /var/lib/kubelet/pods/1c057a0e-3ee3-44f3-b294-434acc8f9491/volumes/kubernetes.io~projected/kube-api-access-2sbqp
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.661094  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-2sbqp\" (UniqueName: \"kubernetes.io/projected/1c057a0e-3ee3-44f3-b294-434acc8f9491-kube-api-access-2sbqp\") pod \"kube-proxy-prhfv\" (UID: \"1c057a0e-3ee3-44f3-b294-434acc8f9491\") " pod="kube-system/kube-proxy-prhfv"
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.895908  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.895974  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.903767  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:56 GMT] X-Content-Type-Options:[nosniff]] 0xc0002be680 2 [] false false map[] 0xc0016ab600 0xc0013f0580}
Jan 20 12:41:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:56.903814  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:41:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:57.483973  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:57.484038  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:57.492646  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[ea78ae93-101d-49bc-82f1-0a9fea29ea3f] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:57 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0002bf580 2 [] false false map[] 0xc00105a900 0xc001a50420}
Jan 20 12:41:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:57.492672  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:57.581999  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[kube-system/etcd-zcy-z390-aorus-master]
Jan 20 12:41:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:57.582092  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:41:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:57.582172  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d updateType=0
Jan 20 12:41:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:57.582248  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d
Jan 20 12:41:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:57.582281  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/etcd-zcy-z390-aorus-master"
Jan 20 12:41:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:57.582377  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/etcd-zcy-z390-aorus-master" oldPhase=Running phase=Running
Jan 20 12:41:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:57.583099  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/etcd-zcy-z390-aorus-master" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:28 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:37 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:37 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:28 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.239.159.53 PodIPs:[{IP:10.239.159.53}] StartTime:2023-01-20 12:31:28 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:etcd State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:22 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:6 Image:k8s.gcr.io/etcd:3.5.3-0 ImageID:k8s.gcr.io/etcd@sha256:13f53ed1d91e2e11aac476ee9a0269fdda6cc4874eba903efd40daf50c55eee5 ContainerID:containerd://9b11acac1b891eafc7ff2b244f1c42938f71a575ce81ff917e3b7ebf61d2e045 Started:0xc001200eee}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:41:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:57.583478  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/etcd-zcy-z390-aorus-master"
Jan 20 12:41:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:57.583548  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/etcd-zcy-z390-aorus-master"
Jan 20 12:41:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:57.584195  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/etcd-zcy-z390-aorus-master"
Jan 20 12:41:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:57.584390  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d isTerminal=false
Jan 20 12:41:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:57.584446  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d updateType=0
Jan 20 12:41:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:57.588177  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:41:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:57.588189  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:41:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:57.588196  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:41:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:57.588805  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:41:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:57.588850  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:41:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:57.588856  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:41:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:57.588865  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:41:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:57.646239  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/etcd-zcy-z390-aorus-master" volumeName="etcd-certs" volumeSpecName="etcd-certs"
Jan 20 12:41:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:57.646341  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/etcd-zcy-z390-aorus-master" volumeName="etcd-data" volumeSpecName="etcd-data"
Jan 20 12:41:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:57.802001  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:41:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:58.484209  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:58.484276  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:58.497299  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[3e05c956-46de-4f8c-b14b-951d61bea20a] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:58 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0004c1220 2 [] false false map[] 0xc001a4b600 0xc001222420}
Jan 20 12:41:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:58.497440  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:58.879601  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/healthz" timeout="1s"
Jan 20 12:41:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:58.879660  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:41:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:58.879670  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:58.879728  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:58.880860  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:58 GMT] X-Content-Type-Options:[nosniff]] 0xc0008dc280 2 [] true false map[] 0xc001a4b900 <nil>}
Jan 20 12:41:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:58.880928  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:58 GMT] X-Content-Type-Options:[nosniff]] 0xc001702040 2 [] true false map[] 0xc00105b500 <nil>}
Jan 20 12:41:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:58.880975  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:41:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:58.881044  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:41:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:59.483954  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:41:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:59.484021  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:41:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:59.498436  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[d3d8dd6d-92dd-423a-930f-4a4cd49938d8] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:41:59 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0004c1a00 2 [] false false map[] 0xc0014c6200 0xc0016280b0}
Jan 20 12:41:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:59.498570  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:41:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:59.581364  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:41:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:59.587198  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:41:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:59.587210  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:41:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:59.588046  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:41:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:59.588591  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:41:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:59.588639  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:41:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:59.588645  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:41:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:59.588654  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:41:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:59.848009  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:41:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:59.866025  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:41:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:59.913069  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:41:51.263628597 -0500 EST"
Jan 20 12:41:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:59.913145  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510782" capacity="511757" time="2023-01-20 12:41:59.910912977 -0500 EST m=+632.466687868"
Jan 20 12:41:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:59.913205  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59469196Ki" capacity="65586124Ki" time="2023-01-20 12:41:59.850286459 -0500 EST m=+632.406061351"
Jan 20 12:41:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:59.913237  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64393760Ki" capacity="65061836Ki" time="2023-01-20 12:41:59.912749361 -0500 EST m=+632.468524253"
Jan 20 12:41:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:59.913272  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902012060Ki" capacity="981310056Ki" time="2023-01-20 12:41:59.850286459 -0500 EST m=+632.406061351"
Jan 20 12:41:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:59.913302  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:41:59.850286459 -0500 EST m=+632.406061351"
Jan 20 12:41:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:59.913340  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902012060Ki" capacity="981310056Ki" time="2023-01-20 12:41:51.263628597 -0500 EST"
Jan 20 12:41:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:41:59.913477  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:42:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:00.483174  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:00.483241  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:00.496304  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[7773c005-f101-4a8e-8f68-30211b9f8e94] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:00 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0004c1480 2 [] false false map[] 0xc001a4a800 0xc001360160}
Jan 20 12:42:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:00.496434  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:01.166748  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:42:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:01.166819  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:01.174776  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[ab87d377-0769-4e85-9164-adc11a2ed547] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:01 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008e2140 2 [] false false map[] 0xc001a4b300 0xc00112ec60}
Jan 20 12:42:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:01.174810  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:01.259438  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:42:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:01.259471  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:42:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:01.260050  199956 interface.go:209] Interface eno2 is up
Jan 20 12:42:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:01.260177  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:42:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:01.260212  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:42:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:01.260237  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:42:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:01.260252  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:42:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:01.260272  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:42:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:01.261258  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:42:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:01.261300  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:42:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:01.261319  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:42:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:01.261339  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:42:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:01.483860  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:01.483923  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:01.491528  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[d7499066-36e1-4520-b37a-82566c29ff09] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:01 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008ddbe0 2 [] false false map[] 0xc001f36a00 0xc000f8cd10}
Jan 20 12:42:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:01.491560  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:01.581937  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:42:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:01.587331  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:42:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:01.587344  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:42:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:01.588210  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:42:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:01.588539  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:42:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:01.588584  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:42:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:01.588591  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:42:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:01.588599  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:42:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:02.483312  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:02.483342  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:02.489779  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[320d3590-0569-4d1f-b081-f13acd41307b] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:02 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008ddf60 2 [] false false map[] 0xc00105bd00 0xc000fe46e0}
Jan 20 12:42:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:02.489855  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:02.689429  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:42:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:02.689492  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:02.690608  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:02 GMT]] 0xc00129c760 2 [] true false map[] 0xc0014c6200 <nil>}
Jan 20 12:42:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:02.690720  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:42:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:02.695947  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:42:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:02.696006  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:02.697015  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:02 GMT]] 0xc00129c7a0 2 [] true false map[] 0xc0014c6400 <nil>}
Jan 20 12:42:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:02.697119  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:42:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:02.709560  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:42:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:02.709627  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:02.709637  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:42:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:02.709698  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:02.710565  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:02 GMT]] 0xc0017024a0 2 [] true false map[] 0xc0014c6900 <nil>}
Jan 20 12:42:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:02.710679  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:42:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:02.710666  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:02 GMT]] 0xc0017024e0 2 [] true false map[] 0xc001774100 <nil>}
Jan 20 12:42:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:02.710765  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:42:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:02.804096  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:42:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:03.483179  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:03.483245  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:03.491589  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[5ca0cfad-28b6-420b-b40f-ace52530bd83] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:03 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123e7c0 2 [] false false map[] 0xc0016aa100 0xc0012a0790}
Jan 20 12:42:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:03.491621  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:03.582063  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:42:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:03.585336  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:42:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:03.585363  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:42:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:03.585378  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:42:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:03.586491  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:42:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:03.586607  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:42:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:03.586625  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:42:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:03.586648  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:42:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:04.484140  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:04.484207  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:04.497505  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[fbd51d33-e0d9-468b-8150-05962c4be0d6] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:04 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000cb3380 2 [] false false map[] 0xc000339e00 0xc0016a8000}
Jan 20 12:42:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:04.497644  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:05.484105  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:05.484175  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:05.492575  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[64f35c06-693d-4d4b-9447-8d2faf7e9fa4] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:05 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123ec40 2 [] false false map[] 0xc0014c6e00 0xc001337b80}
Jan 20 12:42:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:05.492603  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:05.581606  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:42:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:05.587254  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:42:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:05.587266  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:42:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:05.587945  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:42:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:05.588298  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:42:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:05.588349  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:42:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:05.588356  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:42:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:05.588365  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:42:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:06.164376  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:42:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:06.164448  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:06.171934  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:06 GMT] X-Content-Type-Options:[nosniff]] 0xc0020fddc0 2 [] false false map[] 0xc001846500 0xc001561ce0}
Jan 20 12:42:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:06.171978  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:42:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:06.271728  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:42:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:06.271745  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:06.272030  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:06 GMT]] 0xc001702700 29 [] true false map[] 0xc001774400 <nil>}
Jan 20 12:42:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:06.272055  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:42:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:06.483772  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:06.483834  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:06.491466  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[62c6fe06-1c21-4c5c-9773-c52b1162241e] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:06 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123f8c0 2 [] false false map[] 0xc001774600 0xc001e97e40}
Jan 20 12:42:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:06.491494  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:06.895645  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:42:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:06.895708  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:06.903078  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:06 GMT] X-Content-Type-Options:[nosniff]] 0xc0013007c0 2 [] false false map[] 0xc0002c3000 0xc001e74f20}
Jan 20 12:42:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:06.903136  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:42:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:07.483507  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:07.483577  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:07.491675  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[a5cd7056-5056-4162-8ceb-eff9364eeb0c] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:07 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001301280 2 [] false false map[] 0xc001846700 0xc001dc7ce0}
Jan 20 12:42:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:07.491704  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:07.526174  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/etcd.yaml"
Jan 20 12:42:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:07.527017  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver.yaml"
Jan 20 12:42:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:07.528114  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Jan 20 12:42:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:07.529184  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-scheduler.yaml"
Jan 20 12:42:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:07.529768  199956 config.go:279] "Setting pods for source" source="file"
Jan 20 12:42:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:07.581671  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:42:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:07.589281  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:42:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:07.589355  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:42:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:07.591632  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:42:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:07.593631  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:42:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:07.593853  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:42:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:07.593885  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:42:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:07.593929  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:42:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:07.805899  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:42:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:08.483720  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:08.483750  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:08.490228  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[8f3ff784-6b10-4fd3-834b-56f9b2f79729] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:08 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123e320 2 [] false false map[] 0xc001846200 0xc000de6b00}
Jan 20 12:42:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:08.490299  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:08.879636  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:42:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:08.879703  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:08.880844  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:08 GMT] X-Content-Type-Options:[nosniff]] 0xc001718ce0 2 [] true false map[] 0xc0014c6200 <nil>}
Jan 20 12:42:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:08.880960  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:09.483396  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:09.483463  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:09.491755  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[ba4d9ab3-9835-407d-8a18-ad478cc20e68] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:09 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001718fa0 2 [] false false map[] 0xc0002c2d00 0xc0002b6160}
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:09.491785  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:09.581868  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:09.589188  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:09.589249  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:09.589279  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:09.591525  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:09.591759  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:09.591795  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:09.591839  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kata[206922]: time="2023-01-20T12:42:09.743657842-05:00" level=error msg="agent pull image err. context deadline exceeded" name=containerd-shim-v2 pid=206922 sandbox=e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b source=virtcontainers subsystem=kata_agent
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kata[206922]: time="2023-01-20T12:42:09.743825314-05:00" level=error msg="kata runtime PullImage err. context deadline exceeded" name=containerd-shim-v2 pid=206922 sandbox=e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b source=containerd-kata-shim-v2
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:42:09.743657842-05:00" level=error msg="agent pull image err. context deadline exceeded" name=containerd-shim-v2 pid=206922 sandbox=e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b source=virtcontainers subsystem=kata_agent
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:42:09.743825314-05:00" level=error msg="kata runtime PullImage err. context deadline exceeded" name=containerd-shim-v2 pid=206922 sandbox=e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b source=containerd-kata-shim-v2
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:42:09.743687451-05:00" level=warning msg="notify on errors" error="failed to ping agent: Failed to Check if grpc server is working: context deadline exceeded" sandbox=e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b source=virtcontainers subsystem=virtcontainers/monitor
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:42:09.744037707-05:00" level=warning msg="write error to watcher" error="failed to ping agent: Failed to Check if grpc server is working: context deadline exceeded" sandbox=e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b source=virtcontainers subsystem=virtcontainers/monitor
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:42:09.744203921-05:00" level=error msg="Wait for process failed" container=e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b error="ttrpc: closed" name=containerd-shim-v2 pid=e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b sandbox=e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b source=containerd-kata-shim-v2
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kata[206922]: time="2023-01-20T12:42:09.743687451-05:00" level=warning msg="notify on errors" error="failed to ping agent: Failed to Check if grpc server is working: context deadline exceeded" sandbox=e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b source=virtcontainers subsystem=virtcontainers/monitor
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:42:09.745110535-05:00" level=warning msg="notify on errors" error="failed to ping agent: Failed to Check if grpc server is working: Dead agent" sandbox=e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b source=virtcontainers subsystem=virtcontainers/monitor
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:42:09.745313689-05:00" level=warning msg="write error to watcher" error="failed to ping agent: Failed to Check if grpc server is working: Dead agent" sandbox=e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b source=virtcontainers subsystem=virtcontainers/monitor
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:42:09.745890848-05:00" level=warning msg="Agent did not stop sandbox" error="Dead agent" name=containerd-shim-v2 pid=206922 sandbox=e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b sandboxid=e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b source=virtcontainers subsystem=sandbox
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kata[206922]: time="2023-01-20T12:42:09.744037707-05:00" level=warning msg="write error to watcher" error="failed to ping agent: Failed to Check if grpc server is working: context deadline exceeded" sandbox=e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b source=virtcontainers subsystem=virtcontainers/monitor
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kata[206922]: time="2023-01-20T12:42:09.744203921-05:00" level=error msg="Wait for process failed" container=e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b error="ttrpc: closed" name=containerd-shim-v2 pid=e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b sandbox=e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b source=containerd-kata-shim-v2
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kata[206922]: time="2023-01-20T12:42:09.745110535-05:00" level=warning msg="notify on errors" error="failed to ping agent: Failed to Check if grpc server is working: Dead agent" sandbox=e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b source=virtcontainers subsystem=virtcontainers/monitor
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kata[206922]: time="2023-01-20T12:42:09.745313689-05:00" level=warning msg="write error to watcher" error="failed to ping agent: Failed to Check if grpc server is working: Dead agent" sandbox=e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b source=virtcontainers subsystem=virtcontainers/monitor
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kata[206922]: time="2023-01-20T12:42:09.745890848-05:00" level=warning msg="Agent did not stop sandbox" error="Dead agent" name=containerd-shim-v2 pid=206922 sandbox=e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b sandboxid=e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b source=virtcontainers subsystem=sandbox
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kata[206922]: time="2023-01-20T12:42:09.74706098-05:00" level=error msg="Failed to read guest console logs" console-protocol=unix console-url=/run/vc/vm/e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b/console.sock error="read unix @->/run/vc/vm/e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b/console.sock: use of closed network connection" name=containerd-shim-v2 pid=206922 sandbox=e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b source=virtcontainers subsystem=sandbox
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:42:09.74706098-05:00" level=error msg="Failed to read guest console logs" console-protocol=unix console-url=/run/vc/vm/e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b/console.sock error="read unix @->/run/vc/vm/e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b/console.sock: use of closed network connection" name=containerd-shim-v2 pid=206922 sandbox=e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b source=virtcontainers subsystem=sandbox
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER systemd[193112]: run-kata\x2dcontainers-shared-sandboxes-e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b-shared-e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b\x2d3467aa3de2a69153\x2dresolv.conf.mount: Succeeded.
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER systemd[193112]: run-kata\x2dcontainers-shared-sandboxes-e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b-mounts-e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b\x2d3467aa3de2a69153\x2dresolv.conf.mount: Succeeded.
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kubelet[199956]: E0120 12:42:09.750281  199956 remote_image.go:238] "PullImage from image service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" image="ngcn-registry.sh.intel.com:443/ci-example256m:latest"
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kubelet[199956]: E0120 12:42:09.750302  199956 kuberuntime_image.go:51] "Failed to pull image" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" image="ngcn-registry.sh.intel.com:443/ci-example256m:latest"
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kubelet[199956]: E0120 12:42:09.750350  199956 kuberuntime_manager.go:905] container &Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod unsigned-unencrypted-cc-1_default(4f97314d-815b-4787-b174-5c158cd28c9d): ErrImagePull: rpc error: code = DeadlineExceeded desc = context deadline exceeded
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:09.750366  199956 kubelet.go:1503] "syncPod exit" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d isTerminal=false
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kubelet[199956]: E0120 12:42:09.750376  199956 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ci-example256m\" with ErrImagePull: \"rpc error: code = DeadlineExceeded desc = context deadline exceeded\"" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:09.750395  199956 pod_workers.go:988] "Processing pod event done" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:09.750397  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Warning" reason="Failed" message="Failed to pull image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\": rpc error: code = DeadlineExceeded desc = context deadline exceeded"
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:09.750399  199956 pod_workers.go:888] "Processing pod event" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:09.750408  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Warning" reason="Failed" message="Error: ErrImagePull"
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:42:09.744365062-05:00" level=error msg="PullImage \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\" failed" error="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER systemd[193112]: run-kata\x2dcontainers-shared-sandboxes-e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b-shared-e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b-rootfs.mount: Succeeded.
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER systemd[193112]: run-kata\x2dcontainers-shared-sandboxes-e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b-mounts-e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b-rootfs.mount: Succeeded.
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER systemd[1066]: run-kata\x2dcontainers-shared-sandboxes-e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b-shared-e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b\x2d3467aa3de2a69153\x2dresolv.conf.mount: Succeeded.
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER systemd[1066]: run-kata\x2dcontainers-shared-sandboxes-e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b-mounts-e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b\x2d3467aa3de2a69153\x2dresolv.conf.mount: Succeeded.
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:09.770049  199956 manager.go:1044] Destroyed container: "/kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b" (aliases: [e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b /kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b], namespace: "containerd")
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:09.770072  199956 handler.go:325] Added event &{/kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b 2023-01-20 12:42:09.770068523 -0500 EST m=+642.325843346 containerDeletion {<nil>}}
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER systemd[1066]: run-kata\x2dcontainers-shared-sandboxes-e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b-shared-e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b-rootfs.mount: Succeeded.
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER systemd[1066]: run-kata\x2dcontainers-shared-sandboxes-e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b-mounts-e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b-rootfs.mount: Succeeded.
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER systemd[1]: run-kata\x2dcontainers-shared-sandboxes-e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b-shared-e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b\x2d3467aa3de2a69153\x2dresolv.conf.mount: Succeeded.
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER systemd[1]: run-kata\x2dcontainers-shared-sandboxes-e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b-mounts-e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b\x2d3467aa3de2a69153\x2dresolv.conf.mount: Succeeded.
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER systemd[1]: run-kata\x2dcontainers-shared-sandboxes-e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b-shared-e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b-rootfs.mount: Succeeded.
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER systemd[1]: run-kata\x2dcontainers-shared-sandboxes-e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b-mounts-e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b-rootfs.mount: Succeeded.
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:09.771505  199956 manager.go:1044] Destroyed container: "/kata_overhead/e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b" (aliases: [e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b /kata_overhead/e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b], namespace: "containerd")
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:09.771517  199956 handler.go:325] Added event &{/kata_overhead/e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b 2023-01-20 12:42:09.771513995 -0500 EST m=+642.327288819 containerDeletion {<nil>}}
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER systemd[193112]: run-kata\x2dcontainers-shared-sandboxes-e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b-shared.mount: Succeeded.
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER systemd[1]: run-kata\x2dcontainers-shared-sandboxes-e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b-shared.mount: Succeeded.
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kata[206922]: time="2023-01-20T12:42:09.773181584-05:00" level=warning msg="sandbox stopped unexpectedly" error="failed to ping agent: Failed to Check if grpc server is working: context deadline exceeded" name=containerd-shim-v2 pid=206922 sandbox=e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b source=containerd-kata-shim-v2
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kata[206922]: time="2023-01-20T12:42:09.773556318-05:00" level=error msg="failed to cleanup the &{%!s(*cgroups.cgroup=&{0x56331405c980 [0xc0000812e0 0xc0004f8ce0 0xc0004f8d60 0xc0004f8d70 0xc0004f8d80 0xc0004f8dd0 0xc0004f8e10 0xc0004f8e20 0xc0004f8e30 0xc0000125d0 0xc000081300 0xc0004f8e40 0xc0004f8e80 0xc0001dc780] {0 0} <nil>}) /kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b %!s(*specs.LinuxCPU=&{<nil> <nil> <nil> <nil> <nil>  }) [{%!s(bool=false)  %!s(*int64=<nil>) %!s(*int64=<nil>) rwm} {%!s(bool=true) c %!s(*int64=0xc000045030) %!s(*int64=0xc000045038) rwm} {%!s(bool=true) c %!s(*int64=0xc000045040) %!s(*int64=0xc000045048) rwm} {%!s(bool=true) c %!s(*int64=0xc0001dc548) %!s(*int64=0xc0001dc550) rwm} {%!s(bool=true) c %!s(*int64=0xc0001dc578) %!s(*int64=0xc0001dc580) rwm} {%!s(bool=true) c %!s(*int64=0xc0001dc5a8) %!s(*int64=0xc0001dc5b0) rwm} {%!s(bool=true) c %!s(*int64=0xc0001dc5d8) %!s(*int64=0xc0001dc5e0) rwm} {%!s(bool=true) c %!s(*int64=0xc0001dc608) %!s(*int64=0xc0001dc610) rwm} {%!s(bool=true) c %!s(*int64=0xc0001dc638) %!s(*int64=0xc0001dc640) rwm} {%!s(bool=true) c %!s(*int64=0xc0001dc668) %!s(*int64=0xc0001dc670) rwm} {%!s(bool=true) c %!s(*int64=0xc0001dc698) %!s(*int64=0xc0001dc6a0) rwm} {%!s(bool=true) c %!s(*int64=0xc0001dc6c8) %!s(*int64=0xc0001dc6d0) rwm} {%!s(bool=true) c %!s(*int64=0xc0001dc6f8) %!s(*int64=0xc0001dc700) rwm} {%!s(bool=true) c %!s(*int64=0xc0001dc728) %!s(*int64=0xc0001dc730) rwm} {%!s(bool=true) c %!s(*int64=0xc000045108) %!s(*int64=0xc000045110) m} {%!s(bool=true) b %!s(*int64=0xc000045108) %!s(*int64=0xc000045110) m} {%!s(bool=true) c %!s(*int64=0xc000045118) %!s(*int64=0xc000045110) rwm} {%!s(bool=true) c %!s(*int64=0xc000045120) %!s(*int64=0xc000045128) rwm}] {%!s(int32=0) %!s(uint32=0)}} resource controllers" error="cgroups: cgroup deleted" name=containerd-shim-v2 pid=206922 sandbox=e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b source=virtcontainers subsystem=sandbox
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kata[206922]: time="2023-01-20T12:42:09.773604779-05:00" level=warning msg="Calling Cleanup() on an already cleaned up filesystem" name=containerd-shim-v2 pid=206922 sandbox=e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b source=virtcontainers subsystem="filesystem share"
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER systemd[1066]: run-kata\x2dcontainers-shared-sandboxes-e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b-shared.mount: Succeeded.
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:42:09.773181584-05:00" level=warning msg="sandbox stopped unexpectedly" error="failed to ping agent: Failed to Check if grpc server is working: context deadline exceeded" name=containerd-shim-v2 pid=206922 sandbox=e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b source=containerd-kata-shim-v2
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:42:09.773556318-05:00" level=error msg="failed to cleanup the &{%!s(*cgroups.cgroup=&{0x56331405c980 [0xc0000812e0 0xc0004f8ce0 0xc0004f8d60 0xc0004f8d70 0xc0004f8d80 0xc0004f8dd0 0xc0004f8e10 0xc0004f8e20 0xc0004f8e30 0xc0000125d0 0xc000081300 0xc0004f8e40 0xc0004f8e80 0xc0001dc780] {0 0} <nil>}) /kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b %!s(*specs.LinuxCPU=&{<nil> <nil> <nil> <nil> <nil>  }) [{%!s(bool=false)  %!s(*int64=<nil>) %!s(*int64=<nil>) rwm} {%!s(bool=true) c %!s(*int64=0xc000045030) %!s(*int64=0xc000045038) rwm} {%!s(bool=true) c %!s(*int64=0xc000045040) %!s(*int64=0xc000045048) rwm} {%!s(bool=true) c %!s(*int64=0xc0001dc548) %!s(*int64=0xc0001dc550) rwm} {%!s(bool=true) c %!s(*int64=0xc0001dc578) %!s(*int64=0xc0001dc580) rwm} {%!s(bool=true) c %!s(*int64=0xc0001dc5a8) %!s(*int64=0xc0001dc5b0) rwm} {%!s(bool=true) c %!s(*int64=0xc0001dc5d8) %!s(*int64=0xc0001dc5e0) rwm} {%!s(bool=true) c %!s(*int64=0xc0001dc608) %!s(*int64=0xc0001dc610) rwm} {%!s(bool=true) c %!s(*int64=0xc0001dc638) %!s(*int64=0xc0001dc640) rwm} {%!s(bool=true) c %!s(*int64=0xc0001dc668) %!s(*int64=0xc0001dc670) rwm} {%!s(bool=true) c %!s(*int64=0xc0001dc698) %!s(*int64=0xc0001dc6a0) rwm} {%!s(bool=true) c %!s(*int64=0xc0001dc6c8) %!s(*int64=0xc0001dc6d0) rwm} {%!s(bool=true) c %!s(*int64=0xc0001dc6f8) %!s(*int64=0xc0001dc700) rwm} {%!s(bool=true) c %!s(*int64=0xc0001dc728) %!s(*int64=0xc0001dc730) rwm} {%!s(bool=true) c %!s(*int64=0xc000045108) %!s(*int64=0xc000045110) m} {%!s(bool=true) b %!s(*int64=0xc000045108) %!s(*int64=0xc000045110) m} {%!s(bool=true) c %!s(*int64=0xc000045118) %!s(*int64=0xc000045110) rwm} {%!s(bool=true) c %!s(*int64=0xc000045120) %!s(*int64=0xc000045128) rwm}] {%!s(int32=0) %!s(uint32=0)}} resource controllers" error="cgroups: cgroup deleted" name=containerd-shim-v2 pid=206922 sandbox=e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b source=virtcontainers subsystem=sandbox
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:42:09.773604779-05:00" level=warning msg="Calling Cleanup() on an already cleaned up filesystem" name=containerd-shim-v2 pid=206922 sandbox=e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b source=virtcontainers subsystem="filesystem share"
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER systemd[1066]: run-containerd-io.containerd.runtime.v2.task-k8s.io-e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b-rootfs.mount: Succeeded.
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b-rootfs.mount: Succeeded.
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER systemd[193112]: run-containerd-io.containerd.runtime.v2.task-k8s.io-e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b-rootfs.mount: Succeeded.
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:42:09.785322658-05:00" level=error msg="error receiving message" error="read unix /run/containerd/containerd.sock.ttrpc->@: read: connection reset by peer"
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:42:09.785344351-05:00" level=info msg="shim disconnected" id=e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:42:09.785364266-05:00" level=warning msg="cleaning up after shim disconnected" id=e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b namespace=k8s.io
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:42:09.785372236-05:00" level=info msg="cleaning up dead shim"
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:42:09.794830736-05:00" level=error msg="failed to delete" cmd="/usr/local/bin/containerd-shim-kata-qemu-v2 -namespace k8s.io -address /run/containerd/containerd.sock -publish-binary /opt/confidential-containers/bin/containerd -id e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b -bundle /run/containerd/io.containerd.runtime.v2.task/k8s.io/e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b delete" error="exit status 1"
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:42:09.794861908-05:00" level=warning msg="failed to clean up after shim disconnected" error="time=\"2023-01-20T12:42:09-05:00\" level=warning msg=\"failed to cleanup container\" container=e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b error=\"open /run/vc/sbs/e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b: no such file or directory\" name=containerd-shim-v2 pid=207125 sandbox=e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b source=containerd-kata-shim-v2\nio.containerd.kata.v2: open /run/vc/sbs/e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b: no such file or directory: exit status 1" id=e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b namespace=k8s.io
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:09.914051  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:09.932863  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:09.978252  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902012000Ki" capacity="981310056Ki" time="2023-01-20 12:42:09.916744821 -0500 EST m=+642.472519714"
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:09.978297  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945645" capacity="62382080" time="2023-01-20 12:42:09.916744821 -0500 EST m=+642.472519714"
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:09.978318  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902012000Ki" capacity="981310056Ki" time="2023-01-20 12:42:01.262199501 -0500 EST"
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:09.978337  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945645" capacity="62382080" time="2023-01-20 12:42:01.262199501 -0500 EST"
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:09.978358  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510796" capacity="511757" time="2023-01-20 12:42:09.977231608 -0500 EST m=+642.533006471"
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:09.978379  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59467216Ki" capacity="65586124Ki" time="2023-01-20 12:42:09.916744821 -0500 EST m=+642.472519714"
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:09.978397  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64393276Ki" capacity="65061836Ki" time="2023-01-20 12:42:09.978085499 -0500 EST m=+642.533860362"
Jan 20 12:42:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:09.978474  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:10.400638  199956 generic.go:155] "GenericPLEG" podUID=4f97314d-815b-4787-b174-5c158cd28c9d containerID="e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b" oldState=running newState=exited
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:10.401410  199956 kuberuntime_manager.go:1037] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b] pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:10.402078  199956 generic.go:421] "PLEG: Write status" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:10.402099  199956 kubelet.go:2098] "SyncLoop (PLEG): event for pod" pod="default/unsigned-unencrypted-cc-1" event=&{ID:4f97314d-815b-4787-b174-5c158cd28c9d Type:ContainerDied Data:e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b}
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:10.402099  199956 kubelet.go:1501] "syncPod enter" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:10.402110  199956 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b"
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:10.402112  199956 kubelet_pods.go:1435] "Generating pod status" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:10.402130  199956 kubelet_pods.go:1447] "Got phase for pod" pod="default/unsigned-unencrypted-cc-1" oldPhase=Pending phase=Pending
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:10.402227  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:10.402244  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:10.402251  199956 kuberuntime_manager.go:488] "No ready sandbox for pod can be found. Need to start a new one" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:10.402261  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:true CreateSandbox:true SandboxID:e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b Attempt:1 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:10.402272  199956 kuberuntime_manager.go:730] "Stopping PodSandbox for pod, will start new one" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:10.402336  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="" kind="Pod" apiVersion="v1" type="Normal" reason="SandboxChanged" message="Pod sandbox changed, it will be killed and re-created."
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:42:10.402392666-05:00" level=info msg="StopPodSandbox for \"e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b\""
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER systemd[193112]: run-containerd-io.containerd.grpc.v1.cri-sandboxes-e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b-shm.mount: Succeeded.
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER systemd[1]: run-containerd-io.containerd.grpc.v1.cri-sandboxes-e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b-shm.mount: Succeeded.
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER systemd[1066]: run-containerd-io.containerd.grpc.v1.cri-sandboxes-e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b-shm.mount: Succeeded.
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:10.407290  199956 status_manager.go:685] "Patch status for pod" pod="default/unsigned-unencrypted-cc-1" patch="{\"metadata\":{\"uid\":\"4f97314d-815b-4787-b174-5c158cd28c9d\"},\"status\":{\"containerStatuses\":[{\"image\":\"ngcn-registry.sh.intel.com:443/ci-example256m:latest\",\"imageID\":\"\",\"lastState\":{},\"name\":\"ci-example256m\",\"ready\":false,\"restartCount\":0,\"started\":false,\"state\":{\"waiting\":{\"message\":\"rpc error: code = DeadlineExceeded desc = context deadline exceeded\",\"reason\":\"ErrImagePull\"}}}],\"podIP\":\"10.244.0.12\",\"podIPs\":[{\"ip\":\"10.244.0.12\"}]}}"
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:10.407316  199956 config.go:279] "Setting pods for source" source="api"
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:10.407345  199956 status_manager.go:694] "Status for pod updated successfully" pod="default/unsigned-unencrypted-cc-1" statusVersion=2 status={Phase:Pending Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.12 PodIPs:[{IP:10.244.0.12}] StartTime:2023-01-20 12:41:08 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:ci-example256m State:{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = DeadlineExceeded desc = context deadline exceeded,} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest ImageID: ContainerID: Started:0xc0015fe5ee}] QOSClass:Guaranteed EphemeralContainerStatuses:[]}
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:10.407678  199956 kubelet.go:2073] "SyncLoop RECONCILE" source="api" pods=[default/unsigned-unencrypted-cc-1]
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER avahi-daemon[896]: Interface vethfbf8f23a.IPv6 no longer relevant for mDNS.
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER avahi-daemon[896]: Leaving mDNS multicast group on interface vethfbf8f23a.IPv6 with address fe80::3c4a:a0ff:fe3b:4bc7.
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kernel: cni0: port 6(vethfbf8f23a) entered disabled state
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:10.412343  199956 handler.go:293] error while reading "/proc/199956/fd/34" link: readlink /proc/199956/fd/34: no such file or directory
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kernel: device vethfbf8f23a left promiscuous mode
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kernel: cni0: port 6(vethfbf8f23a) entered disabled state
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:10.438423  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="default/unsigned-unencrypted-cc-1" volumeName="kube-api-access-qcb8g" volumeSpecName="kube-api-access-qcb8g"
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:10.458507  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") Volume is already mounted to pod, but remount was requested." pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:10.458698  199956 projected.go:183] Setting up volume kube-api-access-qcb8g for pod 4f97314d-815b-4787-b174-5c158cd28c9d at /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:10.459127  199956 atomic_writer.go:161] pod default/unsigned-unencrypted-cc-1 volume kube-api-access-qcb8g: no update required for target directory /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:10.459199  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") " pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER avahi-daemon[896]: Withdrawing address record for fe80::3c4a:a0ff:fe3b:4bc7 on vethfbf8f23a.
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER NetworkManager[905]: <info>  [1674236530.4655] device (vethfbf8f23a): released from master device cni0
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER gnome-shell[1450]: Removing a network device that was not added
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:10.483943  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:10.484022  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:42:10.491755204-05:00" level=info msg="TearDown network for sandbox \"e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b\" successfully"
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:42:10.491853091-05:00" level=info msg="StopPodSandbox for \"e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b\" returns successfully"
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER systemd[1]: run-netns-cni\x2dd1d5b07f\x2defab\x2dee67\x2d6d1d\x2db58d363f904e.mount: Succeeded.
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER systemd[193112]: run-netns-cni\x2dd1d5b07f\x2defab\x2dee67\x2d6d1d\x2db58d363f904e.mount: Succeeded.
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER systemd[1066]: run-netns-cni\x2dd1d5b07f\x2defab\x2dee67\x2d6d1d\x2db58d363f904e.mount: Succeeded.
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:10.497074  199956 kuberuntime_manager.go:785] "Creating PodSandbox for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:10.497333  199956 kuberuntime_sandbox.go:63] "Running pod with runtime handler" pod="default/unsigned-unencrypted-cc-1" runtimeHandler="kata-qemu"
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:42:10.498051647-05:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:unsigned-unencrypted-cc-1,Uid:4f97314d-815b-4787-b174-5c158cd28c9d,Namespace:default,Attempt:1,}"
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:10.501779  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[778a7cb2-38c2-4e6f-9338-3e271f6e978a] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:10 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008dd980 2 [] false false map[] 0xc001a4af00 0xc000ffc840}
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:10.502296  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER systemd-udevd[207162]: ethtool: autonegotiation is unset or enabled, the speed and duplex are not writable.
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER systemd-udevd[207162]: Using default interface naming scheme 'v245'.
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER systemd-udevd[207162]: veth187c5243: Could not generate persistent MAC: No data available
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER NetworkManager[905]: <info>  [1674236530.5160] manager: (veth187c5243): new Veth device (/org/freedesktop/NetworkManager/Devices/123)
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kernel: cni0: port 6(veth187c5243) entered blocking state
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kernel: cni0: port 6(veth187c5243) entered disabled state
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kernel: device veth187c5243 entered promiscuous mode
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kernel: cni0: port 6(veth187c5243) entered blocking state
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kernel: cni0: port 6(veth187c5243) entered forwarding state
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kernel: IPv6: ADDRCONF(NETDEV_CHANGE): veth187c5243: link becomes ready
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER NetworkManager[905]: <info>  [1674236530.5257] device (veth187c5243): carrier: link connected
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER containerd[201983]: map[string]interface {}{"cniVersion":"0.3.1", "hairpinMode":true, "ipMasq":false, "ipam":map[string]interface {}{"ranges":[][]map[string]interface {}{[]map[string]interface {}{map[string]interface {}{"subnet":"10.244.0.0/24"}}}, "routes":[]types.Route{types.Route{Dst:net.IPNet{IP:net.IP{0xa, 0xf4, 0x0, 0x0}, Mask:net.IPMask{0xff, 0xff, 0x0, 0x0}}, GW:net.IP(nil)}}, "type":"host-local"}, "isDefaultGateway":true, "isGateway":true, "mtu":(*uint)(0xc0000b48e8), "name":"cbr0", "type":"bridge"}
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:10.569832  199956 factory.go:258] Using factory "containerd" for container "/kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603"
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER kernel: eth0: Caught tx_queue_len zero misconfig
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER virtiofsd[207230]: zcy-Z390-AORUS-MASTER virtiofsd[207230]: Use of deprecated flag '-f': This flag has no effect, please remove it
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER virtiofsd[207230]: zcy-Z390-AORUS-MASTER virtiofsd[207230]: Use of deprecated option format '-o': Please specify options without it (e.g., '--cache auto' instead of '-o cache=auto')
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER virtiofsd[207234]: zcy-Z390-AORUS-MASTER virtiofsd[207230]: Waiting for vhost-user socket connection...
Jan 20 12:42:10 zcy-Z390-AORUS-MASTER virtiofsd[207234]: zcy-Z390-AORUS-MASTER virtiofsd[207230]: Client connected, servicing requests
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.166608  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.166630  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.169371  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[3359d8ea-1b06-485d-b100-5c14849d9691] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:11 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001820780 2 [] false false map[] 0xc00105ad00 0xc0014b7a20}
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.169400  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.323219  199956 manager.go:988] Added container: "/kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603" (aliases: [38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 /kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603], namespace: "containerd")
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.323694  199956 handler.go:325] Added event &{/kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 2023-01-20 12:42:10.565877413 -0500 EST containerCreation {<nil>}}
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.323884  199956 container.go:530] Start housekeeping for container "/kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.324633  199956 factory.go:258] Using factory "containerd" for container "/kata_overhead/38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER containerd[201983]: {"cniVersion":"0.3.1","hairpinMode":true,"ipMasq":false,"ipam":{"ranges":[[{"subnet":"10.244.0.0/24"}]],"routes":[{"dst":"10.244.0.0/16"}],"type":"host-local"},"isDefaultGateway":true,"isGateway":true,"mtu":1450,"name":"cbr0","type":"bridge"}time="2023-01-20T12:42:11.326837711-05:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:unsigned-unencrypted-cc-1,Uid:4f97314d-815b-4787-b174-5c158cd28c9d,Namespace:default,Attempt:1,} returns sandbox id \"38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603\""
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.327251  199956 kuberuntime_manager.go:823] "Created PodSandbox for pod" podSandboxID="38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.327860  199956 manager.go:988] Added container: "/kata_overhead/38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603" (aliases: [38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 /kata_overhead/38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603], namespace: "containerd")
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.328069  199956 kuberuntime_manager.go:846] "Determined the ip for pod after sandbox changed" IPs=[10.244.0.13] pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.328306  199956 handler.go:325] Added event &{/kata_overhead/38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 2023-01-20 12:42:10.601877414 -0500 EST containerCreation {<nil>}}
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.328383  199956 container.go:530] Start housekeeping for container "/kata_overhead/38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.328391  199956 kuberuntime_manager.go:889] "Creating container in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.329367  199956 kuberuntime_manager.go:903] "Container start failed in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1" containerMessage="Back-off pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\"" err="ImagePullBackOff"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.329385  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Normal" reason="BackOff" message="Back-off pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\""
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.329435  199956 kubelet.go:1503] "syncPod exit" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d isTerminal=false
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.329433  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Warning" reason="Failed" message="Error: ImagePullBackOff"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: E0120 12:42:11.329476  199956 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ci-example256m\" with ImagePullBackOff: \"Back-off pulling image \\\"ngcn-registry.sh.intel.com:443/ci-example256m:latest\\\"\"" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.329525  199956 pod_workers.go:988] "Processing pod event done" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.329547  199956 pod_workers.go:888] "Processing pod event" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.405329  199956 generic.go:155] "GenericPLEG" podUID=4f97314d-815b-4787-b174-5c158cd28c9d containerID="38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603" oldState=non-existent newState=running
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.406096  199956 kuberuntime_manager.go:1037] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b] pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.408251  199956 generic.go:421] "PLEG: Write status" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.408358  199956 kubelet.go:2098] "SyncLoop (PLEG): event for pod" pod="default/unsigned-unencrypted-cc-1" event=&{ID:4f97314d-815b-4787-b174-5c158cd28c9d Type:ContainerStarted Data:38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603}
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.408362  199956 kubelet.go:1501] "syncPod enter" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.408428  199956 kubelet_pods.go:1435] "Generating pod status" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.408506  199956 kubelet_pods.go:1447] "Got phase for pod" pod="default/unsigned-unencrypted-cc-1" oldPhase=Pending phase=Pending
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.408951  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.409028  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.409078  199956 kuberuntime_manager.go:638] "Container of pod is not in the desired state and shall be started" containerName="ci-example256m" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.409122  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 Attempt:1 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.409410  199956 kuberuntime_manager.go:889] "Creating container in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.410401  199956 kuberuntime_manager.go:903] "Container start failed in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1" containerMessage="Back-off pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\"" err="ImagePullBackOff"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.410431  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Normal" reason="BackOff" message="Back-off pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\""
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.410477  199956 kubelet.go:1503] "syncPod exit" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d isTerminal=false
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.410478  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Warning" reason="Failed" message="Error: ImagePullBackOff"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: E0120 12:42:11.410522  199956 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ci-example256m\" with ImagePullBackOff: \"Back-off pulling image \\\"ngcn-registry.sh.intel.com:443/ci-example256m:latest\\\"\"" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.410572  199956 pod_workers.go:988] "Processing pod event done" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.410594  199956 pod_workers.go:888] "Processing pod event" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.415591  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.415620  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.416014  199956 interface.go:209] Interface eno2 is up
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.416253  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.416285  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.416300  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.416309  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.416319  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.416849  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.416873  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.416885  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.416896  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.419804  199956 config.go:279] "Setting pods for source" source="api"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.420000  199956 status_manager.go:685] "Patch status for pod" pod="default/unsigned-unencrypted-cc-1" patch="{\"metadata\":{\"uid\":\"4f97314d-815b-4787-b174-5c158cd28c9d\"},\"status\":{\"$setElementOrder/podIPs\":[{\"ip\":\"10.244.0.13\"}],\"containerStatuses\":[{\"image\":\"ngcn-registry.sh.intel.com:443/ci-example256m:latest\",\"imageID\":\"\",\"lastState\":{},\"name\":\"ci-example256m\",\"ready\":false,\"restartCount\":0,\"started\":false,\"state\":{\"waiting\":{\"message\":\"Back-off pulling image \\\"ngcn-registry.sh.intel.com:443/ci-example256m:latest\\\"\",\"reason\":\"ImagePullBackOff\"}}}],\"podIP\":\"10.244.0.13\",\"podIPs\":[{\"ip\":\"10.244.0.13\"},{\"$patch\":\"delete\",\"ip\":\"10.244.0.12\"}]}}"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.420098  199956 status_manager.go:694] "Status for pod updated successfully" pod="default/unsigned-unencrypted-cc-1" statusVersion=3 status={Phase:Pending Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.13 PodIPs:[{IP:10.244.0.13}] StartTime:2023-01-20 12:41:08 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:ci-example256m State:{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "ngcn-registry.sh.intel.com:443/ci-example256m:latest",} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest ImageID: ContainerID: Started:0xc00142dd5e}] QOSClass:Guaranteed EphemeralContainerStatuses:[]}
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.420565  199956 kubelet.go:2073] "SyncLoop RECONCILE" source="api" pods=[default/unsigned-unencrypted-cc-1]
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.445619  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="default/unsigned-unencrypted-cc-1" volumeName="kube-api-access-qcb8g" volumeSpecName="kube-api-access-qcb8g"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.465792  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") Volume is already mounted to pod, but remount was requested." pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.465988  199956 projected.go:183] Setting up volume kube-api-access-qcb8g for pod 4f97314d-815b-4787-b174-5c158cd28c9d at /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.466405  199956 atomic_writer.go:161] pod default/unsigned-unencrypted-cc-1 volume kube-api-access-qcb8g: no update required for target directory /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.466476  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") " pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.483155  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.483215  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.496051  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[35eca444-4a6a-4706-8989-6193088e2086] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:11 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000707fa0 2 [] false false map[] 0xc001774500 0xc0016b00b0}
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.496172  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.581970  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.589314  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.589377  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.591595  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.593745  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.593970  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.594002  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:42:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:11.594045  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER avahi-daemon[896]: Joining mDNS multicast group on interface veth187c5243.IPv6 with address fe80::6c2c:6aff:febc:ec59.
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER avahi-daemon[896]: New relevant interface veth187c5243.IPv6 for mDNS.
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER avahi-daemon[896]: Registering new address record for fe80::6c2c:6aff:febc:ec59 on veth187c5243.*.
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.411250  199956 kubelet.go:1501] "syncPod enter" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.411309  199956 kubelet_pods.go:1435] "Generating pod status" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.411415  199956 kubelet_pods.go:1447] "Got phase for pod" pod="default/unsigned-unencrypted-cc-1" oldPhase=Pending phase=Pending
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.411700  199956 status_manager.go:535] "Ignoring same status for pod" pod="default/unsigned-unencrypted-cc-1" status={Phase:Pending Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.13 PodIPs:[{IP:10.244.0.13}] StartTime:2023-01-20 12:41:08 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:ci-example256m State:{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "ngcn-registry.sh.intel.com:443/ci-example256m:latest",} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest ImageID: ContainerID: Started:0xc00101f47e}] QOSClass:Guaranteed EphemeralContainerStatuses:[]}
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.412024  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.412093  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.412140  199956 kuberuntime_manager.go:638] "Container of pod is not in the desired state and shall be started" containerName="ci-example256m" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.412182  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 Attempt:1 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.412466  199956 kuberuntime_manager.go:889] "Creating container in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.413516  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Normal" reason="BackOff" message="Back-off pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\""
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.413523  199956 kuberuntime_manager.go:903] "Container start failed in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1" containerMessage="Back-off pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\"" err="ImagePullBackOff"
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.413559  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Warning" reason="Failed" message="Error: ImagePullBackOff"
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.413594  199956 kubelet.go:1503] "syncPod exit" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d isTerminal=false
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: E0120 12:42:12.413634  199956 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ci-example256m\" with ImagePullBackOff: \"Back-off pulling image \\\"ngcn-registry.sh.intel.com:443/ci-example256m:latest\\\"\"" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.413689  199956 pod_workers.go:988] "Processing pod event done" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.452399  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="default/unsigned-unencrypted-cc-1" volumeName="kube-api-access-qcb8g" volumeSpecName="kube-api-access-qcb8g"
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.472790  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") Volume is already mounted to pod, but remount was requested." pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.472975  199956 projected.go:183] Setting up volume kube-api-access-qcb8g for pod 4f97314d-815b-4787-b174-5c158cd28c9d at /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.473442  199956 atomic_writer.go:161] pod default/unsigned-unencrypted-cc-1 volume kube-api-access-qcb8g: no update required for target directory /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.473514  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") " pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.483169  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.483230  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.491422  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[161a821d-1cb2-49a0-9e17-1b3c8ce2873e] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:12 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0020fc340 2 [] false false map[] 0xc001716b00 0xc0008ca9a0}
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.491451  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.688946  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.689008  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.690039  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:12 GMT]] 0xc000ffaac0 2 [] true false map[] 0xc001717700 <nil>}
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.690144  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.696378  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.696442  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.697559  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:12 GMT]] 0xc0020fc460 2 [] true false map[] 0xc0011c4300 <nil>}
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.697658  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.709941  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.709976  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.710000  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.710032  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.711229  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:12 GMT]] 0xc000ffab60 2 [] true false map[] 0xc00105a800 <nil>}
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.711252  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:12 GMT]] 0xc00129c040 2 [] true false map[] 0xc0011c4800 <nil>}
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.711329  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.711353  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:42:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:12.807720  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:42:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:13.484030  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:13.484100  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:13.492514  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[da95a6b0-37b7-4443-a02e-3bb19104df92] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:13 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0018214c0 2 [] false false map[] 0xc0011c4a00 0xc000de69a0}
Jan 20 12:42:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:13.492543  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:13.581889  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[kube-system/kube-scheduler-zcy-z390-aorus-master]
Jan 20 12:42:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:13.581915  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:42:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:13.581965  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 updateType=0
Jan 20 12:42:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:13.581989  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3
Jan 20 12:42:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:13.581999  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/kube-scheduler-zcy-z390-aorus-master"
Jan 20 12:42:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:13.582027  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" oldPhase=Running phase=Running
Jan 20 12:42:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:13.582113  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:27 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:41 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:41 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:27 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.239.159.53 PodIPs:[{IP:10.239.159.53}] StartTime:2023-01-20 12:31:27 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:kube-scheduler State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:22 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:6 Image:k8s.gcr.io/kube-scheduler:v1.24.0 ImageID:k8s.gcr.io/kube-scheduler@sha256:db842a7c431fd51db7e1911f6d1df27a7b6b6963ceda24852b654d2cd535b776 ContainerID:containerd://13149330f3752d0ff65bcac86c7b522b2040811e815fbc87e56b40b612875d5a Started:0xc0010dc87e}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:42:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:13.582207  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-scheduler-zcy-z390-aorus-master"
Jan 20 12:42:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:13.582225  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/kube-scheduler-zcy-z390-aorus-master"
Jan 20 12:42:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:13.582368  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/kube-scheduler-zcy-z390-aorus-master"
Jan 20 12:42:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:13.582412  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 isTerminal=false
Jan 20 12:42:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:13.582428  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 updateType=0
Jan 20 12:42:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:13.583843  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:42:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:13.583860  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:42:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:13.583868  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:42:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:13.584534  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:42:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:13.584596  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:42:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:13.584605  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:42:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:13.584616  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:42:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:13.662061  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Jan 20 12:42:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:14.483466  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:14.483533  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:14.491364  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[c1ec6878-cf18-44d5-9ab5-9a8df454a1cd] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:14 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00129d220 2 [] false false map[] 0xc0002c3100 0xc000ffdce0}
Jan 20 12:42:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:14.491395  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:15.484193  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:15.484266  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:15.492626  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[80cd73f1-a4a5-4f58-a7cf-817e8484d690] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:15 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0020fc800 2 [] false false map[] 0xc001717900 0xc001762dc0}
Jan 20 12:42:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:15.492655  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:15.581542  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:42:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:15.587265  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:42:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:15.587277  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:42:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:15.587966  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:42:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:15.588437  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:42:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:15.588484  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:42:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:15.588491  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:42:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:15.588499  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:42:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:16.164439  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:42:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:16.164511  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:16.171883  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:16 GMT] X-Content-Type-Options:[nosniff]] 0xc00123e200 2 [] false false map[] 0xc001717c00 0xc0006d4e70}
Jan 20 12:42:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:16.171933  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:42:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:16.272185  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:42:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:16.272245  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:16.273517  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:16 GMT]] 0xc0020fcd40 29 [] true false map[] 0xc001717f00 <nil>}
Jan 20 12:42:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:16.273615  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:42:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:16.483216  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:16.483282  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:16.496028  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[a1c2ec9c-3d2b-479c-9619-bb4b00739bc4] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:16 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123e9e0 2 [] false false map[] 0xc0014c6200 0xc001772e70}
Jan 20 12:42:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:16.496167  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:16.895779  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:42:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:16.895845  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:16.903012  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:16 GMT] X-Content-Type-Options:[nosniff]] 0xc0020fd360 2 [] false false map[] 0xc0014c6800 0xc001910dc0}
Jan 20 12:42:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:16.903069  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:42:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:17.484156  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:17.484220  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:17.492779  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[b91fb21c-e6e2-4f44-aa04-3f45696ae2d2] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:17 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123ec00 2 [] false false map[] 0xc0014c6b00 0xc0017f5d90}
Jan 20 12:42:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:17.492832  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:17.581234  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:42:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:17.587215  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:42:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:17.587227  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:42:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:17.587233  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:42:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:17.587894  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:42:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:17.587941  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:42:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:17.587948  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:42:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:17.587956  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:42:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:17.809573  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:42:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:18.483724  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:18.483790  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:18.491287  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[000e7486-3c99-418b-b146-2d6ecf3615c2] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:18 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0002beb80 2 [] false false map[] 0xc0014c7100 0xc001d389a0}
Jan 20 12:42:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:18.491319  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:18.879261  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/healthz" timeout="1s"
Jan 20 12:42:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:18.879310  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:42:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:18.879333  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:18.879373  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:18.880442  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:18 GMT] X-Content-Type-Options:[nosniff]] 0xc000ffaba0 2 [] true false map[] 0xc000338600 <nil>}
Jan 20 12:42:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:18.880514  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:18 GMT] X-Content-Type-Options:[nosniff]] 0xc0008dc960 2 [] true false map[] 0xc001a4a000 <nil>}
Jan 20 12:42:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:18.880560  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:42:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:18.880628  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:42:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:19.483957  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:19.484022  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:19.492664  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[d0efbb91-79b8-4f5e-b629-8e201228b8fb] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:19 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ffaf00 2 [] false false map[] 0xc001a4a200 0xc001e76370}
Jan 20 12:42:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:19.492697  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:19.581509  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:42:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:19.587334  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:42:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:19.587346  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:42:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:19.588068  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:42:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:19.588400  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:42:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:19.588447  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:42:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:19.588454  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:42:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:19.588462  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:42:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:19.978573  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:42:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:19.986446  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:42:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:19.996244  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945631" capacity="62382080" time="2023-01-20 12:42:11.260838478 -0500 EST"
Jan 20 12:42:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:19.996257  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510775" capacity="511757" time="2023-01-20 12:42:19.995902974 -0500 EST m=+652.551677799"
Jan 20 12:42:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:19.996265  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59523452Ki" capacity="65586124Ki" time="2023-01-20 12:42:19.98076553 -0500 EST m=+652.536540422"
Jan 20 12:42:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:19.996273  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64393520Ki" capacity="65061836Ki" time="2023-01-20 12:42:19.99618486 -0500 EST m=+652.551959685"
Jan 20 12:42:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:19.996279  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902011872Ki" capacity="981310056Ki" time="2023-01-20 12:42:19.98076553 -0500 EST m=+652.536540422"
Jan 20 12:42:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:19.996285  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945631" capacity="62382080" time="2023-01-20 12:42:19.98076553 -0500 EST m=+652.536540422"
Jan 20 12:42:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:19.996291  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902011872Ki" capacity="981310056Ki" time="2023-01-20 12:42:11.260838478 -0500 EST"
Jan 20 12:42:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:19.996317  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:42:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:20.483448  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:20.483520  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:20.491385  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[36bcddfc-b920-4211-8462-29ec6593606b] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:20 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0017026c0 2 [] false false map[] 0xc0014c6200 0xc0019820b0}
Jan 20 12:42:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:20.491417  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:21.166175  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:42:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:21.166247  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:21.174648  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[0d3b4f19-7a15-4bf5-b52a-8fb51d3d94c8] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:21 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008e27a0 2 [] false false map[] 0xc000338700 0xc0002b6370}
Jan 20 12:42:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:21.174677  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:21.483180  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:21.483247  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:21.491401  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[74c32320-c30e-42bc-a1a6-385972c18b71] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:21 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008dc9c0 2 [] false false map[] 0xc000338d00 0xc000de6790}
Jan 20 12:42:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:21.491433  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:21.534208  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:42:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:21.534222  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:42:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:21.534512  199956 interface.go:209] Interface eno2 is up
Jan 20 12:42:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:21.534570  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:42:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:21.534585  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:42:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:21.534596  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:42:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:21.534603  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:42:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:21.534612  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:42:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:21.535092  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:42:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:21.535112  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:42:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:21.535123  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:42:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:21.535131  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:42:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:21.582034  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:42:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:21.589530  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:42:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:21.589592  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:42:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:21.591870  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:42:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:21.593772  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:42:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:21.593992  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:42:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:21.594024  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:42:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:21.594069  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:42:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:22.483401  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:22.483471  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:22.491331  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[d4db7a5f-d93a-4d29-82b5-4b28e70d87ab] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:22 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000cb2de0 2 [] false false map[] 0xc0014c6900 0xc0013dc420}
Jan 20 12:42:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:22.491363  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:22.689982  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:42:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:22.690048  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:22.691096  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:22 GMT]] 0xc0018212a0 2 [] true false map[] 0xc000fcd200 <nil>}
Jan 20 12:42:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:22.691206  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:42:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:22.695306  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:42:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:22.695372  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:22.696410  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:22 GMT]] 0xc001821320 2 [] true false map[] 0xc000fcd400 <nil>}
Jan 20 12:42:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:22.696510  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:42:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:22.709734  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:42:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:22.709795  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:22.709810  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:42:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:22.709867  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:22.710784  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:22 GMT]] 0xc0008e3c20 2 [] true false map[] 0xc0014c6c00 <nil>}
Jan 20 12:42:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:22.710884  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:42:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:22.710904  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:22 GMT]] 0xc0008ddf00 2 [] true false map[] 0xc000fcd600 <nil>}
Jan 20 12:42:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:22.711008  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:42:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:22.810540  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:42:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:23.483445  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:23.483516  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:23.491814  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[9275c713-cf0e-490b-80af-bac1d0f5ed97] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:23 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008e3c40 2 [] false false map[] 0xc000fcda00 0xc001602f20}
Jan 20 12:42:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:23.491845  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:23.581789  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[default/unsigned-unencrypted-cc-1]
Jan 20 12:42:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:23.581883  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:42:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:23.581947  199956 pod_workers.go:888] "Processing pod event" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:42:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:23.582002  199956 kubelet.go:1501] "syncPod enter" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:42:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:23.582031  199956 kubelet_pods.go:1435] "Generating pod status" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:23.582137  199956 kubelet_pods.go:1447] "Got phase for pod" pod="default/unsigned-unencrypted-cc-1" oldPhase=Pending phase=Pending
Jan 20 12:42:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:23.582417  199956 status_manager.go:535] "Ignoring same status for pod" pod="default/unsigned-unencrypted-cc-1" status={Phase:Pending Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.13 PodIPs:[{IP:10.244.0.13}] StartTime:2023-01-20 12:41:08 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:ci-example256m State:{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "ngcn-registry.sh.intel.com:443/ci-example256m:latest",} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest ImageID: ContainerID: Started:0xc000b25229}] QOSClass:Guaranteed EphemeralContainerStatuses:[]}
Jan 20 12:42:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:23.582758  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:23.582829  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:23.582882  199956 kuberuntime_manager.go:638] "Container of pod is not in the desired state and shall be started" containerName="ci-example256m" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:23.582921  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 Attempt:1 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:23.583190  199956 kuberuntime_manager.go:889] "Creating container in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:23.584167  199956 kuberuntime_image.go:47] "Pulling image without credentials" image="ngcn-registry.sh.intel.com:443/ci-example256m:latest"
Jan 20 12:42:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:23.584170  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Normal" reason="Pulling" message="Pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\""
Jan 20 12:42:23 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:42:23.585044520-05:00" level=info msg="PullImage \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\""
Jan 20 12:42:23 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:42:23.585155730-05:00" level=info msg="TaskManager get ImageService succeed." id=38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603
Jan 20 12:42:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:23.587390  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:42:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:23.587403  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:42:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:23.587410  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:42:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:23.587870  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:42:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:23.587918  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:42:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:23.587925  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:42:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:23.587934  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:42:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:23.635197  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="default/unsigned-unencrypted-cc-1" volumeName="kube-api-access-qcb8g" volumeSpecName="kube-api-access-qcb8g"
Jan 20 12:42:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:23.652316  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") Volume is already mounted to pod, but remount was requested." pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:23.652520  199956 projected.go:183] Setting up volume kube-api-access-qcb8g for pod 4f97314d-815b-4787-b174-5c158cd28c9d at /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:42:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:23.652979  199956 atomic_writer.go:161] pod default/unsigned-unencrypted-cc-1 volume kube-api-access-qcb8g: no update required for target directory /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:42:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:23.653055  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") " pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:24.483231  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:24.483303  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:24.491263  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[d00dd4d3-3c4a-450f-9877-a44b08f30e60] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:24 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001718a80 2 [] false false map[] 0xc0011c4900 0xc001763080}
Jan 20 12:42:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:24.491292  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:25.483743  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:25.483773  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:25.490479  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[dfa8eee0-13a7-4c5d-8929-f5a638492d21] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:25 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0020fc4e0 2 [] false false map[] 0xc0011c5d00 0xc001ad8630}
Jan 20 12:42:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:25.490544  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:25.581441  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:42:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:25.588819  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:42:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:25.588881  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:42:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:25.591034  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:42:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:25.593109  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:42:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:25.593334  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:42:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:25.593367  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:42:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:25.593410  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:42:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:26.164187  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:42:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:26.164259  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:26.175308  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:26 GMT] X-Content-Type-Options:[nosniff]] 0xc000706000 2 [] false false map[] 0xc0016aa400 0xc001c374a0}
Jan 20 12:42:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:26.175447  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:42:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:26.272265  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:42:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:26.272328  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:26.273445  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:26 GMT]] 0xc0007060a0 29 [] true false map[] 0xc0016ab400 <nil>}
Jan 20 12:42:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:26.273561  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:42:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:26.483172  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:26.483198  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:26.488058  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[0fecaaa3-d023-4323-9993-8562e039f372] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:26 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00129c1e0 2 [] false false map[] 0xc000fcc100 0xc0016f6160}
Jan 20 12:42:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:26.488114  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:26.524160  199956 handler.go:293] error while reading "/proc/199956/fd/34" link: readlink /proc/199956/fd/34: no such file or directory
Jan 20 12:42:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:26.581200  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[kube-system/kube-apiserver-zcy-z390-aorus-master]
Jan 20 12:42:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:26.581288  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd updateType=0
Jan 20 12:42:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:26.581348  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd
Jan 20 12:42:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:26.581380  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/kube-apiserver-zcy-z390-aorus-master"
Jan 20 12:42:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:26.581463  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" oldPhase=Running phase=Running
Jan 20 12:42:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:26.581746  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:28 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:41 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:41 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:28 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.239.159.53 PodIPs:[{IP:10.239.159.53}] StartTime:2023-01-20 12:31:28 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:kube-apiserver State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:22 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:6 Image:k8s.gcr.io/kube-apiserver:v1.24.0 ImageID:k8s.gcr.io/kube-apiserver@sha256:a04522b882e919de6141b47d72393fb01226c78e7388400f966198222558c955 ContainerID:containerd://5900ac5c1383a321a6ae837b57d6d29d7a660a102c0806210a9ea57290673062 Started:0xc001a41afe}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:42:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:26.582063  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-apiserver-zcy-z390-aorus-master"
Jan 20 12:42:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:26.582137  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/kube-apiserver-zcy-z390-aorus-master"
Jan 20 12:42:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:26.583259  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/kube-apiserver-zcy-z390-aorus-master"
Jan 20 12:42:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:26.583417  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd isTerminal=false
Jan 20 12:42:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:26.583471  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd updateType=0
Jan 20 12:42:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:26.654273  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" volumeName="ca-certs" volumeSpecName="ca-certs"
Jan 20 12:42:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:26.654383  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Jan 20 12:42:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:26.654454  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" volumeName="etc-pki" volumeSpecName="etc-pki"
Jan 20 12:42:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:26.654521  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Jan 20 12:42:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:26.654591  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Jan 20 12:42:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:26.654657  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Jan 20 12:42:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:26.895034  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:42:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:26.895103  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:26.902851  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:26 GMT] X-Content-Type-Options:[nosniff]] 0xc00129c980 2 [] false false map[] 0xc000fcc600 0xc0006a29a0}
Jan 20 12:42:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:26.902903  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.483938  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.484002  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.492682  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[668289b2-1816-4725-a8d6-cee04dc360b0] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:27 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00129d080 2 [] false false map[] 0xc001716200 0xc000afe580}
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.492735  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.526315  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/etcd.yaml"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.527033  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver.yaml"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.527952  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.528841  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-scheduler.yaml"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.529340  199956 config.go:279] "Setting pods for source" source="file"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.537903  199956 kubelet_getters.go:176] "Pod status updated" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" status=Running
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.537941  199956 kubelet_getters.go:176] "Pod status updated" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" status=Running
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.537950  199956 kubelet_getters.go:176] "Pod status updated" pod="kube-system/etcd-zcy-z390-aorus-master" status=Running
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.537958  199956 kubelet_getters.go:176] "Pod status updated" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" status=Running
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.581461  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.587276  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.587288  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.587295  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.587971  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.588016  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.588023  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.588031  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.632881  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.632902  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.632911  199956 factory.go:255] Factory "raw" can handle container "/user.slice", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.632921  199956 manager.go:925] ignoring container "/user.slice"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.632928  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-user-0.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.632934  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-user-0.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.632943  199956 manager.go:925] ignoring container "/system.slice/run-user-0.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.632950  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-timesyncd.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.632956  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-timesyncd.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.632963  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-timesyncd.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.632971  199956 manager.go:925] ignoring container "/system.slice/systemd-timesyncd.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.632977  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/session-42.scope"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.632983  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/session-42.scope"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.632990  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/session-42.scope", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.632999  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/session-42.scope"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633004  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-fs-fuse-connections.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633011  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-fs-fuse-connections.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633019  199956 manager.go:925] ignoring container "/system.slice/sys-fs-fuse-connections.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633025  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/-.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633031  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/-.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633039  199956 manager.go:925] ignoring container "/system.slice/-.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633044  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-0f9ae677fe935afcf43e145281deae0d663ba95fda6759ff24f0dd3e1cee17a9-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633053  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-0f9ae677fe935afcf43e145281deae0d663ba95fda6759ff24f0dd3e1cee17a9-rootfs.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633064  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-0f9ae677fe935afcf43e145281deae0d663ba95fda6759ff24f0dd3e1cee17a9-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633071  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-shm.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633079  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-shm.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633089  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-shm.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633096  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-kernel-tracing.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633103  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-kernel-tracing.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633111  199956 manager.go:925] ignoring container "/system.slice/sys-kernel-tracing.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633117  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633125  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-rootfs.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633135  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633142  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/NetworkManager.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633148  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/NetworkManager.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633154  199956 factory.go:255] Factory "raw" can handle container "/system.slice/NetworkManager.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633162  199956 manager.go:925] ignoring container "/system.slice/NetworkManager.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633168  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-9b11acac1b891eafc7ff2b244f1c42938f71a575ce81ff917e3b7ebf61d2e045-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633176  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-9b11acac1b891eafc7ff2b244f1c42938f71a575ce81ff917e3b7ebf61d2e045-rootfs.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633186  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-9b11acac1b891eafc7ff2b244f1c42938f71a575ce81ff917e3b7ebf61d2e045-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633194  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-kernel-config.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633200  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-kernel-config.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633209  199956 manager.go:925] ignoring container "/system.slice/sys-kernel-config.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633214  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/user@0.service/dbus.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633220  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/user@0.service/dbus.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633227  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/user@0.service/dbus.socket", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633236  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/user@0.service/dbus.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633242  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-sysusers.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633248  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-sysusers.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633255  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-sysusers.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633264  199956 manager.go:925] ignoring container "/system.slice/systemd-sysusers.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633270  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-rfkill.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633275  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-rfkill.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633282  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-rfkill.socket", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633297  199956 manager.go:925] ignoring container "/system.slice/systemd-rfkill.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633303  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/apparmor.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633309  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/apparmor.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633315  199956 factory.go:255] Factory "raw" can handle container "/system.slice/apparmor.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633324  199956 manager.go:925] ignoring container "/system.slice/apparmor.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633330  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/uuidd.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633335  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/uuidd.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633341  199956 factory.go:255] Factory "raw" can handle container "/system.slice/uuidd.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633349  199956 manager.go:925] ignoring container "/system.slice/uuidd.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633355  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-snapd-ns.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633361  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-snapd-ns.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633369  199956 manager.go:925] ignoring container "/system.slice/run-snapd-ns.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633375  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633383  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-rootfs.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633393  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633401  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-shm.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633409  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-shm.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633420  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-shm.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633427  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-lvm2\\x2dpvscan.slice"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633433  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-lvm2\\x2dpvscan.slice"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633439  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-lvm2\\x2dpvscan.slice", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633447  199956 manager.go:925] ignoring container "/system.slice/system-lvm2\\x2dpvscan.slice"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633453  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-5900ac5c1383a321a6ae837b57d6d29d7a660a102c0806210a9ea57290673062-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633461  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-5900ac5c1383a321a6ae837b57d6d29d7a660a102c0806210a9ea57290673062-rootfs.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633471  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-5900ac5c1383a321a6ae837b57d6d29d7a660a102c0806210a9ea57290673062-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633479  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/switcheroo-control.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633484  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/switcheroo-control.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633491  199956 factory.go:255] Factory "raw" can handle container "/system.slice/switcheroo-control.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633499  199956 manager.go:925] ignoring container "/system.slice/switcheroo-control.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633504  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-resolved.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633510  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-resolved.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633517  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-resolved.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633525  199956 manager.go:925] ignoring container "/system.slice/systemd-resolved.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633531  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2d58469e7a\\x2dc331\\x2d785c\\x2dc760\\x2d93c7232334bd.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633539  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2d58469e7a\\x2dc331\\x2d785c\\x2dc760\\x2d93c7232334bd.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633549  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2d58469e7a\\x2dc331\\x2d785c\\x2dc760\\x2d93c7232334bd.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633555  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/cron.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633560  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/cron.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633567  199956 factory.go:255] Factory "raw" can handle container "/system.slice/cron.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633575  199956 manager.go:925] ignoring container "/system.slice/cron.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633580  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/unattended-upgrades.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633586  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/unattended-upgrades.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633592  199956 factory.go:255] Factory "raw" can handle container "/system.slice/unattended-upgrades.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633600  199956 manager.go:925] ignoring container "/system.slice/unattended-upgrades.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633606  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journald-audit.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633612  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journald-audit.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633618  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journald-audit.socket", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633626  199956 manager.go:925] ignoring container "/system.slice/systemd-journald-audit.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633632  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633640  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-rootfs.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633650  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633658  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-b0713fbc\\x2defc5\\x2d4044\\x2d9d08\\x2d2326a0752f87-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dgcgm6.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633667  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-b0713fbc\\x2defc5\\x2d4044\\x2d9d08\\x2d2326a0752f87-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dgcgm6.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633678  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-b0713fbc\\x2defc5\\x2d4044\\x2d9d08\\x2d2326a0752f87-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dgcgm6.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633686  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/accounts-daemon.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633691  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/accounts-daemon.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633698  199956 factory.go:255] Factory "raw" can handle container "/system.slice/accounts-daemon.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633706  199956 manager.go:925] ignoring container "/system.slice/accounts-daemon.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633711  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-snapd-16292.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633718  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-snapd-16292.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633726  199956 manager.go:925] ignoring container "/system.slice/snap-snapd-16292.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633731  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/cups-browsed.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633737  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/cups-browsed.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633743  199956 factory.go:255] Factory "raw" can handle container "/system.slice/cups-browsed.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633751  199956 manager.go:925] ignoring container "/system.slice/cups-browsed.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633757  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-1000.slice/session-1.scope"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633763  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-1000.slice/session-1.scope"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633770  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-1000.slice/session-1.scope", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633778  199956 manager.go:925] ignoring container "/user.slice/user-1000.slice/session-1.scope"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633783  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-sysctl.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633789  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-sysctl.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633795  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-sysctl.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633804  199956 manager.go:925] ignoring container "/system.slice/systemd-sysctl.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633809  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/openvpn.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633816  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/openvpn.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633822  199956 factory.go:255] Factory "raw" can handle container "/system.slice/openvpn.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633830  199956 manager.go:925] ignoring container "/system.slice/openvpn.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633835  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/ssh.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633841  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/ssh.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633847  199956 factory.go:255] Factory "raw" can handle container "/system.slice/ssh.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633855  199956 manager.go:925] ignoring container "/system.slice/ssh.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633861  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-shm.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633869  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-shm.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633879  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-shm.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633886  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-core20-1611.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633892  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-core20-1611.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633900  199956 manager.go:925] ignoring container "/system.slice/snap-core20-1611.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633906  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633914  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-rootfs.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633924  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633932  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/upower.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633938  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/upower.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633944  199956 factory.go:255] Factory "raw" can handle container "/system.slice/upower.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633952  199956 manager.go:925] ignoring container "/system.slice/upower.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633958  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/networkd-dispatcher.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633964  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/networkd-dispatcher.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633970  199956 factory.go:255] Factory "raw" can handle container "/system.slice/networkd-dispatcher.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633979  199956 manager.go:925] ignoring container "/system.slice/networkd-dispatcher.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633984  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.633993  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-rootfs.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634003  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634010  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/colord.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634015  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/colord.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634021  199956 factory.go:255] Factory "raw" can handle container "/system.slice/colord.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634029  199956 manager.go:925] ignoring container "/system.slice/colord.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634035  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-d2688d45\\x2d2487\\x2d46e7\\x2daecb\\x2de3479626909d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d4pnfq.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634043  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-d2688d45\\x2d2487\\x2d46e7\\x2daecb\\x2de3479626909d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d4pnfq.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634055  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-d2688d45\\x2d2487\\x2d46e7\\x2daecb\\x2de3479626909d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d4pnfq.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634064  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2d719b045b\\x2df019\\x2d025c\\x2d185d\\x2da976163f375a.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634071  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2d719b045b\\x2df019\\x2d025c\\x2d185d\\x2da976163f375a.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634080  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2d719b045b\\x2df019\\x2d025c\\x2d185d\\x2da976163f375a.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634087  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-186424b47c7b9022ecfe8996dc73cd9101f7539259cd65914279c84c9a02a288-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634096  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-186424b47c7b9022ecfe8996dc73cd9101f7539259cd65914279c84c9a02a288-rootfs.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634106  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-186424b47c7b9022ecfe8996dc73cd9101f7539259cd65914279c84c9a02a288-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634113  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-cf25b9ea\\x2d6823\\x2d4eff\\x2db30d\\x2d36a09f9d897e-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dtqzsm.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634122  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-cf25b9ea\\x2d6823\\x2d4eff\\x2db30d\\x2d36a09f9d897e-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dtqzsm.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634133  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-cf25b9ea\\x2d6823\\x2d4eff\\x2db30d\\x2d36a09f9d897e-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dtqzsm.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634141  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snapd.seeded.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634146  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/snapd.seeded.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634153  199956 factory.go:255] Factory "raw" can handle container "/system.slice/snapd.seeded.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634161  199956 manager.go:925] ignoring container "/system.slice/snapd.seeded.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634166  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-bare-5.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634173  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-bare-5.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634182  199956 manager.go:925] ignoring container "/system.slice/snap-bare-5.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634188  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-udevd.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634193  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-udevd.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634199  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-udevd.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634207  199956 manager.go:925] ignoring container "/system.slice/systemd-udevd.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634213  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-shm.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634221  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-shm.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634231  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-shm.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634238  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/kmod-static-nodes.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634244  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/kmod-static-nodes.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634250  199956 factory.go:255] Factory "raw" can handle container "/system.slice/kmod-static-nodes.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634258  199956 manager.go:925] ignoring container "/system.slice/kmod-static-nodes.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634264  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/docker.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634270  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/docker.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634276  199956 factory.go:255] Factory "raw" can handle container "/system.slice/docker.socket", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634284  199956 manager.go:925] ignoring container "/system.slice/docker.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634289  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/ModemManager.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634295  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/ModemManager.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634301  199956 factory.go:255] Factory "raw" can handle container "/system.slice/ModemManager.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634309  199956 manager.go:925] ignoring container "/system.slice/ModemManager.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634314  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-shm.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634323  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-shm.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634333  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-shm.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634340  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2d562f9e70\\x2db2b7\\x2d3ac4\\x2d2311\\x2db91510a5a9ac.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634348  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2d562f9e70\\x2db2b7\\x2d3ac4\\x2d2311\\x2db91510a5a9ac.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634357  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2d562f9e70\\x2db2b7\\x2d3ac4\\x2d2311\\x2db91510a5a9ac.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634364  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/gdm.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634369  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/gdm.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634376  199956 factory.go:255] Factory "raw" can handle container "/system.slice/gdm.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634383  199956 manager.go:925] ignoring container "/system.slice/gdm.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634389  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-shm.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634397  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-shm.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634407  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-shm.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634415  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b3af3b285c950aba4fcd0176473261f7f4700aeaafe9c73ae15b15a7d6304092-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634423  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b3af3b285c950aba4fcd0176473261f7f4700aeaafe9c73ae15b15a7d6304092-rootfs.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634439  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b3af3b285c950aba4fcd0176473261f7f4700aeaafe9c73ae15b15a7d6304092-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634446  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-7af065b7\\x2d9095\\x2d4d91\\x2d9b9e\\x2d2644e7b1f4bf-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dx6vjr.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634456  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-7af065b7\\x2d9095\\x2d4d91\\x2d9b9e\\x2d2644e7b1f4bf-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dx6vjr.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634467  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-7af065b7\\x2d9095\\x2d4d91\\x2d9b9e\\x2d2644e7b1f4bf-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dx6vjr.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634475  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/udisks2.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634480  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/udisks2.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634486  199956 factory.go:255] Factory "raw" can handle container "/system.slice/udisks2.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634494  199956 manager.go:925] ignoring container "/system.slice/udisks2.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634500  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journal-flush.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634505  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journal-flush.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634512  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journal-flush.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634520  199956 manager.go:925] ignoring container "/system.slice/systemd-journal-flush.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634526  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journald.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634532  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journald.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634538  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journald.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634547  199956 manager.go:925] ignoring container "/system.slice/systemd-journald.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634552  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-initctl.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634558  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-initctl.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634565  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-initctl.socket", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634573  199956 manager.go:925] ignoring container "/system.slice/systemd-initctl.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634578  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634587  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-rootfs.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634597  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634604  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dm-event.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634610  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/dm-event.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634616  199956 factory.go:255] Factory "raw" can handle container "/system.slice/dm-event.socket", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634624  199956 manager.go:925] ignoring container "/system.slice/dm-event.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634630  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f78ed441ff85d669a403a76c0987f2d86913431bd96d454b1a3605bdcf0474f2-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634638  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f78ed441ff85d669a403a76c0987f2d86913431bd96d454b1a3605bdcf0474f2-rootfs.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634648  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f78ed441ff85d669a403a76c0987f2d86913431bd96d454b1a3605bdcf0474f2-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634656  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/NetworkManager-wait-online.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634662  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/NetworkManager-wait-online.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634668  199956 factory.go:255] Factory "raw" can handle container "/system.slice/NetworkManager-wait-online.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634676  199956 manager.go:925] ignoring container "/system.slice/NetworkManager-wait-online.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634682  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634688  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634694  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634702  199956 manager.go:925] ignoring container "/user.slice/user-0.slice"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634707  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-fsckd.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634712  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-fsckd.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634719  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-fsckd.socket", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634727  199956 manager.go:925] ignoring container "/system.slice/systemd-fsckd.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634732  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-shm.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634740  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-shm.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634751  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-shm.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634759  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-gtk\\x2dcommon\\x2dthemes-1535.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634766  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-gtk\\x2dcommon\\x2dthemes-1535.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634774  199956 manager.go:925] ignoring container "/system.slice/snap-gtk\\x2dcommon\\x2dthemes-1535.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634780  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/session-44.scope"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634785  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/session-44.scope"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634792  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/session-44.scope", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634801  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/session-44.scope"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634806  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/rtkit-daemon.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634812  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/rtkit-daemon.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634818  199956 factory.go:255] Factory "raw" can handle container "/system.slice/rtkit-daemon.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634826  199956 manager.go:925] ignoring container "/system.slice/rtkit-daemon.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634832  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-1cb44bdbde0b41852e382d7dab7c184af8dd4e5b25d5cfe6a10a9c8ab601659d-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634841  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-1cb44bdbde0b41852e382d7dab7c184af8dd4e5b25d5cfe6a10a9c8ab601659d-rootfs.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634851  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-1cb44bdbde0b41852e382d7dab7c184af8dd4e5b25d5cfe6a10a9c8ab601659d-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634858  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-shm.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634866  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-shm.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634876  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-shm.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634884  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snapd.apparmor.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634889  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/snapd.apparmor.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634896  199956 factory.go:255] Factory "raw" can handle container "/system.slice/snapd.apparmor.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634904  199956 manager.go:925] ignoring container "/system.slice/snapd.apparmor.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634909  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/avahi-daemon.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634915  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/avahi-daemon.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634921  199956 factory.go:255] Factory "raw" can handle container "/system.slice/avahi-daemon.socket", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634929  199956 manager.go:925] ignoring container "/system.slice/avahi-daemon.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634935  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-1000.slice"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634940  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-1000.slice"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634946  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-1000.slice", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634954  199956 manager.go:925] ignoring container "/user.slice/user-1000.slice"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634960  199956 factory.go:262] Factory "containerd" was unable to handle container "/init.scope"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634965  199956 factory.go:262] Factory "systemd" was unable to handle container "/init.scope"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634971  199956 factory.go:255] Factory "raw" can handle container "/init.scope", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634979  199956 manager.go:925] ignoring container "/init.scope"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634984  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/ufw.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634989  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/ufw.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.634995  199956 factory.go:255] Factory "raw" can handle container "/system.slice/ufw.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635003  199956 manager.go:925] ignoring container "/system.slice/ufw.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635009  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dev-hugepages.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635015  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/dev-hugepages.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635024  199956 manager.go:925] ignoring container "/system.slice/dev-hugepages.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635029  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-user-1000.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635036  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-user-1000.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635044  199956 manager.go:925] ignoring container "/system.slice/run-user-1000.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635049  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/acpid.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635055  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/acpid.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635062  199956 factory.go:255] Factory "raw" can handle container "/system.slice/acpid.socket", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635070  199956 manager.go:925] ignoring container "/system.slice/acpid.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635077  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f8443d2868215ec42d3fa335663976e33df166c5e98369aa3f9d7ddb9235bead-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635085  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f8443d2868215ec42d3fa335663976e33df166c5e98369aa3f9d7ddb9235bead-rootfs.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635095  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f8443d2868215ec42d3fa335663976e33df166c5e98369aa3f9d7ddb9235bead-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635102  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-machine-id-commit.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635108  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-machine-id-commit.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635115  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-machine-id-commit.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635123  199956 manager.go:925] ignoring container "/system.slice/systemd-machine-id-commit.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635129  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snapd.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635134  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/snapd.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635140  199956 factory.go:255] Factory "raw" can handle container "/system.slice/snapd.socket", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635148  199956 manager.go:925] ignoring container "/system.slice/snapd.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635153  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dbus.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635159  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/dbus.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635165  199956 factory.go:255] Factory "raw" can handle container "/system.slice/dbus.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635173  199956 manager.go:925] ignoring container "/system.slice/dbus.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635178  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/boot-efi.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635185  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/boot-efi.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635193  199956 manager.go:925] ignoring container "/system.slice/boot-efi.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635198  199956 factory.go:262] Factory "containerd" was unable to handle container "/kata_overhead"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635204  199956 factory.go:262] Factory "systemd" was unable to handle container "/kata_overhead"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635210  199956 factory.go:255] Factory "raw" can handle container "/kata_overhead", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635218  199956 manager.go:925] ignoring container "/kata_overhead"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635223  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/lvm2-lvmpolld.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635229  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/lvm2-lvmpolld.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635236  199956 factory.go:255] Factory "raw" can handle container "/system.slice/lvm2-lvmpolld.socket", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635244  199956 manager.go:925] ignoring container "/system.slice/lvm2-lvmpolld.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635249  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snapd.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635254  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/snapd.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635261  199956 factory.go:255] Factory "raw" can handle container "/system.slice/snapd.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635269  199956 manager.go:925] ignoring container "/system.slice/snapd.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635274  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/kerneloops.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635280  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/kerneloops.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635286  199956 factory.go:255] Factory "raw" can handle container "/system.slice/kerneloops.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635294  199956 manager.go:925] ignoring container "/system.slice/kerneloops.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635300  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-tmpfiles-setup.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635306  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-tmpfiles-setup.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635312  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-tmpfiles-setup.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635321  199956 manager.go:925] ignoring container "/system.slice/systemd-tmpfiles-setup.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635327  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/cups.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635332  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/cups.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635339  199956 factory.go:255] Factory "raw" can handle container "/system.slice/cups.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635347  199956 manager.go:925] ignoring container "/system.slice/cups.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635352  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-1000.slice/user-runtime-dir@1000.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635358  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-1000.slice/user-runtime-dir@1000.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635364  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-1000.slice/user-runtime-dir@1000.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635373  199956 manager.go:925] ignoring container "/user.slice/user-1000.slice/user-runtime-dir@1000.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635379  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/lvm2-monitor.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635384  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/lvm2-monitor.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635390  199956 factory.go:255] Factory "raw" can handle container "/system.slice/lvm2-monitor.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635398  199956 manager.go:925] ignoring container "/system.slice/lvm2-monitor.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635404  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/rsyslog.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635409  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/rsyslog.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635415  199956 factory.go:255] Factory "raw" can handle container "/system.slice/rsyslog.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635423  199956 manager.go:925] ignoring container "/system.slice/rsyslog.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635430  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/acpid.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635435  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/acpid.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635442  199956 factory.go:255] Factory "raw" can handle container "/system.slice/acpid.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635450  199956 manager.go:925] ignoring container "/system.slice/acpid.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635455  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dev-mqueue.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635461  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/dev-mqueue.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635469  199956 manager.go:925] ignoring container "/system.slice/dev-mqueue.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635475  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-04e472cb\\x2db6f9\\x2d4065\\x2db509\\x2dd7e1579fc48d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dhqj8d.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635484  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-04e472cb\\x2db6f9\\x2d4065\\x2db509\\x2dd7e1579fc48d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dhqj8d.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635495  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-04e472cb\\x2db6f9\\x2d4065\\x2db509\\x2dd7e1579fc48d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dhqj8d.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635503  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-user-1000-gvfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635510  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-user-1000-gvfs.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635518  199956 manager.go:925] ignoring container "/system.slice/run-user-1000-gvfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635524  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635533  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-rootfs.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635542  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635550  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-udevd-kernel.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635556  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-udevd-kernel.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635562  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-udevd-kernel.socket", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635571  199956 manager.go:925] ignoring container "/system.slice/systemd-udevd-kernel.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635581  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/bluetooth.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635586  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/bluetooth.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635593  199956 factory.go:255] Factory "raw" can handle container "/system.slice/bluetooth.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635601  199956 manager.go:925] ignoring container "/system.slice/bluetooth.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635606  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/syslog.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635611  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/syslog.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635618  199956 factory.go:255] Factory "raw" can handle container "/system.slice/syslog.socket", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635626  199956 manager.go:925] ignoring container "/system.slice/syslog.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635631  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/polkit.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635636  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/polkit.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635642  199956 factory.go:255] Factory "raw" can handle container "/system.slice/polkit.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635651  199956 manager.go:925] ignoring container "/system.slice/polkit.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635656  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-kernel-debug-tracing.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635663  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-kernel-debug-tracing.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635672  199956 manager.go:925] ignoring container "/system.slice/sys-kernel-debug-tracing.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635677  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/setvtrgb.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635683  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/setvtrgb.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635689  199956 factory.go:255] Factory "raw" can handle container "/system.slice/setvtrgb.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635697  199956 manager.go:925] ignoring container "/system.slice/setvtrgb.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635702  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/wpa_supplicant.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635708  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/wpa_supplicant.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635714  199956 factory.go:255] Factory "raw" can handle container "/system.slice/wpa_supplicant.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635722  199956 manager.go:925] ignoring container "/system.slice/wpa_supplicant.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635728  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/user@0.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635734  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/user@0.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635740  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/user@0.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635748  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/user@0.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635754  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/user@0.service/dbus.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635759  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/user@0.service/dbus.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635766  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/user@0.service/dbus.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635774  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/user@0.service/dbus.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635780  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-1000.slice/user@1000.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635788  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-1000.slice/user@1000.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635795  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-1000.slice/user@1000.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635804  199956 manager.go:925] ignoring container "/user.slice/user-1000.slice/user@1000.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635810  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-systemd\\x2dfsck.slice/systemd-fsck@dev-disk-by\\x2duuid-4B5A\\x2dDC89.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635818  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-systemd\\x2dfsck.slice/systemd-fsck@dev-disk-by\\x2duuid-4B5A\\x2dDC89.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635827  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-systemd\\x2dfsck.slice/systemd-fsck@dev-disk-by\\x2duuid-4B5A\\x2dDC89.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635836  199956 manager.go:925] ignoring container "/system.slice/system-systemd\\x2dfsck.slice/systemd-fsck@dev-disk-by\\x2duuid-4B5A\\x2dDC89.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635843  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-kernel-debug.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635849  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-kernel-debug.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635858  199956 manager.go:925] ignoring container "/system.slice/sys-kernel-debug.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635863  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-13149330f3752d0ff65bcac86c7b522b2040811e815fbc87e56b40b612875d5a-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635872  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-13149330f3752d0ff65bcac86c7b522b2040811e815fbc87e56b40b612875d5a-rootfs.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635882  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-13149330f3752d0ff65bcac86c7b522b2040811e815fbc87e56b40b612875d5a-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635889  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-snap\\x2dstore-558.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635896  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-snap\\x2dstore-558.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635905  199956 manager.go:925] ignoring container "/system.slice/snap-snap\\x2dstore-558.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635910  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/alsa-restore.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635916  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/alsa-restore.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635922  199956 factory.go:255] Factory "raw" can handle container "/system.slice/alsa-restore.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635931  199956 manager.go:925] ignoring container "/system.slice/alsa-restore.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635936  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/console-setup.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635942  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/console-setup.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635948  199956 factory.go:255] Factory "raw" can handle container "/system.slice/console-setup.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635956  199956 manager.go:925] ignoring container "/system.slice/console-setup.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635962  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-udev-trigger.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635968  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-udev-trigger.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635974  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-udev-trigger.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635982  199956 manager.go:925] ignoring container "/system.slice/systemd-udev-trigger.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635988  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-tmpfiles-setup-dev.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.635994  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-tmpfiles-setup-dev.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636000  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-tmpfiles-setup-dev.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636009  199956 manager.go:925] ignoring container "/system.slice/systemd-tmpfiles-setup-dev.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636016  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/proc-sys-fs-binfmt_misc.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636023  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/proc-sys-fs-binfmt_misc.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636031  199956 manager.go:925] ignoring container "/system.slice/proc-sys-fs-binfmt_misc.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636037  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-shm.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636045  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-shm.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636055  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-shm.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636063  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-logind.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636068  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-logind.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636075  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-logind.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636083  199956 manager.go:925] ignoring container "/system.slice/systemd-logind.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636089  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-modprobe.slice"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636094  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-modprobe.slice"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636100  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-modprobe.slice", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636108  199956 manager.go:925] ignoring container "/system.slice/system-modprobe.slice"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636114  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636122  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-rootfs.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636132  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636139  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/user-runtime-dir@0.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636145  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/user-runtime-dir@0.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636152  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/user-runtime-dir@0.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636160  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/user-runtime-dir@0.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636166  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e489181a7f95431b6d92f037dbe3f788b46a5e024df7aaf29e0115dab1168ab1-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636174  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e489181a7f95431b6d92f037dbe3f788b46a5e024df7aaf29e0115dab1168ab1-rootfs.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636184  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e489181a7f95431b6d92f037dbe3f788b46a5e024df7aaf29e0115dab1168ab1-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636191  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/uuidd.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636196  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/uuidd.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636202  199956 factory.go:255] Factory "raw" can handle container "/system.slice/uuidd.socket", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636210  199956 manager.go:925] ignoring container "/system.slice/uuidd.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636216  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/blk-availability.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636221  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/blk-availability.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636228  199956 factory.go:255] Factory "raw" can handle container "/system.slice/blk-availability.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636236  199956 manager.go:925] ignoring container "/system.slice/blk-availability.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636243  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/keyboard-setup.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636249  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/keyboard-setup.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636255  199956 factory.go:255] Factory "raw" can handle container "/system.slice/keyboard-setup.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636263  199956 manager.go:925] ignoring container "/system.slice/keyboard-setup.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636269  199956 factory.go:262] Factory "containerd" was unable to handle container "/docker"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636274  199956 factory.go:262] Factory "systemd" was unable to handle container "/docker"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636280  199956 factory.go:255] Factory "raw" can handle container "/docker", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636288  199956 manager.go:925] ignoring container "/docker"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636293  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/containerd.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636298  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/containerd.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636305  199956 factory.go:255] Factory "raw" can handle container "/system.slice/containerd.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636313  199956 manager.go:925] ignoring container "/system.slice/containerd.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636319  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e10efaa459a32161059fda75d4bcaf3bbdb896781f772b508e43fe00bc7c9003-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636327  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e10efaa459a32161059fda75d4bcaf3bbdb896781f772b508e43fe00bc7c9003-rootfs.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636337  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e10efaa459a32161059fda75d4bcaf3bbdb896781f772b508e43fe00bc7c9003-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636344  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/irqbalance.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636349  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/irqbalance.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636356  199956 factory.go:255] Factory "raw" can handle container "/system.slice/irqbalance.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636364  199956 manager.go:925] ignoring container "/system.slice/irqbalance.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636369  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-gnome\\x2d3\\x2d38\\x2d2004-115.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636376  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-gnome\\x2d3\\x2d38\\x2d2004-115.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636385  199956 manager.go:925] ignoring container "/system.slice/snap-gnome\\x2d3\\x2d38\\x2d2004-115.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636391  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636399  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-rootfs.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636409  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636416  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journald.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636421  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journald.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636427  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journald.socket", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636436  199956 manager.go:925] ignoring container "/system.slice/systemd-journald.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636441  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dbus.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636447  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/dbus.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636453  199956 factory.go:255] Factory "raw" can handle container "/system.slice/dbus.socket", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636462  199956 manager.go:925] ignoring container "/system.slice/dbus.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636468  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-lvm2\\x2dpvscan.slice/lvm2-pvscan@8:10.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636474  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-lvm2\\x2dpvscan.slice/lvm2-pvscan@8:10.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636481  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-lvm2\\x2dpvscan.slice/lvm2-pvscan@8:10.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636490  199956 manager.go:925] ignoring container "/system.slice/system-lvm2\\x2dpvscan.slice/lvm2-pvscan@8:10.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636496  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-update-utmp.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636501  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-update-utmp.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636508  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-update-utmp.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636516  199956 manager.go:925] ignoring container "/system.slice/systemd-update-utmp.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636521  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-modules-load.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636527  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-modules-load.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636533  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-modules-load.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636542  199956 manager.go:925] ignoring container "/system.slice/systemd-modules-load.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636547  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636555  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-rootfs.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636565  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636573  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2d8b7da23a\\x2d3511\\x2dfe06\\x2d72d7\\x2d26a549eb607d.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636581  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2d8b7da23a\\x2d3511\\x2dfe06\\x2d72d7\\x2d26a549eb607d.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636590  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2d8b7da23a\\x2d3511\\x2dfe06\\x2d72d7\\x2d26a549eb607d.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636596  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-d94e1927\\x2dd22d\\x2d4831\\x2da764\\x2d0170eae346a1-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d9qh7j.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636605  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-d94e1927\\x2dd22d\\x2d4831\\x2da764\\x2d0170eae346a1-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d9qh7j.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636618  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-d94e1927\\x2dd22d\\x2d4831\\x2da764\\x2d0170eae346a1-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d9qh7j.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636625  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636634  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-rootfs.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636645  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636652  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-1c057a0e\\x2d3ee3\\x2d44f3\\x2db294\\x2d434acc8f9491-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d2sbqp.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636660  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-1c057a0e\\x2d3ee3\\x2d44f3\\x2db294\\x2d434acc8f9491-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d2sbqp.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636671  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-1c057a0e\\x2d3ee3\\x2d44f3\\x2db294\\x2d434acc8f9491-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d2sbqp.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636679  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-random-seed.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636685  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-random-seed.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636698  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-random-seed.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636707  199956 manager.go:925] ignoring container "/system.slice/systemd-random-seed.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636715  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-getty.slice"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636721  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-getty.slice"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636727  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-getty.slice", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636735  199956 manager.go:925] ignoring container "/system.slice/system-getty.slice"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636741  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-remount-fs.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636747  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-remount-fs.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636753  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-remount-fs.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636762  199956 manager.go:925] ignoring container "/system.slice/systemd-remount-fs.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636768  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-user-sessions.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636774  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-user-sessions.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636780  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-user-sessions.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636789  199956 manager.go:925] ignoring container "/system.slice/systemd-user-sessions.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636794  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e0c5b991f81d5128566686552e45a9bbc8c584e4f6c94909edb3a42720dacc90-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636802  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e0c5b991f81d5128566686552e45a9bbc8c584e4f6c94909edb3a42720dacc90-rootfs.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636813  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e0c5b991f81d5128566686552e45a9bbc8c584e4f6c94909edb3a42720dacc90-rootfs.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636820  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/thermald.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636825  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/thermald.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636832  199956 factory.go:255] Factory "raw" can handle container "/system.slice/thermald.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636840  199956 manager.go:925] ignoring container "/system.slice/thermald.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636845  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journald-dev-log.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636851  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journald-dev-log.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636857  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journald-dev-log.socket", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636866  199956 manager.go:925] ignoring container "/system.slice/systemd-journald-dev-log.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636871  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636876  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636882  199956 factory.go:255] Factory "raw" can handle container "/system.slice", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636890  199956 manager.go:925] ignoring container "/system.slice"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636909  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/cups.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636911  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/cups.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636914  199956 factory.go:255] Factory "raw" can handle container "/system.slice/cups.socket", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636918  199956 manager.go:925] ignoring container "/system.slice/cups.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636921  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-udevd-control.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636925  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-udevd-control.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636929  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-udevd-control.socket", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636933  199956 manager.go:925] ignoring container "/system.slice/systemd-udevd-control.socket"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636935  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/apport.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636938  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/apport.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636941  199956 factory.go:255] Factory "raw" can handle container "/system.slice/apport.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636946  199956 manager.go:925] ignoring container "/system.slice/apport.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636949  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/session-40.scope"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636952  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/session-40.scope"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636955  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/session-40.scope", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636959  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/session-40.scope"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636962  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2dd678d9bc\\x2d8fcb\\x2d9fbe\\x2d1142\\x2ddede54862bab.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636965  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2dd678d9bc\\x2d8fcb\\x2d9fbe\\x2d1142\\x2ddede54862bab.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636970  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2dd678d9bc\\x2d8fcb\\x2d9fbe\\x2d1142\\x2ddede54862bab.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636973  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-shm.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636977  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-shm.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636982  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-shm.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636985  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-snapd-ns-snap\\x2dstore.mnt.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636989  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-snapd-ns-snap\\x2dstore.mnt.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636993  199956 manager.go:925] ignoring container "/system.slice/run-snapd-ns-snap\\x2dstore.mnt.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636996  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/avahi-daemon.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.636999  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/avahi-daemon.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.637002  199956 factory.go:255] Factory "raw" can handle container "/system.slice/avahi-daemon.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.637006  199956 manager.go:925] ignoring container "/system.slice/avahi-daemon.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.637008  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/docker.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.637011  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/docker.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.637014  199956 factory.go:255] Factory "raw" can handle container "/system.slice/docker.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.637018  199956 manager.go:925] ignoring container "/system.slice/docker.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.637021  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-shm.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.637025  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-shm.mount", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.637030  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-shm.mount"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.637033  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-systemd\\x2dfsck.slice"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.637037  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-systemd\\x2dfsck.slice"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.637040  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-systemd\\x2dfsck.slice", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.637044  199956 manager.go:925] ignoring container "/system.slice/system-systemd\\x2dfsck.slice"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.637047  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/whoopsie.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.637050  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/whoopsie.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.637055  199956 factory.go:255] Factory "raw" can handle container "/system.slice/whoopsie.service", but ignoring.
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.637059  199956 manager.go:925] ignoring container "/system.slice/whoopsie.service"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.644713  199956 qos_container_manager_linux.go:379] "Updated QoS cgroup configuration"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.653453  199956 kuberuntime_gc.go:171] "Removing sandbox" sandboxID="e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:42:27.653530996-05:00" level=info msg="StopPodSandbox for \"e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b\""
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:42:27.656108837-05:00" level=info msg="TearDown network for sandbox \"e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b\" successfully"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:42:27.656124415-05:00" level=info msg="StopPodSandbox for \"e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b\" returns successfully"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:42:27.656295649-05:00" level=info msg="RemovePodSandbox for \"e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b\""
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:42:27.656320050-05:00" level=info msg="Forcibly stopping sandbox \"e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b\""
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:42:27.658785127-05:00" level=info msg="TearDown network for sandbox \"e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b\" successfully"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:42:27.661379234-05:00" level=info msg="RemovePodSandbox \"e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b\" returns successfully"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.661779  199956 kubelet.go:1280] "Container garbage collection succeeded"
Jan 20 12:42:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:27.811669  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:42:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:28.453650  199956 generic.go:155] "GenericPLEG" podUID=4f97314d-815b-4787-b174-5c158cd28c9d containerID="e00667442f988fc228417ea792d5cf3f378d0fec1fa1e351c62cf42fea94619b" oldState=exited newState=non-existent
Jan 20 12:42:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:28.454511  199956 kuberuntime_manager.go:1037] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603] pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:28.455940  199956 generic.go:421] "PLEG: Write status" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:42:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:28.484029  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:28.484089  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:28.492141  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[a96ee096-01bd-426e-9221-0c23af0860ea] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:28 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0002bfa20 2 [] false false map[] 0xc001a4a400 0xc00198c000}
Jan 20 12:42:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:28.492169  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:28.878696  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:42:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:28.878760  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:28.880020  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:28 GMT] X-Content-Type-Options:[nosniff]] 0xc0014867a0 2 [] true false map[] 0xc001a4a800 <nil>}
Jan 20 12:42:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:28.880135  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.483877  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.483949  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.498418  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[78fac575-7f82-4736-aa3b-81aa97ceb5ef] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:29 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001486d60 2 [] false false map[] 0xc0014ca200 0xc0019f9d90}
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.498553  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.582069  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[kube-flannel/kube-flannel-ds-hprn4]
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.582154  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.582344  199956 pod_workers.go:888] "Processing pod event" pod="kube-flannel/kube-flannel-ds-hprn4" podUID=04e472cb-b6f9-4065-b509-d7e1579fc48d updateType=0
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.582428  199956 kubelet.go:1501] "syncPod enter" pod="kube-flannel/kube-flannel-ds-hprn4" podUID=04e472cb-b6f9-4065-b509-d7e1579fc48d
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.582462  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.582578  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-flannel/kube-flannel-ds-hprn4" oldPhase=Running phase=Running
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.582970  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-flannel/kube-flannel-ds-hprn4" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:44 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:45 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:45 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.239.159.53 PodIPs:[{IP:10.239.159.53}] StartTime:2023-01-20 12:31:42 -0500 EST InitContainerStatuses:[{Name:install-cni-plugin State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2023-01-20 12:31:43 -0500 EST,FinishedAt:2023-01-20 12:31:43 -0500 EST,ContainerID:containerd://349eee1bf5a2d95206979822c956ea6f555dbb786434c7b9fe46524872f1a18d,}} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:docker.io/rancher/mirrored-flannelcni-flannel-cni-plugin:v1.1.0 ImageID:docker.io/rancher/mirrored-flannelcni-flannel-cni-plugin@sha256:28d3a6be9f450282bf42e4dad143d41da23e3d91f66f19c01ee7fd21fd17cb2b ContainerID:containerd://349eee1bf5a2d95206979822c956ea6f555dbb786434c7b9fe46524872f1a18d Started:<nil>} {Name:install-cni State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2023-01-20 12:31:43 -0500 EST,FinishedAt:2023-01-20 12:31:43 -0500 EST,ContainerID:containerd://85a4ede91a47b8d7df192dc39004a2a7fa45e095191a7335eaed8ec826cd9f36,}} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:docker.io/rancher/mirrored-flannelcni-flannel:v0.19.1 ImageID:docker.io/rancher/mirrored-flannelcni-flannel@sha256:e3b0f06121b0f7a11a684e910bba89b3ab49cd6e73aeb6c719f83b2456946366 ContainerID:containerd://85a4ede91a47b8d7df192dc39004a2a7fa45e095191a7335eaed8ec826cd9f36 Started:<nil>}] ContainerStatuses:[{Name:kube-flannel State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:44 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:docker.io/rancher/mirrored-flannelcni-flannel:v0.19.1 ImageID:docker.io/rancher/mirrored-flannelcni-flannel@sha256:e3b0f06121b0f7a11a684e910bba89b3ab49cd6e73aeb6c719f83b2456946366 ContainerID:containerd://0f9ae677fe935afcf43e145281deae0d663ba95fda6759ff24f0dd3e1cee17a9 Started:0xc001ea6599}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.583367  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.583438  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.583859  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.584027  199956 kubelet.go:1503] "syncPod exit" pod="kube-flannel/kube-flannel-ds-hprn4" podUID=04e472cb-b6f9-4065-b509-d7e1579fc48d isTerminal=false
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.584080  199956 pod_workers.go:988] "Processing pod event done" pod="kube-flannel/kube-flannel-ds-hprn4" podUID=04e472cb-b6f9-4065-b509-d7e1579fc48d updateType=0
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.588221  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.588232  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.588985  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.589439  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.589483  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.589488  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.589497  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.674048  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="run" volumeSpecName="run"
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.674154  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="cni-plugin" volumeSpecName="cni-plugin"
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.674226  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="cni" volumeSpecName="cni"
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.674305  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="flannel-cfg" volumeSpecName="flannel-cfg"
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.674380  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="xtables-lock" volumeSpecName="xtables-lock"
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.674497  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="kube-api-access-hqj8d" volumeSpecName="kube-api-access-hqj8d"
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.693285  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"flannel-cfg\" (UniqueName: \"kubernetes.io/configmap/04e472cb-b6f9-4065-b509-d7e1579fc48d-flannel-cfg\") pod \"kube-flannel-ds-hprn4\" (UID: \"04e472cb-b6f9-4065-b509-d7e1579fc48d\") Volume is already mounted to pod, but remount was requested." pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.693421  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-hqj8d\" (UniqueName: \"kubernetes.io/projected/04e472cb-b6f9-4065-b509-d7e1579fc48d-kube-api-access-hqj8d\") pod \"kube-flannel-ds-hprn4\" (UID: \"04e472cb-b6f9-4065-b509-d7e1579fc48d\") Volume is already mounted to pod, but remount was requested." pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.693562  199956 projected.go:183] Setting up volume kube-api-access-hqj8d for pod 04e472cb-b6f9-4065-b509-d7e1579fc48d at /var/lib/kubelet/pods/04e472cb-b6f9-4065-b509-d7e1579fc48d/volumes/kubernetes.io~projected/kube-api-access-hqj8d
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.693598  199956 configmap.go:181] Setting up volume flannel-cfg for pod 04e472cb-b6f9-4065-b509-d7e1579fc48d at /var/lib/kubelet/pods/04e472cb-b6f9-4065-b509-d7e1579fc48d/volumes/kubernetes.io~configmap/flannel-cfg
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.693727  199956 configmap.go:205] Received configMap kube-flannel/kube-flannel-cfg containing (2) pieces of data, 365 total bytes
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.693799  199956 quota_linux.go:271] SupportsQuotas called, but quotas disabled
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.694001  199956 atomic_writer.go:161] pod kube-flannel/kube-flannel-ds-hprn4 volume kube-api-access-hqj8d: no update required for target directory /var/lib/kubelet/pods/04e472cb-b6f9-4065-b509-d7e1579fc48d/volumes/kubernetes.io~projected/kube-api-access-hqj8d
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.694029  199956 atomic_writer.go:161] pod kube-flannel/kube-flannel-ds-hprn4 volume flannel-cfg: no update required for target directory /var/lib/kubelet/pods/04e472cb-b6f9-4065-b509-d7e1579fc48d/volumes/kubernetes.io~configmap/flannel-cfg
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.694069  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-hqj8d\" (UniqueName: \"kubernetes.io/projected/04e472cb-b6f9-4065-b509-d7e1579fc48d-kube-api-access-hqj8d\") pod \"kube-flannel-ds-hprn4\" (UID: \"04e472cb-b6f9-4065-b509-d7e1579fc48d\") " pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.694102  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"flannel-cfg\" (UniqueName: \"kubernetes.io/configmap/04e472cb-b6f9-4065-b509-d7e1579fc48d-flannel-cfg\") pod \"kube-flannel-ds-hprn4\" (UID: \"04e472cb-b6f9-4065-b509-d7e1579fc48d\") " pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:42:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:29.997313  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:42:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:30.005336  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:42:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:30.015140  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59496688Ki" capacity="65586124Ki" time="2023-01-20 12:42:29.999306736 -0500 EST m=+662.555081622"
Jan 20 12:42:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:30.015156  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64393228Ki" capacity="65061836Ki" time="2023-01-20 12:42:30.015081258 -0500 EST m=+662.570856083"
Jan 20 12:42:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:30.015163  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902011736Ki" capacity="981310056Ki" time="2023-01-20 12:42:29.999306736 -0500 EST m=+662.555081622"
Jan 20 12:42:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:30.015170  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:42:29.999306736 -0500 EST m=+662.555081622"
Jan 20 12:42:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:30.015175  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902011736Ki" capacity="981310056Ki" time="2023-01-20 12:42:21.262156717 -0500 EST"
Jan 20 12:42:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:30.015181  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:42:21.262156717 -0500 EST"
Jan 20 12:42:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:30.015187  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510775" capacity="511757" time="2023-01-20 12:42:30.014820038 -0500 EST m=+662.570594863"
Jan 20 12:42:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:30.015213  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:42:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:30.484294  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:30.484465  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:30.493405  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[4780b437-e1e4-449e-a3f5-49398c7d3ecd] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:30 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e8040 2 [] false false map[] 0xc0014c6b00 0xc001fda630}
Jan 20 12:42:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:30.493436  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:31.166109  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:42:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:31.166182  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:31.174639  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[dcb23451-9171-4d51-8c33-cc11a9fa5a56] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:31 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0020fc020 2 [] false false map[] 0xc001a4a800 0xc00112eb00}
Jan 20 12:42:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:31.174710  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:31.483722  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:31.483790  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:31.491616  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[00ad1499-921d-46ad-9f95-d4bf8963f59b] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:31 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0020fc420 2 [] false false map[] 0xc0014c7000 0xc0002b6160}
Jan 20 12:42:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:31.491647  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:31.581953  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:42:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:31.587399  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:42:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:31.587412  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:42:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:31.587419  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:42:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:31.588122  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:42:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:31.588171  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:42:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:31.588177  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:42:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:31.588185  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:42:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:31.659788  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:42:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:31.659794  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:42:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:31.659946  199956 interface.go:209] Interface eno2 is up
Jan 20 12:42:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:31.659973  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:42:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:31.659979  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:42:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:31.659984  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:42:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:31.659987  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:42:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:31.659991  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:42:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:31.660290  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:42:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:31.660311  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:42:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:31.660315  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:42:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:31.660319  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:42:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:32.483735  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:32.483803  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:32.491320  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[95111cfe-588e-43c9-b155-70fb6fe1b8aa] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:32 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0000a5640 2 [] false false map[] 0xc000339800 0xc0013b0c60}
Jan 20 12:42:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:32.491354  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:32.582141  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr]
Jan 20 12:42:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:32.582220  199956 pod_workers.go:888] "Processing pod event" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d updateType=0
Jan 20 12:42:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:32.582281  199956 kubelet.go:1501] "syncPod enter" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d
Jan 20 12:42:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:32.582313  199956 kubelet_pods.go:1435] "Generating pod status" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:42:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:32.582416  199956 kubelet_pods.go:1447] "Got phase for pod" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" oldPhase=Running phase=Running
Jan 20 12:42:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:32.582812  199956 status_manager.go:535] "Ignoring same status for pod" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:58 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:08 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:08 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:58 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.4 PodIPs:[{IP:10.244.0.4}] StartTime:2023-01-20 12:31:58 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:kube-rbac-proxy State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:59 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:gcr.io/kubebuilder/kube-rbac-proxy:v0.13.0 ImageID:gcr.io/kubebuilder/kube-rbac-proxy@sha256:d99a8d144816b951a67648c12c0b988936ccd25cf3754f3cd85ab8c01592248f ContainerID:containerd://1cb44bdbde0b41852e382d7dab7c184af8dd4e5b25d5cfe6a10a9c8ab601659d Started:0xc001a40cdf} {Name:manager State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:59 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:quay.io/confidential-containers/operator:v0.2.0 ImageID:quay.io/confidential-containers/operator@sha256:c965b55253a9abe4c2f7596c42467fa59f2cc741bfafeed1d25629ed6f8df12d ContainerID:containerd://186424b47c7b9022ecfe8996dc73cd9101f7539259cd65914279c84c9a02a288 Started:0xc001a40cf0}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:42:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:32.583129  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:42:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:32.583197  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:42:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:32.583847  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:42:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:32.584020  199956 kubelet.go:1503] "syncPod exit" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d isTerminal=false
Jan 20 12:42:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:32.584075  199956 pod_workers.go:988] "Processing pod event done" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d updateType=0
Jan 20 12:42:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:32.595691  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" volumeName="kube-api-access-4pnfq" volumeSpecName="kube-api-access-4pnfq"
Jan 20 12:42:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:32.613786  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-4pnfq\" (UniqueName: \"kubernetes.io/projected/d2688d45-2487-46e7-aecb-e3479626909d-kube-api-access-4pnfq\") pod \"cc-operator-controller-manager-79797456f6-m6znr\" (UID: \"d2688d45-2487-46e7-aecb-e3479626909d\") Volume is already mounted to pod, but remount was requested." pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:42:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:32.613828  199956 projected.go:183] Setting up volume kube-api-access-4pnfq for pod d2688d45-2487-46e7-aecb-e3479626909d at /var/lib/kubelet/pods/d2688d45-2487-46e7-aecb-e3479626909d/volumes/kubernetes.io~projected/kube-api-access-4pnfq
Jan 20 12:42:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:32.613924  199956 atomic_writer.go:161] pod confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr volume kube-api-access-4pnfq: no update required for target directory /var/lib/kubelet/pods/d2688d45-2487-46e7-aecb-e3479626909d/volumes/kubernetes.io~projected/kube-api-access-4pnfq
Jan 20 12:42:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:32.613939  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-4pnfq\" (UniqueName: \"kubernetes.io/projected/d2688d45-2487-46e7-aecb-e3479626909d-kube-api-access-4pnfq\") pod \"cc-operator-controller-manager-79797456f6-m6znr\" (UID: \"d2688d45-2487-46e7-aecb-e3479626909d\") " pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:42:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:32.688970  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:42:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:32.689036  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:32.690144  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:32 GMT]] 0xc0020fdb80 2 [] true false map[] 0xc001716200 <nil>}
Jan 20 12:42:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:32.690255  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:42:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:32.695376  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:42:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:32.695433  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:32.696493  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:32 GMT]] 0xc0020fdbc0 2 [] true false map[] 0xc0016ab400 <nil>}
Jan 20 12:42:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:32.696600  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:42:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:32.709840  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:42:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:32.709902  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:32.709913  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:42:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:32.709969  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:32.710950  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:32 GMT]] 0xc0000a59c0 2 [] true false map[] 0xc001a4b100 <nil>}
Jan 20 12:42:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:32.711006  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:32 GMT]] 0xc00129c040 2 [] true false map[] 0xc0016ab600 <nil>}
Jan 20 12:42:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:32.711049  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:42:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:32.711103  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:42:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:32.813577  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:42:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:33.483948  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:33.484016  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:33.491917  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[35a7ac25-fdda-4222-aeb4-b2454c03c827] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:33 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e95c0 2 [] false false map[] 0xc001716400 0xc0011deb00}
Jan 20 12:42:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:33.491948  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:33.581759  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:42:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:33.587359  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:42:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:33.587371  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:42:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:33.588077  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:42:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:33.588507  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:42:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:33.588551  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:42:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:33.588558  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:42:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:33.588567  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:42:34 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:34.483973  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:34 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:34.484045  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:34 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:34.492229  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[55af9312-6635-43e2-bb8a-2e2088709e63] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:34 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ffbfe0 2 [] false false map[] 0xc001a4b400 0xc0013be840}
Jan 20 12:42:34 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:34.492257  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:35.483787  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:35.483854  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:35.491688  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[0187df29-e078-4196-9517-58db0ca8f205] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:35 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e9bc0 2 [] false false map[] 0xc001716700 0xc001488bb0}
Jan 20 12:42:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:35.491719  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:35.581230  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:42:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:35.587247  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:42:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:35.587259  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:42:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:35.587266  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:42:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:35.587944  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:42:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:35.588022  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:42:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:35.588028  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:42:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:35.588036  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:42:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:36.164802  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:42:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:36.164874  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:36.172018  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:36 GMT] X-Content-Type-Options:[nosniff]] 0xc000cb3b80 2 [] false false map[] 0xc001f37a00 0xc001ae9e40}
Jan 20 12:42:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:36.172066  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:42:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:36.271916  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:42:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:36.271979  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:36.273197  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:36 GMT]] 0xc000cb3cc0 29 [] true false map[] 0xc001f37d00 <nil>}
Jan 20 12:42:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:36.273302  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:42:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:36.484000  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:36.484077  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:36.496837  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[513f3282-55f2-4554-9bfe-166350858bc0] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:36 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001820080 2 [] false false map[] 0xc001f37f00 0xc001b4a000}
Jan 20 12:42:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:36.496974  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:36.895571  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:42:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:36.895638  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:36.903045  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:36 GMT] X-Content-Type-Options:[nosniff]] 0xc0008e21e0 2 [] false false map[] 0xc001717400 0xc001c4c840}
Jan 20 12:42:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:36.903106  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:42:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:37.483948  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:37.484016  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:37.492624  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[5cc37075-c6fc-443c-9483-fe8b365a6aed] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:37 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000706380 2 [] false false map[] 0xc001a4a400 0xc001b4a210}
Jan 20 12:42:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:37.492653  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:37.581984  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:42:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:37.587398  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:42:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:37.587410  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:42:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:37.587912  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:42:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:37.588251  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:42:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:37.588298  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:42:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:37.588305  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:42:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:37.588313  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:42:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:37.815479  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:42:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:38.483182  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:38.483240  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:38.495758  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[bc529218-9fef-48fd-a3d3-4d635bba2f89] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:38 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008e2f00 2 [] false false map[] 0xc000fcc300 0xc00112f1e0}
Jan 20 12:42:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:38.495895  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:38.879230  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/healthz" timeout="1s"
Jan 20 12:42:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:38.879297  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:42:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:38.879302  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:38.879356  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:38.880512  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:38 GMT] X-Content-Type-Options:[nosniff]] 0xc00123e0c0 2 [] true false map[] 0xc001a4a800 <nil>}
Jan 20 12:42:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:38.880528  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:38 GMT] X-Content-Type-Options:[nosniff]] 0xc00129da40 2 [] true false map[] 0xc000cf7000 <nil>}
Jan 20 12:42:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:38.880631  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:42:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:38.880657  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:42:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:39.483397  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:39.483466  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:39.491806  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[54b7ff49-e383-47e5-83ec-8c95832264fe] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:39 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0009782a0 2 [] false false map[] 0xc001a4aa00 0xc000ba9080}
Jan 20 12:42:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:39.491840  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:39.581714  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:42:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:39.587389  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:42:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:39.587402  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:42:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:39.587408  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:42:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:39.587912  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:42:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:39.587957  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:42:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:39.587964  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:42:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:39.587973  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:42:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:40.015861  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:42:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:40.023492  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:42:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:40.033395  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64393540Ki" capacity="65061836Ki" time="2023-01-20 12:42:40.033338862 -0500 EST m=+672.589113688"
Jan 20 12:42:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:40.033408  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902011700Ki" capacity="981310056Ki" time="2023-01-20 12:42:40.018120833 -0500 EST m=+672.573895726"
Jan 20 12:42:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:40.033415  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:42:40.018120833 -0500 EST m=+672.573895726"
Jan 20 12:42:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:40.033420  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902011700Ki" capacity="981310056Ki" time="2023-01-20 12:42:31.261881697 -0500 EST"
Jan 20 12:42:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:40.033426  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:42:31.261881697 -0500 EST"
Jan 20 12:42:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:40.033431  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510775" capacity="511757" time="2023-01-20 12:42:40.033077003 -0500 EST m=+672.588851828"
Jan 20 12:42:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:40.033437  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59484040Ki" capacity="65586124Ki" time="2023-01-20 12:42:40.018120833 -0500 EST m=+672.573895726"
Jan 20 12:42:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:40.033464  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:42:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:40.483941  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:40.484014  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:40.496526  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[7b4038db-fdd1-401c-95da-daff5bc1d7cc] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:40 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0020fc020 2 [] false false map[] 0xc001847400 0xc0012d1810}
Jan 20 12:42:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:40.496662  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:41.166139  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:42:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:41.166208  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:41.179965  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[8baba743-6256-4234-80fc-997315541bbd] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:41 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0000a5e60 2 [] false false map[] 0xc001847600 0xc00102be40}
Jan 20 12:42:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:41.180106  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:41.483511  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:41.483578  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:41.491639  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[8da2e492-c2ff-43a5-a9ae-d71c53c1be72] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:41 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ffa500 2 [] false false map[] 0xc001a4b000 0xc00105e210}
Jan 20 12:42:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:41.491669  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:41.581972  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:42:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:41.587425  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:42:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:41.587437  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:42:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:41.587897  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:42:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:41.588263  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:42:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:41.588309  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:42:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:41.588315  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:42:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:41.588324  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:42:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:41.780071  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:42:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:41.780079  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:42:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:41.780272  199956 interface.go:209] Interface eno2 is up
Jan 20 12:42:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:41.780299  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:42:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:41.780306  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:42:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:41.780311  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:42:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:41.780315  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:42:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:41.780319  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:42:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:41.780606  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:42:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:41.780615  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:42:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:41.780619  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:42:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:41.780624  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:42:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:42.483755  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:42.483824  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:42.496210  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[14231d86-00b1-431f-97f8-f204fef6a965] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:42 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ce9f20 2 [] false false map[] 0xc001f37600 0xc00192d130}
Jan 20 12:42:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:42.496354  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:42.689416  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:42:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:42.689479  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:42.690495  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:42 GMT]] 0xc00123e600 2 [] true false map[] 0xc00105a800 <nil>}
Jan 20 12:42:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:42.690605  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:42:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:42.695719  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:42:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:42.695732  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:42.696117  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:42 GMT]] 0xc000ce9f80 2 [] true false map[] 0xc0002c3800 <nil>}
Jan 20 12:42:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:42.696135  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:42:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:42.710425  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:42:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:42.710438  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:42.710454  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:42:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:42.710466  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:42.710707  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:42 GMT]] 0xc00123e640 2 [] true false map[] 0xc001774100 <nil>}
Jan 20 12:42:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:42.710728  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:42:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:42.710730  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:42 GMT]] 0xc000ffb500 2 [] true false map[] 0xc0014ca000 <nil>}
Jan 20 12:42:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:42.710748  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:42:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:42.817349  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:42:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:43.399249  199956 handler.go:293] error while reading "/proc/199956/fd/34" link: readlink /proc/199956/fd/34: no such file or directory
Jan 20 12:42:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:43.483638  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:43.483701  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:43.491420  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[4c6b2b92-de4a-4ced-8972-9d045677b3b0] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:43 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123e8e0 2 [] false false map[] 0xc00105aa00 0xc001a65550}
Jan 20 12:42:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:43.491451  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:43.582043  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:42:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:43.588195  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:42:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:43.588207  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:42:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:43.588928  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:42:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:43.589413  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:42:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:43.589487  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:42:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:43.589493  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:42:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:43.589516  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:42:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:44.483750  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:44.483825  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:44.491322  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[fe60823c-ec3c-4505-a7cc-3dc3241bf441] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:44 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123eac0 2 [] false false map[] 0xc001774600 0xc002144fd0}
Jan 20 12:42:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:44.491354  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:45.483952  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:45.484024  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:45.492815  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[e6ec6f42-8f5a-42a9-a98b-67d0613b4542] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:45 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000320d00 2 [] false false map[] 0xc001f36900 0xc0015f2160}
Jan 20 12:42:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:45.492845  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:45.582002  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[kube-system/kube-controller-manager-zcy-z390-aorus-master]
Jan 20 12:42:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:45.582094  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:42:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:45.582241  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce updateType=0
Jan 20 12:42:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:45.582324  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce
Jan 20 12:42:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:45.582358  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master"
Jan 20 12:42:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:45.582448  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" oldPhase=Running phase=Running
Jan 20 12:42:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:45.582734  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:28 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:37 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:37 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:28 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.239.159.53 PodIPs:[{IP:10.239.159.53}] StartTime:2023-01-20 12:31:28 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:kube-controller-manager State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:22 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:6 Image:k8s.gcr.io/kube-controller-manager:v1.24.0 ImageID:k8s.gcr.io/kube-controller-manager@sha256:df044a154e79a18f749d3cd9d958c3edde2b6a00c815176472002b7bbf956637 ContainerID:containerd://f8443d2868215ec42d3fa335663976e33df166c5e98369aa3f9d7ddb9235bead Started:0xc0016599be}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:42:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:45.583091  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master"
Jan 20 12:42:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:45.583166  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master"
Jan 20 12:42:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:45.584091  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/kube-controller-manager-zcy-z390-aorus-master"
Jan 20 12:42:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:45.584258  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce isTerminal=false
Jan 20 12:42:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:45.584315  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce updateType=0
Jan 20 12:42:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:45.587469  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:42:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:45.587481  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:42:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:45.587487  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:42:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:45.588129  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:42:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:45.588216  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:42:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:45.588223  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:42:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:45.588250  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:42:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:45.592595  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="ca-certs" volumeSpecName="ca-certs"
Jan 20 12:42:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:45.592724  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Jan 20 12:42:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:45.592803  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="etc-pki" volumeSpecName="etc-pki"
Jan 20 12:42:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:45.592873  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="flexvolume-dir" volumeSpecName="flexvolume-dir"
Jan 20 12:42:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:45.592943  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Jan 20 12:42:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:45.593007  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Jan 20 12:42:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:45.593078  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Jan 20 12:42:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:45.593147  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.164663  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.164759  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.171980  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:46 GMT] X-Content-Type-Options:[nosniff]] 0xc0012e9680 2 [] false false map[] 0xc00105a900 0xc0015f22c0}
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.172025  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.271828  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.271888  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.273123  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:46 GMT]] 0xc0012e9700 29 [] true false map[] 0xc001f36d00 <nil>}
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.273226  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.483908  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.483975  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.491355  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[1ce8621d-a7a4-47a2-b5cb-18844a96ad33] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:46 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000321640 2 [] false false map[] 0xc001716100 0xc0007ba370}
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.491382  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.581476  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[kube-system/coredns-6d4b75cb6d-zdl2m]
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.581552  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e updateType=0
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.581605  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.581635  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.581717  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/coredns-6d4b75cb6d-zdl2m" oldPhase=Running phase=Running
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.581991  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/coredns-6d4b75cb6d-zdl2m" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:44 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:44 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.3 PodIPs:[{IP:10.244.0.3}] StartTime:2023-01-20 12:31:42 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:coredns State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:44 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:k8s.gcr.io/coredns/coredns:v1.8.6 ImageID:k8s.gcr.io/coredns/coredns@sha256:5b6ec0d6de9baaf3e92d0f66cd96a25b9edbce8716f5f15dcd1a616b3abd590e ContainerID:containerd://e489181a7f95431b6d92f037dbe3f788b46a5e024df7aaf29e0115dab1168ab1 Started:0xc00174705e}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.582310  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.582377  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.582859  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.583031  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e isTerminal=false
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.583084  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e updateType=0
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.601931  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/coredns-6d4b75cb6d-zdl2m" volumeName="config-volume" volumeSpecName="config-volume"
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.602087  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/coredns-6d4b75cb6d-zdl2m" volumeName="kube-api-access-tqzsm" volumeSpecName="kube-api-access-tqzsm"
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.610095  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/cf25b9ea-6823-4eff-b30d-36a09f9d897e-config-volume\") pod \"coredns-6d4b75cb6d-zdl2m\" (UID: \"cf25b9ea-6823-4eff-b30d-36a09f9d897e\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.610207  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-tqzsm\" (UniqueName: \"kubernetes.io/projected/cf25b9ea-6823-4eff-b30d-36a09f9d897e-kube-api-access-tqzsm\") pod \"coredns-6d4b75cb6d-zdl2m\" (UID: \"cf25b9ea-6823-4eff-b30d-36a09f9d897e\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.610355  199956 projected.go:183] Setting up volume kube-api-access-tqzsm for pod cf25b9ea-6823-4eff-b30d-36a09f9d897e at /var/lib/kubelet/pods/cf25b9ea-6823-4eff-b30d-36a09f9d897e/volumes/kubernetes.io~projected/kube-api-access-tqzsm
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.610404  199956 configmap.go:181] Setting up volume config-volume for pod cf25b9ea-6823-4eff-b30d-36a09f9d897e at /var/lib/kubelet/pods/cf25b9ea-6823-4eff-b30d-36a09f9d897e/volumes/kubernetes.io~configmap/config-volume
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.610487  199956 configmap.go:205] Received configMap kube-system/coredns containing (1) pieces of data, 339 total bytes
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.610561  199956 quota_linux.go:271] SupportsQuotas called, but quotas disabled
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.610765  199956 atomic_writer.go:161] pod kube-system/coredns-6d4b75cb6d-zdl2m volume kube-api-access-tqzsm: no update required for target directory /var/lib/kubelet/pods/cf25b9ea-6823-4eff-b30d-36a09f9d897e/volumes/kubernetes.io~projected/kube-api-access-tqzsm
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.610788  199956 atomic_writer.go:161] pod kube-system/coredns-6d4b75cb6d-zdl2m volume config-volume: no update required for target directory /var/lib/kubelet/pods/cf25b9ea-6823-4eff-b30d-36a09f9d897e/volumes/kubernetes.io~configmap/config-volume
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.610831  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-tqzsm\" (UniqueName: \"kubernetes.io/projected/cf25b9ea-6823-4eff-b30d-36a09f9d897e-kube-api-access-tqzsm\") pod \"coredns-6d4b75cb6d-zdl2m\" (UID: \"cf25b9ea-6823-4eff-b30d-36a09f9d897e\") " pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.610868  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/cf25b9ea-6823-4eff-b30d-36a09f9d897e-config-volume\") pod \"coredns-6d4b75cb6d-zdl2m\" (UID: \"cf25b9ea-6823-4eff-b30d-36a09f9d897e\") " pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.895625  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.895691  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.902942  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:46 GMT] X-Content-Type-Options:[nosniff]] 0xc0008dc880 2 [] false false map[] 0xc001716700 0xc001018160}
Jan 20 12:42:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:46.902971  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.483553  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.483625  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.491777  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[5ac090e5-2b92-4341-bfaf-790bbf4c1551] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:47 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e9e00 2 [] false false map[] 0xc001f36f00 0xc001187c30}
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.491807  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.526210  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/etcd.yaml"
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.528325  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver.yaml"
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.531280  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.532357  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-scheduler.yaml"
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.532648  199956 config.go:279] "Setting pods for source" source="file"
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.581416  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[confidential-containers-system/cc-operator-pre-install-daemon-qjplj]
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.581463  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.581554  199956 pod_workers.go:888] "Processing pod event" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" podUID=b0713fbc-efc5-4044-9d08-2326a0752f87 updateType=0
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.581600  199956 kubelet.go:1501] "syncPod enter" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" podUID=b0713fbc-efc5-4044-9d08-2326a0752f87
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.581618  199956 kubelet_pods.go:1435] "Generating pod status" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj"
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.581663  199956 kubelet_pods.go:1447] "Got phase for pod" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" oldPhase=Running phase=Running
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.582031  199956 status_manager.go:535] "Ignoring same status for pod" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:01 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:05 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:05 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:01 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.5 PodIPs:[{IP:10.244.0.5}] StartTime:2023-01-20 12:32:01 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:cc-runtime-pre-install-pod State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:32:03 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:quay.io/confidential-containers/container-engine-for-cc-payload:1034f9fcf947b22eea080a6f77d8e164e2369849 ImageID:quay.io/confidential-containers/container-engine-for-cc-payload@sha256:f86f078b3a47026a066e65c7d836d9b9a43bf177555c276624d90f42e50279a1 ContainerID:containerd://e0c5b991f81d5128566686552e45a9bbc8c584e4f6c94909edb3a42720dacc90 Started:0xc001115f2a}] QOSClass:BestEffort EphemeralContainerStatuses:[]}
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.582230  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj"
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.582274  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj"
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.582470  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj"
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.582562  199956 kubelet.go:1503] "syncPod exit" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" podUID=b0713fbc-efc5-4044-9d08-2326a0752f87 isTerminal=false
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.582594  199956 pod_workers.go:988] "Processing pod event done" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" podUID=b0713fbc-efc5-4044-9d08-2326a0752f87 updateType=0
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.585308  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.585338  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.586639  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.587970  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.588118  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.588142  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.588169  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.610416  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" volumeName="confidential-containers-artifacts" volumeSpecName="confidential-containers-artifacts"
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.610521  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" volumeName="etc-systemd-system" volumeSpecName="etc-systemd-system"
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.610595  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" volumeName="dbus" volumeSpecName="dbus"
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.610660  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" volumeName="systemd" volumeSpecName="systemd"
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.610786  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" volumeName="kube-api-access-gcgm6" volumeSpecName="kube-api-access-gcgm6"
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.618679  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-gcgm6\" (UniqueName: \"kubernetes.io/projected/b0713fbc-efc5-4044-9d08-2326a0752f87-kube-api-access-gcgm6\") pod \"cc-operator-pre-install-daemon-qjplj\" (UID: \"b0713fbc-efc5-4044-9d08-2326a0752f87\") Volume is already mounted to pod, but remount was requested." pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj"
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.618872  199956 projected.go:183] Setting up volume kube-api-access-gcgm6 for pod b0713fbc-efc5-4044-9d08-2326a0752f87 at /var/lib/kubelet/pods/b0713fbc-efc5-4044-9d08-2326a0752f87/volumes/kubernetes.io~projected/kube-api-access-gcgm6
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.619277  199956 atomic_writer.go:161] pod confidential-containers-system/cc-operator-pre-install-daemon-qjplj volume kube-api-access-gcgm6: no update required for target directory /var/lib/kubelet/pods/b0713fbc-efc5-4044-9d08-2326a0752f87/volumes/kubernetes.io~projected/kube-api-access-gcgm6
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.619356  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-gcgm6\" (UniqueName: \"kubernetes.io/projected/b0713fbc-efc5-4044-9d08-2326a0752f87-kube-api-access-gcgm6\") pod \"cc-operator-pre-install-daemon-qjplj\" (UID: \"b0713fbc-efc5-4044-9d08-2326a0752f87\") " pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj"
Jan 20 12:42:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:47.819265  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:42:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:48.483991  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:48.484067  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:48.493548  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[a0dd3724-b394-4f28-8880-bc0fb74a930f] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:48 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00129cea0 2 [] false false map[] 0xc000fcc100 0xc001558000}
Jan 20 12:42:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:48.493581  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:48.879016  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:42:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:48.879088  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:48.880301  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:48 GMT] X-Content-Type-Options:[nosniff]] 0xc00129cf40 2 [] true false map[] 0xc00105b500 <nil>}
Jan 20 12:42:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:48.880415  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:42:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:49.484053  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:49.484120  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:49.492836  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[19f82cae-4fe3-41ef-b93f-8f6ab62520b8] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:49 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0009a02c0 2 [] false false map[] 0xc00105b700 0xc0017cf600}
Jan 20 12:42:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:49.492881  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:49.581429  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:42:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:49.587318  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:42:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:49.587331  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:42:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:49.587780  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:42:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:49.588134  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:42:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:49.588179  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:42:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:49.588186  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:42:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:49.588194  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:42:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:50.034271  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:42:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:50.040097  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:42:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:50.051316  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59484340Ki" capacity="65586124Ki" time="2023-01-20 12:42:50.036435891 -0500 EST m=+682.592210784"
Jan 20 12:42:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:50.051330  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64393324Ki" capacity="65061836Ki" time="2023-01-20 12:42:50.051249875 -0500 EST m=+682.607024701"
Jan 20 12:42:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:50.051338  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902011648Ki" capacity="981310056Ki" time="2023-01-20 12:42:50.036435891 -0500 EST m=+682.592210784"
Jan 20 12:42:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:50.051345  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:42:50.036435891 -0500 EST m=+682.592210784"
Jan 20 12:42:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:50.051351  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902011648Ki" capacity="981310056Ki" time="2023-01-20 12:42:41.261881763 -0500 EST"
Jan 20 12:42:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:50.051358  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:42:41.261881763 -0500 EST"
Jan 20 12:42:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:50.051364  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510776" capacity="511757" time="2023-01-20 12:42:50.050961547 -0500 EST m=+682.606736375"
Jan 20 12:42:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:50.051392  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:42:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:50.483392  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:50.483465  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:50.491627  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[01905b7e-020b-46bb-91e1-80e2572ab711] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:50 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0020fc380 2 [] false false map[] 0xc000fcc100 0xc0006a2790}
Jan 20 12:42:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:50.491666  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:50.581191  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[confidential-containers-system/cc-operator-daemon-install-t6mp7]
Jan 20 12:42:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:50.581278  199956 pod_workers.go:888] "Processing pod event" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" podUID=7af065b7-9095-4d91-9b9e-2644e7b1f4bf updateType=0
Jan 20 12:42:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:50.581342  199956 kubelet.go:1501] "syncPod enter" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" podUID=7af065b7-9095-4d91-9b9e-2644e7b1f4bf
Jan 20 12:42:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:50.581381  199956 kubelet_pods.go:1435] "Generating pod status" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7"
Jan 20 12:42:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:50.581468  199956 kubelet_pods.go:1447] "Got phase for pod" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" oldPhase=Running phase=Running
Jan 20 12:42:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:50.581760  199956 status_manager.go:535] "Ignoring same status for pod" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:04 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:20 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:20 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:04 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.6 PodIPs:[{IP:10.244.0.6}] StartTime:2023-01-20 12:32:04 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:cc-runtime-install-pod State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:32:20 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:quay.io/confidential-containers/runtime-payload-ci:kata-containers-amd64 ImageID:quay.io/confidential-containers/runtime-payload-ci@sha256:4736ba274765c889404fb98f01de0a997e68d2d7e5acca2440488f0e1337032b ContainerID:containerd://e10efaa459a32161059fda75d4bcaf3bbdb896781f772b508e43fe00bc7c9003 Started:0xc00108e6de}] QOSClass:BestEffort EphemeralContainerStatuses:[]}
Jan 20 12:42:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:50.582096  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7"
Jan 20 12:42:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:50.582165  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7"
Jan 20 12:42:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:50.582593  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="confidential-containers-system/cc-operator-daemon-install-t6mp7"
Jan 20 12:42:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:50.582795  199956 kubelet.go:1503] "syncPod exit" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" podUID=7af065b7-9095-4d91-9b9e-2644e7b1f4bf isTerminal=false
Jan 20 12:42:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:50.582855  199956 pod_workers.go:988] "Processing pod event done" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" podUID=7af065b7-9095-4d91-9b9e-2644e7b1f4bf updateType=0
Jan 20 12:42:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:50.632711  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" volumeName="containerd-conf" volumeSpecName="containerd-conf"
Jan 20 12:42:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:50.632818  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" volumeName="kata-artifacts" volumeSpecName="kata-artifacts"
Jan 20 12:42:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:50.632889  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" volumeName="dbus" volumeSpecName="dbus"
Jan 20 12:42:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:50.632955  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" volumeName="systemd" volumeSpecName="systemd"
Jan 20 12:42:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:50.633023  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" volumeName="local-bin" volumeSpecName="local-bin"
Jan 20 12:42:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:50.633143  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" volumeName="kube-api-access-x6vjr" volumeSpecName="kube-api-access-x6vjr"
Jan 20 12:42:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:50.642905  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-x6vjr\" (UniqueName: \"kubernetes.io/projected/7af065b7-9095-4d91-9b9e-2644e7b1f4bf-kube-api-access-x6vjr\") pod \"cc-operator-daemon-install-t6mp7\" (UID: \"7af065b7-9095-4d91-9b9e-2644e7b1f4bf\") Volume is already mounted to pod, but remount was requested." pod="confidential-containers-system/cc-operator-daemon-install-t6mp7"
Jan 20 12:42:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:50.643090  199956 projected.go:183] Setting up volume kube-api-access-x6vjr for pod 7af065b7-9095-4d91-9b9e-2644e7b1f4bf at /var/lib/kubelet/pods/7af065b7-9095-4d91-9b9e-2644e7b1f4bf/volumes/kubernetes.io~projected/kube-api-access-x6vjr
Jan 20 12:42:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:50.643514  199956 atomic_writer.go:161] pod confidential-containers-system/cc-operator-daemon-install-t6mp7 volume kube-api-access-x6vjr: no update required for target directory /var/lib/kubelet/pods/7af065b7-9095-4d91-9b9e-2644e7b1f4bf/volumes/kubernetes.io~projected/kube-api-access-x6vjr
Jan 20 12:42:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:50.643589  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-x6vjr\" (UniqueName: \"kubernetes.io/projected/7af065b7-9095-4d91-9b9e-2644e7b1f4bf-kube-api-access-x6vjr\") pod \"cc-operator-daemon-install-t6mp7\" (UID: \"7af065b7-9095-4d91-9b9e-2644e7b1f4bf\") " pod="confidential-containers-system/cc-operator-daemon-install-t6mp7"
Jan 20 12:42:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:51.166395  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:42:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:51.166461  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:51.174638  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[8560ed95-a216-48c0-a970-0ea7e151e415] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:51 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001821180 2 [] false false map[] 0xc001716100 0xc000affef0}
Jan 20 12:42:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:51.174666  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:51.483366  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:51.483432  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:51.491559  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[b94016e3-9677-4141-bab6-fce19feb96d9] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:51 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001821420 2 [] false false map[] 0xc0000dda00 0xc000fd80b0}
Jan 20 12:42:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:51.491589  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:51.582200  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:42:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:51.588282  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:42:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:51.588294  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:42:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:51.588718  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:42:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:51.589090  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:42:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:51.589137  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:42:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:51.589144  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:42:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:51.589153  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:42:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:51.864660  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:42:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:51.864670  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:42:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:51.864871  199956 interface.go:209] Interface eno2 is up
Jan 20 12:42:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:51.864909  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:42:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:51.864920  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:42:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:51.864926  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:42:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:51.864931  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:42:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:51.864936  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:42:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:51.865244  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:42:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:51.865257  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:42:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:51.865263  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:42:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:51.865269  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:42:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:52.483730  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:52.483799  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:52.491602  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[c4b9465d-690c-4e61-9fc5-2a18289d9776] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:52 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123e060 2 [] false false map[] 0xc0000dde00 0xc000b1edc0}
Jan 20 12:42:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:52.491638  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:52.690001  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:42:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:52.690064  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:52.691187  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:52 GMT]] 0xc000706680 2 [] true false map[] 0xc000fcde00 <nil>}
Jan 20 12:42:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:52.691300  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:42:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:52.695406  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:42:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:52.695469  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:52.696487  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:52 GMT]] 0xc000706700 2 [] true false map[] 0xc001f36000 <nil>}
Jan 20 12:42:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:52.696592  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:42:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:52.709830  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:42:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:52.709895  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:52.709905  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:42:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:52.709964  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:52.710973  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:52 GMT]] 0xc000706740 2 [] true false map[] 0xc001774400 <nil>}
Jan 20 12:42:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:52.710998  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:52 GMT]] 0xc001703c20 2 [] true false map[] 0xc001f36a00 <nil>}
Jan 20 12:42:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:52.711078  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:42:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:52.711103  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:42:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:52.820751  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:42:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:53.483409  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:53.483484  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:53.491782  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[fe8a59c5-1f89-4881-9daf-36bb1255b3e0] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:53 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0007069e0 2 [] false false map[] 0xc001774600 0xc0011c7a20}
Jan 20 12:42:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:53.491811  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:53.581993  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:42:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:53.587459  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:42:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:53.587471  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:42:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:53.587478  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:42:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:53.588043  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:42:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:53.588122  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:42:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:53.588129  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:42:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:53.588138  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:42:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:54.484195  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:54.484263  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:54.492711  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[ffcf0516-886f-4de1-aa6b-11493c7d93eb] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:54 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000320940 2 [] false false map[] 0xc001f36f00 0xc0016286e0}
Jan 20 12:42:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:54.492741  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:55.483895  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:55.483964  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:55.491771  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[747dda66-6d0d-48b4-8c75-32afa732754f] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:55 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123f480 2 [] false false map[] 0xc001775000 0xc001a08580}
Jan 20 12:42:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:55.491801  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:55.581479  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:42:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:55.588803  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:42:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:55.588866  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:42:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:55.590869  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:42:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:55.592863  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:42:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:55.593081  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:42:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:55.593112  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:42:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:55.593155  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:42:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:56.165096  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:42:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:56.165170  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:56.172993  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:56 GMT] X-Content-Type-Options:[nosniff]] 0xc0008e2be0 2 [] false false map[] 0xc001a4a300 0xc001e71600}
Jan 20 12:42:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:56.173019  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:42:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:56.271903  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:42:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:56.271962  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:56.273200  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:56 GMT]] 0xc000320c60 29 [] true false map[] 0xc001775500 <nil>}
Jan 20 12:42:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:56.273305  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:42:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:56.484153  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:56.484211  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:56.492747  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[26d0d5b6-1667-47ff-979e-cf705960e13e] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:56 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008e2c80 2 [] false false map[] 0xc001f37200 0xc001648790}
Jan 20 12:42:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:56.492772  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:56.686808  199956 handler.go:293] error while reading "/proc/199956/fd/34" link: readlink /proc/199956/fd/34: no such file or directory
Jan 20 12:42:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:56.895824  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:42:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:56.895892  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:56.903117  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:56 GMT] X-Content-Type-Options:[nosniff]] 0xc0008e2e00 2 [] false false map[] 0xc001f37500 0xc001fae580}
Jan 20 12:42:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:56.903182  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:42:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:57.483345  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:57.483412  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:57.491831  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[a0933fa9-c951-4acd-ae40-0614763a4634] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:57 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008e3640 2 [] false false map[] 0xc001a4a600 0xc002083130}
Jan 20 12:42:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:57.491860  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:57.581978  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:42:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:57.587468  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:42:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:57.587480  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:42:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:57.587488  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:42:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:57.587969  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:42:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:57.588015  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:42:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:57.588021  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:42:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:57.588029  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:42:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:57.822339  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:42:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:58.483430  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:58.483498  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:58.491510  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[35f13e55-dfc7-4716-8fe2-d1244465dd42] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:58 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ffa6a0 2 [] false false map[] 0xc001846200 0xc001c4fe40}
Jan 20 12:42:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:58.491540  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:58.879520  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:42:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:58.879579  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/healthz" timeout="1s"
Jan 20 12:42:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:58.879593  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:58.879646  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:58.880850  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:58 GMT] X-Content-Type-Options:[nosniff]] 0xc000320f20 2 [] true false map[] 0xc001846500 <nil>}
Jan 20 12:42:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:58.880927  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:58 GMT] X-Content-Type-Options:[nosniff]] 0xc001702060 2 [] true false map[] 0xc0014ca100 <nil>}
Jan 20 12:42:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:58.880969  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:42:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:58.881033  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:42:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:59.483437  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:42:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:59.483503  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:42:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:59.491802  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[75828a1d-be07-44ba-9e7f-295dc1e8ee57] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:42:59 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ffada0 2 [] false false map[] 0xc0014ca300 0xc000b1e0b0}
Jan 20 12:42:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:59.491831  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:42:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:59.582043  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:42:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:59.587476  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:42:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:59.587488  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:42:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:59.588106  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:42:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:59.588531  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:42:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:59.588578  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:42:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:59.588585  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:42:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:42:59.588594  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:43:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:00.052303  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:43:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:00.056931  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:43:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:00.069654  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902011608Ki" capacity="981310056Ki" time="2023-01-20 12:42:51.264109472 -0500 EST"
Jan 20 12:43:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:00.069680  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:42:51.264109472 -0500 EST"
Jan 20 12:43:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:00.069687  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510776" capacity="511757" time="2023-01-20 12:43:00.069365349 -0500 EST m=+692.625140171"
Jan 20 12:43:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:00.069706  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59478120Ki" capacity="65586124Ki" time="2023-01-20 12:43:00.052894314 -0500 EST m=+692.608669140"
Jan 20 12:43:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:00.069712  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64393200Ki" capacity="65061836Ki" time="2023-01-20 12:43:00.069607733 -0500 EST m=+692.625382555"
Jan 20 12:43:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:00.069718  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902011608Ki" capacity="981310056Ki" time="2023-01-20 12:43:00.052894314 -0500 EST m=+692.608669140"
Jan 20 12:43:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:00.069722  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:43:00.052894314 -0500 EST m=+692.608669140"
Jan 20 12:43:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:00.069769  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:43:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:00.484157  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:00.484188  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:00.490510  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[d9291e2d-116e-414d-9e4e-04ea6e1f8262] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:00 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0018206c0 2 [] false false map[] 0xc0014cab00 0xc001209d90}
Jan 20 12:43:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:00.490582  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:01.166790  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:43:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:01.166829  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:01.172142  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[8abe2d5c-11ed-4707-9a42-50f666492001] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:01 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0020fc4a0 2 [] false false map[] 0xc0014cae00 0xc0013e0000}
Jan 20 12:43:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:01.172204  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:01.483981  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:01.484046  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:01.491582  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[eaf988cc-ccbb-498d-b58e-f9a747829aea] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:01 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001820dc0 2 [] false false map[] 0xc001717e00 0xc00169b1e0}
Jan 20 12:43:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:01.491612  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:01.581988  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:43:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:01.587429  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:43:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:01.587442  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:43:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:01.587448  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:43:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:01.588165  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:43:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:01.588244  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:43:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:01.588250  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:43:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:01.588258  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:43:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:02.018209  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:43:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:02.018217  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:43:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:02.018370  199956 interface.go:209] Interface eno2 is up
Jan 20 12:43:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:02.018397  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:43:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:02.018404  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:43:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:02.018408  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:43:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:02.018412  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:43:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:02.018415  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:43:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:02.018804  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:43:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:02.018828  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:43:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:02.018846  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:43:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:02.018849  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:43:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:02.483417  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:02.483481  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:02.491677  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[dad96943-c1af-4507-9fec-e99eb3b2cbcd] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:02 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008dc5a0 2 [] false false map[] 0xc000fccc00 0xc001a3f760}
Jan 20 12:43:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:02.491708  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:02.689538  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:43:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:02.689612  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:02.690737  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:02 GMT]] 0xc0008dc7c0 2 [] true false map[] 0xc0016aba00 <nil>}
Jan 20 12:43:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:02.690852  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:43:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:02.696063  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:43:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:02.696129  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:02.697190  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:02 GMT]] 0xc0008dc800 2 [] true false map[] 0xc0016abc00 <nil>}
Jan 20 12:43:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:02.697289  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:43:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:02.710520  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:43:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:02.710585  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:02.710590  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:43:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:02.710650  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:02.711636  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:02 GMT]] 0xc00129dd20 2 [] true false map[] 0xc0014cb100 <nil>}
Jan 20 12:43:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:02.711647  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:02 GMT]] 0xc0008dc860 2 [] true false map[] 0xc0016abe00 <nil>}
Jan 20 12:43:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:02.711742  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:43:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:02.711762  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:43:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:02.823356  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:43:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:03.483661  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:03.483737  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:03.491970  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[56c8f02a-376f-4225-aeff-6acc8584e964] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:03 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00129cd20 2 [] false false map[] 0xc001846200 0xc000f8cb00}
Jan 20 12:43:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:03.492001  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:03.581554  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:43:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:03.587387  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:43:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:03.587400  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:43:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:03.587828  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:43:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:03.588199  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:43:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:03.588248  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:43:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:03.588256  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:43:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:03.588264  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:43:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:04.483824  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:04.483897  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:04.491705  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[9e01c112-fe39-4c6a-9365-ca86494ee48c] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:04 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008ddcc0 2 [] false false map[] 0xc001846700 0xc000fd91e0}
Jan 20 12:43:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:04.491737  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:04.581945  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[kube-system/etcd-zcy-z390-aorus-master]
Jan 20 12:43:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:04.581982  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d updateType=0
Jan 20 12:43:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:04.582004  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d
Jan 20 12:43:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:04.582017  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/etcd-zcy-z390-aorus-master"
Jan 20 12:43:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:04.582055  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/etcd-zcy-z390-aorus-master" oldPhase=Running phase=Running
Jan 20 12:43:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:04.582172  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/etcd-zcy-z390-aorus-master" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:28 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:37 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:37 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:28 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.239.159.53 PodIPs:[{IP:10.239.159.53}] StartTime:2023-01-20 12:31:28 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:etcd State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:22 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:6 Image:k8s.gcr.io/etcd:3.5.3-0 ImageID:k8s.gcr.io/etcd@sha256:13f53ed1d91e2e11aac476ee9a0269fdda6cc4874eba903efd40daf50c55eee5 ContainerID:containerd://9b11acac1b891eafc7ff2b244f1c42938f71a575ce81ff917e3b7ebf61d2e045 Started:0xc00070497e}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:43:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:04.582298  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/etcd-zcy-z390-aorus-master"
Jan 20 12:43:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:04.582329  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/etcd-zcy-z390-aorus-master"
Jan 20 12:43:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:04.582560  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/etcd-zcy-z390-aorus-master"
Jan 20 12:43:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:04.582625  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d isTerminal=false
Jan 20 12:43:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:04.582646  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d updateType=0
Jan 20 12:43:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:04.639485  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/etcd-zcy-z390-aorus-master" volumeName="etcd-certs" volumeSpecName="etcd-certs"
Jan 20 12:43:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:04.639587  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/etcd-zcy-z390-aorus-master" volumeName="etcd-data" volumeSpecName="etcd-data"
Jan 20 12:43:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:05.483508  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:05.483579  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:05.491893  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[5c44b70e-a49d-45aa-8e29-8890711b754f] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:05 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ffa5a0 2 [] false false map[] 0xc000fcc100 0xc001095ef0}
Jan 20 12:43:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:05.491926  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:05.581978  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:43:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:05.587475  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:43:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:05.587488  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:43:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:05.588090  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:43:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:05.588498  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:43:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:05.588547  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:43:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:05.588554  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:43:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:05.588563  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:43:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:06.165043  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:43:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:06.165108  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:06.172026  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:06 GMT] X-Content-Type-Options:[nosniff]] 0xc0012e9320 2 [] false false map[] 0xc000fcc700 0xc001247550}
Jan 20 12:43:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:06.172055  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:43:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:06.272018  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:43:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:06.272077  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:06.273243  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:06 GMT]] 0xc000ffa740 29 [] true false map[] 0xc001f36100 <nil>}
Jan 20 12:43:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:06.273348  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:43:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:06.483187  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:06.483247  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:06.496721  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[291e2521-e1d8-4907-9c2b-ea437bd9ebff] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:06 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ce8d20 2 [] false false map[] 0xc000cf7f00 0xc0012df6b0}
Jan 20 12:43:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:06.496855  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:06.895734  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:43:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:06.895803  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:06.903125  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:06 GMT] X-Content-Type-Options:[nosniff]] 0xc000ce92e0 2 [] false false map[] 0xc0011c4200 0xc0013f1600}
Jan 20 12:43:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:06.903206  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:43:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:07.483187  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:07.483255  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:07.491730  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[6cfc4496-c85b-4769-a51c-8de0619827de] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:07 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ffa9c0 2 [] false false map[] 0xc0011c4700 0xc0014813f0}
Jan 20 12:43:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:07.491757  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:07.525390  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/etcd.yaml"
Jan 20 12:43:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:07.527508  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver.yaml"
Jan 20 12:43:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:07.530447  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Jan 20 12:43:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:07.531427  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-scheduler.yaml"
Jan 20 12:43:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:07.531718  199956 config.go:279] "Setting pods for source" source="file"
Jan 20 12:43:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:07.582074  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:43:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:07.587461  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:43:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:07.587471  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:43:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:07.587477  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:43:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:07.588100  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:43:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:07.588162  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:43:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:07.588168  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:43:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:07.588199  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:43:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:07.645262  199956 handler.go:293] error while reading "/proc/199956/fd/34" link: readlink /proc/199956/fd/34: no such file or directory
Jan 20 12:43:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:07.825064  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:43:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:08.483943  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:08.484035  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:08.491737  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[b496e952-3262-4b09-938f-a185dba0f5cc] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:08 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000707cc0 2 [] false false map[] 0xc0014c7000 0xc001a5bc30}
Jan 20 12:43:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:08.491770  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:08.879317  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:43:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:08.879391  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:08.880561  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:08 GMT] X-Content-Type-Options:[nosniff]] 0xc00123e140 2 [] true false map[] 0xc000338200 <nil>}
Jan 20 12:43:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:08.880684  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:43:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:09.484131  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:09.484194  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:09.492804  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[e78ccfa5-3d19-47bf-ae9e-7af9cf5452d0] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:09 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123e1a0 2 [] false false map[] 0xc000338600 0xc001d558c0}
Jan 20 12:43:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:09.492857  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:09.581752  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[kube-system/coredns-6d4b75cb6d-48zl2]
Jan 20 12:43:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:09.581840  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:43:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:09.581983  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 updateType=0
Jan 20 12:43:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:09.582062  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1
Jan 20 12:43:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:09.582095  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:43:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:09.582181  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/coredns-6d4b75cb6d-48zl2" oldPhase=Running phase=Running
Jan 20 12:43:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:09.582482  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/coredns-6d4b75cb6d-48zl2" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:45 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:45 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.2 PodIPs:[{IP:10.244.0.2}] StartTime:2023-01-20 12:31:42 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:coredns State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:43 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:k8s.gcr.io/coredns/coredns:v1.8.6 ImageID:k8s.gcr.io/coredns/coredns@sha256:5b6ec0d6de9baaf3e92d0f66cd96a25b9edbce8716f5f15dcd1a616b3abd590e ContainerID:containerd://b3af3b285c950aba4fcd0176473261f7f4700aeaafe9c73ae15b15a7d6304092 Started:0xc00147f119}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:43:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:09.582831  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:43:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:09.582903  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:43:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:09.583400  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:43:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:09.583570  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 isTerminal=false
Jan 20 12:43:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:09.583625  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 updateType=0
Jan 20 12:43:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:09.587439  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:43:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:09.587451  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:43:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:09.588203  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:43:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:09.588654  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:43:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:09.588709  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:43:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:09.588729  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:43:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:09.588737  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:43:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:09.676276  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/coredns-6d4b75cb6d-48zl2" volumeName="config-volume" volumeSpecName="config-volume"
Jan 20 12:43:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:09.676431  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/coredns-6d4b75cb6d-48zl2" volumeName="kube-api-access-9qh7j" volumeSpecName="kube-api-access-9qh7j"
Jan 20 12:43:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:09.685531  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-9qh7j\" (UniqueName: \"kubernetes.io/projected/d94e1927-d22d-4831-a764-0170eae346a1-kube-api-access-9qh7j\") pod \"coredns-6d4b75cb6d-48zl2\" (UID: \"d94e1927-d22d-4831-a764-0170eae346a1\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:43:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:09.685660  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/d94e1927-d22d-4831-a764-0170eae346a1-config-volume\") pod \"coredns-6d4b75cb6d-48zl2\" (UID: \"d94e1927-d22d-4831-a764-0170eae346a1\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:43:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:09.685795  199956 projected.go:183] Setting up volume kube-api-access-9qh7j for pod d94e1927-d22d-4831-a764-0170eae346a1 at /var/lib/kubelet/pods/d94e1927-d22d-4831-a764-0170eae346a1/volumes/kubernetes.io~projected/kube-api-access-9qh7j
Jan 20 12:43:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:09.685839  199956 configmap.go:181] Setting up volume config-volume for pod d94e1927-d22d-4831-a764-0170eae346a1 at /var/lib/kubelet/pods/d94e1927-d22d-4831-a764-0170eae346a1/volumes/kubernetes.io~configmap/config-volume
Jan 20 12:43:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:09.685978  199956 configmap.go:205] Received configMap kube-system/coredns containing (1) pieces of data, 339 total bytes
Jan 20 12:43:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:09.686049  199956 quota_linux.go:271] SupportsQuotas called, but quotas disabled
Jan 20 12:43:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:09.686235  199956 atomic_writer.go:161] pod kube-system/coredns-6d4b75cb6d-48zl2 volume config-volume: no update required for target directory /var/lib/kubelet/pods/d94e1927-d22d-4831-a764-0170eae346a1/volumes/kubernetes.io~configmap/config-volume
Jan 20 12:43:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:09.686266  199956 atomic_writer.go:161] pod kube-system/coredns-6d4b75cb6d-48zl2 volume kube-api-access-9qh7j: no update required for target directory /var/lib/kubelet/pods/d94e1927-d22d-4831-a764-0170eae346a1/volumes/kubernetes.io~projected/kube-api-access-9qh7j
Jan 20 12:43:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:09.686299  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/d94e1927-d22d-4831-a764-0170eae346a1-config-volume\") pod \"coredns-6d4b75cb6d-48zl2\" (UID: \"d94e1927-d22d-4831-a764-0170eae346a1\") " pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:43:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:09.686341  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-9qh7j\" (UniqueName: \"kubernetes.io/projected/d94e1927-d22d-4831-a764-0170eae346a1-kube-api-access-9qh7j\") pod \"coredns-6d4b75cb6d-48zl2\" (UID: \"d94e1927-d22d-4831-a764-0170eae346a1\") " pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:43:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:10.070837  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:43:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:10.078615  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:43:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:10.089322  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59469056Ki" capacity="65586124Ki" time="2023-01-20 12:43:10.073094857 -0500 EST m=+702.628869750"
Jan 20 12:43:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:10.089338  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64393208Ki" capacity="65061836Ki" time="2023-01-20 12:43:10.089258617 -0500 EST m=+702.645033442"
Jan 20 12:43:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:10.089345  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902011564Ki" capacity="981310056Ki" time="2023-01-20 12:43:10.073094857 -0500 EST m=+702.628869750"
Jan 20 12:43:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:10.089370  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:43:10.073094857 -0500 EST m=+702.628869750"
Jan 20 12:43:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:10.089377  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902011564Ki" capacity="981310056Ki" time="2023-01-20 12:43:01.262545704 -0500 EST"
Jan 20 12:43:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:10.089383  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:43:01.262545704 -0500 EST"
Jan 20 12:43:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:10.089389  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510775" capacity="511757" time="2023-01-20 12:43:10.08895755 -0500 EST m=+702.644732375"
Jan 20 12:43:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:10.089426  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:43:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:10.483592  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:10.483665  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:10.491579  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[9720e583-898f-4caa-8558-96d36264d7f3] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:10 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008e2aa0 2 [] false false map[] 0xc000fcc100 0xc000f8c4d0}
Jan 20 12:43:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:10.491610  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:11.166536  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:43:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:11.166606  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:11.174775  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[ba9ccbea-e506-43de-9f61-23051d37c1b7] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:11 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008e2f00 2 [] false false map[] 0xc0014c6b00 0xc001982840}
Jan 20 12:43:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:11.174805  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:11.483712  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:11.483772  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:11.496993  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[38be3a8f-c346-4424-bcb2-ac7ff8df86c5] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:11 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001718f00 2 [] false false map[] 0xc0014c7000 0xc000f02840}
Jan 20 12:43:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:11.497127  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:11.582126  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:43:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:11.588336  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:43:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:11.588350  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:43:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:11.588839  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:43:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:11.589189  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:43:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:11.589239  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:43:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:11.589246  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:43:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:11.589255  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:43:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:12.360765  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:43:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:12.360774  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:43:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:12.360930  199956 interface.go:209] Interface eno2 is up
Jan 20 12:43:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:12.360970  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:43:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:12.360978  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:43:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:12.360983  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:43:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:12.360986  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:43:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:12.360990  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:43:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:12.361273  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:43:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:12.361283  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:43:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:12.361286  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:43:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:12.361290  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:43:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:12.483901  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:12.483966  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:12.491504  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[9041e484-63f6-47da-8858-69a20fb8c9b4] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:12 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001719060 2 [] false false map[] 0xc0014cae00 0xc001186370}
Jan 20 12:43:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:12.491532  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:12.689672  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:43:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:12.689742  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:12.690873  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:12 GMT]] 0xc00129ca40 2 [] true false map[] 0xc0014cb700 <nil>}
Jan 20 12:43:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:12.690979  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:43:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:12.696200  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:43:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:12.696264  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:12.697268  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:12 GMT]] 0xc001719280 2 [] true false map[] 0xc0014cb900 <nil>}
Jan 20 12:43:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:12.697369  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:43:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:12.709819  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:43:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:12.709879  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:12.709891  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:43:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:12.709954  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:12.710892  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:12 GMT]] 0xc0008dc560 2 [] true false map[] 0xc001a4b700 <nil>}
Jan 20 12:43:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:12.710996  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:43:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:12.711001  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:12 GMT]] 0xc00129cb00 2 [] true false map[] 0xc0014cbb00 <nil>}
Jan 20 12:43:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:12.711105  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:43:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:12.826693  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:43:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:13.483775  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:13.483846  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:13.491656  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[07de42b1-149e-43b2-8ee6-04255960806f] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:13 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000320a00 2 [] false false map[] 0xc0014cbd00 0xc001279ef0}
Jan 20 12:43:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:13.491689  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:13.582013  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:43:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:13.587476  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:43:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:13.587489  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:43:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:13.587496  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:43:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:13.588067  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:43:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:13.588114  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:43:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:13.588121  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:43:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:13.588129  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:43:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:14.483500  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:14.483569  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:14.496632  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[51773d66-34b7-4656-943b-eb0684c2df10] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:14 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008dcd80 2 [] false false map[] 0xc000cf6100 0xc001345ef0}
Jan 20 12:43:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:14.496852  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:15.483796  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:15.483872  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:15.491839  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[28d4a72b-018b-4656-8cdf-b5cd8ff503d7] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:15 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008dd4c0 2 [] false false map[] 0xc001f36100 0xc001764000}
Jan 20 12:43:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:15.491869  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:15.582011  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[kube-system/kube-proxy-prhfv]
Jan 20 12:43:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:15.582097  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:43:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:15.582230  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/kube-proxy-prhfv" podUID=1c057a0e-3ee3-44f3-b294-434acc8f9491 updateType=0
Jan 20 12:43:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:15.582307  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/kube-proxy-prhfv" podUID=1c057a0e-3ee3-44f3-b294-434acc8f9491
Jan 20 12:43:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:15.582337  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/kube-proxy-prhfv"
Jan 20 12:43:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:15.582418  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/kube-proxy-prhfv" oldPhase=Running phase=Running
Jan 20 12:43:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:15.582713  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/kube-proxy-prhfv" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:43 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:43 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.239.159.53 PodIPs:[{IP:10.239.159.53}] StartTime:2023-01-20 12:31:42 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:kube-proxy State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:43 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:k8s.gcr.io/kube-proxy:v1.24.0 ImageID:k8s.gcr.io/kube-proxy@sha256:c957d602267fa61082ab8847914b2118955d0739d592cc7b01e278513478d6a8 ContainerID:containerd://f78ed441ff85d669a403a76c0987f2d86913431bd96d454b1a3605bdcf0474f2 Started:0xc001115fd9}] QOSClass:BestEffort EphemeralContainerStatuses:[]}
Jan 20 12:43:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:15.583062  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-proxy-prhfv"
Jan 20 12:43:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:15.583132  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/kube-proxy-prhfv"
Jan 20 12:43:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:15.583543  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/kube-proxy-prhfv"
Jan 20 12:43:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:15.583698  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/kube-proxy-prhfv" podUID=1c057a0e-3ee3-44f3-b294-434acc8f9491 isTerminal=false
Jan 20 12:43:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:15.583749  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/kube-proxy-prhfv" podUID=1c057a0e-3ee3-44f3-b294-434acc8f9491 updateType=0
Jan 20 12:43:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:15.587505  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:43:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:15.587515  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:43:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:15.588121  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:43:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:15.588494  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:43:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:15.588536  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:43:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:15.588542  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:43:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:15.588550  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:43:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:15.618998  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-proxy-prhfv" volumeName="kube-proxy" volumeSpecName="kube-proxy"
Jan 20 12:43:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:15.619106  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-proxy-prhfv" volumeName="xtables-lock" volumeSpecName="xtables-lock"
Jan 20 12:43:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:15.619178  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-proxy-prhfv" volumeName="lib-modules" volumeSpecName="lib-modules"
Jan 20 12:43:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:15.619297  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-proxy-prhfv" volumeName="kube-api-access-2sbqp" volumeSpecName="kube-api-access-2sbqp"
Jan 20 12:43:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:15.629184  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/1c057a0e-3ee3-44f3-b294-434acc8f9491-kube-proxy\") pod \"kube-proxy-prhfv\" (UID: \"1c057a0e-3ee3-44f3-b294-434acc8f9491\") Volume is already mounted to pod, but remount was requested." pod="kube-system/kube-proxy-prhfv"
Jan 20 12:43:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:15.629316  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-2sbqp\" (UniqueName: \"kubernetes.io/projected/1c057a0e-3ee3-44f3-b294-434acc8f9491-kube-api-access-2sbqp\") pod \"kube-proxy-prhfv\" (UID: \"1c057a0e-3ee3-44f3-b294-434acc8f9491\") Volume is already mounted to pod, but remount was requested." pod="kube-system/kube-proxy-prhfv"
Jan 20 12:43:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:15.629452  199956 projected.go:183] Setting up volume kube-api-access-2sbqp for pod 1c057a0e-3ee3-44f3-b294-434acc8f9491 at /var/lib/kubelet/pods/1c057a0e-3ee3-44f3-b294-434acc8f9491/volumes/kubernetes.io~projected/kube-api-access-2sbqp
Jan 20 12:43:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:15.629498  199956 configmap.go:181] Setting up volume kube-proxy for pod 1c057a0e-3ee3-44f3-b294-434acc8f9491 at /var/lib/kubelet/pods/1c057a0e-3ee3-44f3-b294-434acc8f9491/volumes/kubernetes.io~configmap/kube-proxy
Jan 20 12:43:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:15.629575  199956 configmap.go:205] Received configMap kube-system/kube-proxy containing (2) pieces of data, 1458 total bytes
Jan 20 12:43:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:15.629652  199956 quota_linux.go:271] SupportsQuotas called, but quotas disabled
Jan 20 12:43:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:15.629860  199956 atomic_writer.go:161] pod kube-system/kube-proxy-prhfv volume kube-api-access-2sbqp: no update required for target directory /var/lib/kubelet/pods/1c057a0e-3ee3-44f3-b294-434acc8f9491/volumes/kubernetes.io~projected/kube-api-access-2sbqp
Jan 20 12:43:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:15.629923  199956 atomic_writer.go:161] pod kube-system/kube-proxy-prhfv volume kube-proxy: no update required for target directory /var/lib/kubelet/pods/1c057a0e-3ee3-44f3-b294-434acc8f9491/volumes/kubernetes.io~configmap/kube-proxy
Jan 20 12:43:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:15.629925  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-2sbqp\" (UniqueName: \"kubernetes.io/projected/1c057a0e-3ee3-44f3-b294-434acc8f9491-kube-api-access-2sbqp\") pod \"kube-proxy-prhfv\" (UID: \"1c057a0e-3ee3-44f3-b294-434acc8f9491\") " pod="kube-system/kube-proxy-prhfv"
Jan 20 12:43:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:15.629994  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/1c057a0e-3ee3-44f3-b294-434acc8f9491-kube-proxy\") pod \"kube-proxy-prhfv\" (UID: \"1c057a0e-3ee3-44f3-b294-434acc8f9491\") " pod="kube-system/kube-proxy-prhfv"
Jan 20 12:43:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:16.164646  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:43:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:16.164694  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:16.169884  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:16 GMT] X-Content-Type-Options:[nosniff]] 0xc00129cd00 2 [] false false map[] 0xc0016ab000 0xc001120dc0}
Jan 20 12:43:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:16.169947  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:43:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:16.271831  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:43:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:16.271897  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:16.273181  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:16 GMT]] 0xc000321c80 29 [] true false map[] 0xc001717d00 <nil>}
Jan 20 12:43:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:16.273290  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:43:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:16.484071  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:16.484134  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:16.497448  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[3d743604-1e27-46d1-9eac-feaced39311c] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:16 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00129cdc0 2 [] false false map[] 0xc001717f00 0xc0019e7e40}
Jan 20 12:43:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:16.497585  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:16.896061  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:43:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:16.896130  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:16.903919  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:16 GMT] X-Content-Type-Options:[nosniff]] 0xc000321dc0 2 [] false false map[] 0xc0000dd800 0xc001c05ef0}
Jan 20 12:43:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:16.903971  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:43:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:17.483219  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:17.483283  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:17.491727  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[539e262f-9641-47a3-ba00-c36c79652e49] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:17 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000706340 2 [] false false map[] 0xc0016ab600 0xc001e32420}
Jan 20 12:43:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:17.491757  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:17.581153  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:43:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:17.586494  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:43:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:17.586505  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:43:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:17.586512  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:43:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:17.587063  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:43:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:17.587112  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:43:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:17.587118  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:43:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:17.587126  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:43:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:17.828347  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:43:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:18.483729  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:18.483799  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:18.497093  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[592f408c-3c86-4a23-a0d9-bd0b933b8ac5] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:18 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00129de20 2 [] false false map[] 0xc001a4bb00 0xc001ceaa50}
Jan 20 12:43:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:18.497224  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:18.879697  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:43:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:18.879775  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:18.879788  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/healthz" timeout="1s"
Jan 20 12:43:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:18.879852  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:18.880923  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:18 GMT] X-Content-Type-Options:[nosniff]] 0xc000978760 2 [] true false map[] 0xc001a4be00 <nil>}
Jan 20 12:43:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:18.881041  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:43:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:18.881018  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:18 GMT] X-Content-Type-Options:[nosniff]] 0xc0007079c0 2 [] true false map[] 0xc001774600 <nil>}
Jan 20 12:43:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:18.881123  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:43:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:19.483119  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:19.483182  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:19.491882  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[b7f1c1c8-e8af-41a6-9cb3-2ab1f7f43eb1] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:19 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0000a4080 2 [] false false map[] 0xc0002c2600 0xc0020a3970}
Jan 20 12:43:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:19.491912  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:19.581801  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:43:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:19.588800  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:43:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:19.588819  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:43:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:19.589439  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:43:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:19.589776  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:43:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:19.589824  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:43:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:19.589831  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:43:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:19.589840  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:43:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:20.089581  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:43:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:20.097502  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:43:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:20.107664  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59459048Ki" capacity="65586124Ki" time="2023-01-20 12:43:20.091633677 -0500 EST m=+712.647408569"
Jan 20 12:43:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:20.107677  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64393372Ki" capacity="65061836Ki" time="2023-01-20 12:43:20.107605313 -0500 EST m=+712.663380138"
Jan 20 12:43:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:20.107684  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902011528Ki" capacity="981310056Ki" time="2023-01-20 12:43:20.091633677 -0500 EST m=+712.647408569"
Jan 20 12:43:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:20.107690  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:43:20.091633677 -0500 EST m=+712.647408569"
Jan 20 12:43:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:20.107695  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902011528Ki" capacity="981310056Ki" time="2023-01-20 12:43:11.262195886 -0500 EST"
Jan 20 12:43:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:20.107701  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:43:11.262195886 -0500 EST"
Jan 20 12:43:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:20.107707  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510775" capacity="511757" time="2023-01-20 12:43:20.107335341 -0500 EST m=+712.663110166"
Jan 20 12:43:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:20.107733  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:43:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:20.483394  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:20.483463  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:20.491746  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[210f8b85-ea1d-499e-be1d-97e582cbdd69] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:20 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0000a4420 2 [] false false map[] 0xc000fcc100 0xc0015f3080}
Jan 20 12:43:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:20.491780  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:20.896618  199956 reflector.go:536] object-"confidential-containers-system"/"kube-root-ca.crt": Watch close - *v1.ConfigMap total 0 items received
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.166712  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.166784  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.174536  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[a8b2b0c3-740d-4aaa-8f1b-ee35a94b6e13] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:21 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123e1c0 2 [] false false map[] 0xc00105ab00 0xc001273340}
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.174567  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.209462  199956 handler.go:293] error while reading "/proc/199956/fd/34" link: readlink /proc/199956/fd/34: no such file or directory
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kata[207222]: time="2023-01-20T12:43:21.328669074-05:00" level=warning msg="notify on errors" error="failed to ping agent: Failed to Check if grpc server is working: context deadline exceeded" sandbox=38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 source=virtcontainers subsystem=virtcontainers/monitor
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kata[207222]: time="2023-01-20T12:43:21.328902324-05:00" level=warning msg="write error to watcher" error="failed to ping agent: Failed to Check if grpc server is working: context deadline exceeded" sandbox=38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 source=virtcontainers subsystem=virtcontainers/monitor
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:43:21.328669074-05:00" level=warning msg="notify on errors" error="failed to ping agent: Failed to Check if grpc server is working: context deadline exceeded" sandbox=38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 source=virtcontainers subsystem=virtcontainers/monitor
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:43:21.328902324-05:00" level=warning msg="write error to watcher" error="failed to ping agent: Failed to Check if grpc server is working: context deadline exceeded" sandbox=38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 source=virtcontainers subsystem=virtcontainers/monitor
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:43:21.329047720-05:00" level=error msg="agent pull image err. ttrpc: closed" name=containerd-shim-v2 pid=207222 sandbox=38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 source=virtcontainers subsystem=kata_agent
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:43:21.329181426-05:00" level=error msg="kata runtime PullImage err. ttrpc: closed" name=containerd-shim-v2 pid=207222 sandbox=38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 source=containerd-kata-shim-v2
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:43:21.329256379-05:00" level=warning msg="sandbox stopped unexpectedly" error="failed to ping agent: Failed to Check if grpc server is working: context deadline exceeded" name=containerd-shim-v2 pid=207222 sandbox=38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 source=containerd-kata-shim-v2
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kata[207222]: time="2023-01-20T12:43:21.32904772-05:00" level=error msg="agent pull image err. ttrpc: closed" name=containerd-shim-v2 pid=207222 sandbox=38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 source=virtcontainers subsystem=kata_agent
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: E0120 12:43:21.330601  199956 remote_image.go:238] "PullImage from image service failed" err="rpc error: code = Unknown desc = ttrpc: closed" image="ngcn-registry.sh.intel.com:443/ci-example256m:latest"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: E0120 12:43:21.330670  199956 kuberuntime_image.go:51] "Failed to pull image" err="rpc error: code = Unknown desc = ttrpc: closed" image="ngcn-registry.sh.intel.com:443/ci-example256m:latest"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: E0120 12:43:21.330842  199956 kuberuntime_manager.go:905] container &Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod unsigned-unencrypted-cc-1_default(4f97314d-815b-4787-b174-5c158cd28c9d): ErrImagePull: rpc error: code = Unknown desc = ttrpc: closed
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.330900  199956 kubelet.go:1503] "syncPod exit" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d isTerminal=false
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: E0120 12:43:21.330939  199956 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ci-example256m\" with ErrImagePull: \"rpc error: code = Unknown desc = ttrpc: closed\"" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.330986  199956 pod_workers.go:988] "Processing pod event done" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:43:21.330030694-05:00" level=error msg="PullImage \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\" failed" error="rpc error: code = Unknown desc = ttrpc: closed"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:43:21.329079732-05:00" level=error msg="Wait for process failed" container=38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 error="ttrpc: closed" name=containerd-shim-v2 pid=38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 sandbox=38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 source=containerd-kata-shim-v2
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kata[207222]: time="2023-01-20T12:43:21.329079732-05:00" level=error msg="Wait for process failed" container=38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 error="ttrpc: closed" name=containerd-shim-v2 pid=38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 sandbox=38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 source=containerd-kata-shim-v2
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.331416  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Warning" reason="Failed" message="Failed to pull image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\": rpc error: code = Unknown desc = ttrpc: closed"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.331454  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Warning" reason="Failed" message="Error: ErrImagePull"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kata[207222]: time="2023-01-20T12:43:21.329181426-05:00" level=error msg="kata runtime PullImage err. ttrpc: closed" name=containerd-shim-v2 pid=207222 sandbox=38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 source=containerd-kata-shim-v2
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kata[207222]: time="2023-01-20T12:43:21.329256379-05:00" level=warning msg="sandbox stopped unexpectedly" error="failed to ping agent: Failed to Check if grpc server is working: context deadline exceeded" name=containerd-shim-v2 pid=207222 sandbox=38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 source=containerd-kata-shim-v2
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kata[207222]: time="2023-01-20T12:43:21.333514534-05:00" level=warning msg="notify on errors" error="failed to ping agent: Failed to Check if grpc server is working: Dead agent" sandbox=38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 source=virtcontainers subsystem=virtcontainers/monitor
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kata[207222]: time="2023-01-20T12:43:21.333727421-05:00" level=warning msg="write error to watcher" error="failed to ping agent: Failed to Check if grpc server is working: Dead agent" sandbox=38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 source=virtcontainers subsystem=virtcontainers/monitor
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER systemd[1]: run-kata\x2dcontainers-shared-sandboxes-38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603-shared-38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603\x2d13215e78a77d383e\x2dresolv.conf.mount: Succeeded.
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER systemd[1]: run-kata\x2dcontainers-shared-sandboxes-38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603-mounts-38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603\x2d13215e78a77d383e\x2dresolv.conf.mount: Succeeded.
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:43:21.333514534-05:00" level=warning msg="notify on errors" error="failed to ping agent: Failed to Check if grpc server is working: Dead agent" sandbox=38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 source=virtcontainers subsystem=virtcontainers/monitor
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:43:21.333727421-05:00" level=warning msg="write error to watcher" error="failed to ping agent: Failed to Check if grpc server is working: Dead agent" sandbox=38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 source=virtcontainers subsystem=virtcontainers/monitor
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER systemd[1066]: run-kata\x2dcontainers-shared-sandboxes-38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603-shared-38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603\x2d13215e78a77d383e\x2dresolv.conf.mount: Succeeded.
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER systemd[1066]: run-kata\x2dcontainers-shared-sandboxes-38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603-mounts-38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603\x2d13215e78a77d383e\x2dresolv.conf.mount: Succeeded.
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER systemd[1066]: run-kata\x2dcontainers-shared-sandboxes-38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603-shared-38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603-rootfs.mount: Succeeded.
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER systemd[1066]: run-kata\x2dcontainers-shared-sandboxes-38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603-mounts-38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603-rootfs.mount: Succeeded.
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER systemd[193112]: run-kata\x2dcontainers-shared-sandboxes-38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603-shared-38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603\x2d13215e78a77d383e\x2dresolv.conf.mount: Succeeded.
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER systemd[193112]: run-kata\x2dcontainers-shared-sandboxes-38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603-mounts-38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603\x2d13215e78a77d383e\x2dresolv.conf.mount: Succeeded.
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER systemd[1]: run-kata\x2dcontainers-shared-sandboxes-38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603-shared-38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603-rootfs.mount: Succeeded.
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER systemd[1]: run-kata\x2dcontainers-shared-sandboxes-38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603-mounts-38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603-rootfs.mount: Succeeded.
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER systemd[193112]: run-kata\x2dcontainers-shared-sandboxes-38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603-shared-38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603-rootfs.mount: Succeeded.
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER systemd[193112]: run-kata\x2dcontainers-shared-sandboxes-38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603-mounts-38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603-rootfs.mount: Succeeded.
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kata[207222]: time="2023-01-20T12:43:21.336627371-05:00" level=warning msg="Agent did not stop sandbox" error="Dead agent" name=containerd-shim-v2 pid=207222 sandbox=38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 sandboxid=38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 source=virtcontainers subsystem=sandbox
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:43:21.336627371-05:00" level=warning msg="Agent did not stop sandbox" error="Dead agent" name=containerd-shim-v2 pid=207222 sandbox=38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 sandboxid=38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 source=virtcontainers subsystem=sandbox
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kata[207222]: time="2023-01-20T12:43:21.336888605-05:00" level=error msg="Failed to read guest console logs" console-protocol=unix console-url=/run/vc/vm/38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603/console.sock error="read unix @->/run/vc/vm/38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603/console.sock: use of closed network connection" name=containerd-shim-v2 pid=207222 sandbox=38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 source=virtcontainers subsystem=sandbox
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:43:21.336888605-05:00" level=error msg="Failed to read guest console logs" console-protocol=unix console-url=/run/vc/vm/38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603/console.sock error="read unix @->/run/vc/vm/38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603/console.sock: use of closed network connection" name=containerd-shim-v2 pid=207222 sandbox=38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 source=virtcontainers subsystem=sandbox
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.362019  199956 manager.go:1044] Destroyed container: "/kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603" (aliases: [38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 /kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603], namespace: "containerd")
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.362053  199956 handler.go:325] Added event &{/kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 2023-01-20 12:43:21.362043908 -0500 EST m=+713.917818732 containerDeletion {<nil>}}
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.364173  199956 manager.go:1044] Destroyed container: "/kata_overhead/38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603" (aliases: [38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 /kata_overhead/38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603], namespace: "containerd")
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.364187  199956 handler.go:325] Added event &{/kata_overhead/38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 2023-01-20 12:43:21.36418496 -0500 EST m=+713.919959785 containerDeletion {<nil>}}
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER systemd[193112]: run-kata\x2dcontainers-shared-sandboxes-38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603-shared.mount: Succeeded.
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER systemd[1]: run-kata\x2dcontainers-shared-sandboxes-38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603-shared.mount: Succeeded.
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER systemd[1066]: run-kata\x2dcontainers-shared-sandboxes-38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603-shared.mount: Succeeded.
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER systemd[193112]: run-containerd-io.containerd.runtime.v2.task-k8s.io-38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603-rootfs.mount: Succeeded.
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER systemd[1066]: run-containerd-io.containerd.runtime.v2.task-k8s.io-38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603-rootfs.mount: Succeeded.
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603-rootfs.mount: Succeeded.
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kata[207222]: time="2023-01-20T12:43:21.369831767-05:00" level=error msg="failed to cleanup the &{%!s(*cgroups.cgroup=&{0x565133dd0980 [0xc000081500 0xc0002c3710 0xc0002c3720 0xc0002c3730 0xc0002c3740 0xc0002c3750 0xc0002c3760 0xc0002c3770 0xc0002c3780 0xc00017e4c8 0xc000081520 0xc0002c3790 0xc0002c37b0 0xc00021c240] {0 0} <nil>}) /kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 %!s(*specs.LinuxCPU=&{<nil> <nil> <nil> <nil> <nil>  }) [{%!s(bool=false)  %!s(*int64=<nil>) %!s(*int64=<nil>) rwm} {%!s(bool=true) c %!s(*int64=0xc0003457b0) %!s(*int64=0xc0003457b8) rwm} {%!s(bool=true) c %!s(*int64=0xc0003457c0) %!s(*int64=0xc0003457c8) rwm} {%!s(bool=true) c %!s(*int64=0xc00017bf28) %!s(*int64=0xc00017bf30) rwm} {%!s(bool=true) c %!s(*int64=0xc00017bf58) %!s(*int64=0xc00017bf60) rwm} {%!s(bool=true) c %!s(*int64=0xc00017bf88) %!s(*int64=0xc00017bf90) rwm} {%!s(bool=true) c %!s(*int64=0xc00017bfb8) %!s(*int64=0xc00017bfc0) rwm} {%!s(bool=true) c %!s(*int64=0xc00021c0c8) %!s(*int64=0xc00021c0d0) rwm} {%!s(bool=true) c %!s(*int64=0xc00021c0f8) %!s(*int64=0xc00021c100) rwm} {%!s(bool=true) c %!s(*int64=0xc00021c128) %!s(*int64=0xc00021c130) rwm} {%!s(bool=true) c %!s(*int64=0xc00021c158) %!s(*int64=0xc00021c160) rwm} {%!s(bool=true) c %!s(*int64=0xc00021c188) %!s(*int64=0xc00021c190) rwm} {%!s(bool=true) c %!s(*int64=0xc00021c1b8) %!s(*int64=0xc00021c1c0) rwm} {%!s(bool=true) c %!s(*int64=0xc00021c1e8) %!s(*int64=0xc00021c1f0) rwm} {%!s(bool=true) c %!s(*int64=0xc000345988) %!s(*int64=0xc0003459a0) m} {%!s(bool=true) b %!s(*int64=0xc000345988) %!s(*int64=0xc0003459a0) m} {%!s(bool=true) c %!s(*int64=0xc0003459a8) %!s(*int64=0xc0003459a0) rwm} {%!s(bool=true) c %!s(*int64=0xc0003459b0) %!s(*int64=0xc0003459b8) rwm}] {%!s(int32=0) %!s(uint32=0)}} resource controllers" error="cgroups: cgroup deleted" name=containerd-shim-v2 pid=207222 sandbox=38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 source=virtcontainers subsystem=sandbox
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kata[207222]: time="2023-01-20T12:43:21.369902199-05:00" level=warning msg="Calling Cleanup() on an already cleaned up filesystem" name=containerd-shim-v2 pid=207222 sandbox=38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 source=virtcontainers subsystem="filesystem share"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:43:21.369831767-05:00" level=error msg="failed to cleanup the &{%!s(*cgroups.cgroup=&{0x565133dd0980 [0xc000081500 0xc0002c3710 0xc0002c3720 0xc0002c3730 0xc0002c3740 0xc0002c3750 0xc0002c3760 0xc0002c3770 0xc0002c3780 0xc00017e4c8 0xc000081520 0xc0002c3790 0xc0002c37b0 0xc00021c240] {0 0} <nil>}) /kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 %!s(*specs.LinuxCPU=&{<nil> <nil> <nil> <nil> <nil>  }) [{%!s(bool=false)  %!s(*int64=<nil>) %!s(*int64=<nil>) rwm} {%!s(bool=true) c %!s(*int64=0xc0003457b0) %!s(*int64=0xc0003457b8) rwm} {%!s(bool=true) c %!s(*int64=0xc0003457c0) %!s(*int64=0xc0003457c8) rwm} {%!s(bool=true) c %!s(*int64=0xc00017bf28) %!s(*int64=0xc00017bf30) rwm} {%!s(bool=true) c %!s(*int64=0xc00017bf58) %!s(*int64=0xc00017bf60) rwm} {%!s(bool=true) c %!s(*int64=0xc00017bf88) %!s(*int64=0xc00017bf90) rwm} {%!s(bool=true) c %!s(*int64=0xc00017bfb8) %!s(*int64=0xc00017bfc0) rwm} {%!s(bool=true) c %!s(*int64=0xc00021c0c8) %!s(*int64=0xc00021c0d0) rwm} {%!s(bool=true) c %!s(*int64=0xc00021c0f8) %!s(*int64=0xc00021c100) rwm} {%!s(bool=true) c %!s(*int64=0xc00021c128) %!s(*int64=0xc00021c130) rwm} {%!s(bool=true) c %!s(*int64=0xc00021c158) %!s(*int64=0xc00021c160) rwm} {%!s(bool=true) c %!s(*int64=0xc00021c188) %!s(*int64=0xc00021c190) rwm} {%!s(bool=true) c %!s(*int64=0xc00021c1b8) %!s(*int64=0xc00021c1c0) rwm} {%!s(bool=true) c %!s(*int64=0xc00021c1e8) %!s(*int64=0xc00021c1f0) rwm} {%!s(bool=true) c %!s(*int64=0xc000345988) %!s(*int64=0xc0003459a0) m} {%!s(bool=true) b %!s(*int64=0xc000345988) %!s(*int64=0xc0003459a0) m} {%!s(bool=true) c %!s(*int64=0xc0003459a8) %!s(*int64=0xc0003459a0) rwm} {%!s(bool=true) c %!s(*int64=0xc0003459b0) %!s(*int64=0xc0003459b8) rwm}] {%!s(int32=0) %!s(uint32=0)}} resource controllers" error="cgroups: cgroup deleted" name=containerd-shim-v2 pid=207222 sandbox=38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 source=virtcontainers subsystem=sandbox
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:43:21.369902199-05:00" level=warning msg="Calling Cleanup() on an already cleaned up filesystem" name=containerd-shim-v2 pid=207222 sandbox=38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 source=virtcontainers subsystem="filesystem share"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:43:21.371551043-05:00" level=error msg="error receiving message" error="read unix /run/containerd/containerd.sock.ttrpc->@: read: connection reset by peer"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:43:21.371577680-05:00" level=info msg="shim disconnected" id=38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:43:21.371597929-05:00" level=warning msg="cleaning up after shim disconnected" id=38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 namespace=k8s.io
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:43:21.371604543-05:00" level=info msg="cleaning up dead shim"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:43:21.381985941-05:00" level=error msg="failed to delete" cmd="/usr/local/bin/containerd-shim-kata-qemu-v2 -namespace k8s.io -address /run/containerd/containerd.sock -publish-binary /opt/confidential-containers/bin/containerd -id 38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 -bundle /run/containerd/io.containerd.runtime.v2.task/k8s.io/38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 delete" error="exit status 1"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:43:21.382038272-05:00" level=warning msg="failed to clean up after shim disconnected" error="time=\"2023-01-20T12:43:21-05:00\" level=warning msg=\"failed to cleanup container\" container=38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 error=\"open /run/vc/sbs/38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603: no such file or directory\" name=containerd-shim-v2 pid=207381 sandbox=38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 source=containerd-kata-shim-v2\nio.containerd.kata.v2: open /run/vc/sbs/38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603: no such file or directory: exit status 1" id=38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 namespace=k8s.io
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.483778  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.483848  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.493061  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[61f5e24e-b800-46b3-a8ee-9f3368ca1ad9] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:21 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008e2300 2 [] false false map[] 0xc0011c4300 0xc0013ce0b0}
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.493128  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.580723  199956 generic.go:155] "GenericPLEG" podUID=4f97314d-815b-4787-b174-5c158cd28c9d containerID="38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603" oldState=running newState=exited
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.581442  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.581661  199956 kuberuntime_manager.go:1037] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603] pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.583039  199956 generic.go:421] "PLEG: Write status" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.588882  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.588945  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.590852  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.592654  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.592913  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.592947  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.592990  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.593039  199956 kubelet.go:2098] "SyncLoop (PLEG): event for pod" pod="default/unsigned-unencrypted-cc-1" event=&{ID:4f97314d-815b-4787-b174-5c158cd28c9d Type:ContainerDied Data:38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603}
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.593084  199956 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.593123  199956 pod_workers.go:888] "Processing pod event" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.593148  199956 kubelet.go:1501] "syncPod enter" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.593177  199956 kubelet_pods.go:1435] "Generating pod status" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.593243  199956 kubelet_pods.go:1447] "Got phase for pod" pod="default/unsigned-unencrypted-cc-1" oldPhase=Pending phase=Pending
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.593654  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.593729  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.593763  199956 kuberuntime_manager.go:488] "No ready sandbox for pod can be found. Need to start a new one" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.593806  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:true CreateSandbox:true SandboxID:38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603 Attempt:2 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.593855  199956 kuberuntime_manager.go:730] "Stopping PodSandbox for pod, will start new one" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.594002  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="" kind="Pod" apiVersion="v1" type="Normal" reason="SandboxChanged" message="Pod sandbox changed, it will be killed and re-created."
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:43:21.594259159-05:00" level=info msg="StopPodSandbox for \"38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603\""
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER systemd[193112]: run-containerd-io.containerd.grpc.v1.cri-sandboxes-38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603-shm.mount: Succeeded.
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER systemd[1]: run-containerd-io.containerd.grpc.v1.cri-sandboxes-38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603-shm.mount: Succeeded.
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER systemd[1066]: run-containerd-io.containerd.grpc.v1.cri-sandboxes-38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603-shm.mount: Succeeded.
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.614454  199956 config.go:279] "Setting pods for source" source="api"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.615234  199956 status_manager.go:685] "Patch status for pod" pod="default/unsigned-unencrypted-cc-1" patch="{\"metadata\":{\"uid\":\"4f97314d-815b-4787-b174-5c158cd28c9d\"},\"status\":{\"containerStatuses\":[{\"image\":\"ngcn-registry.sh.intel.com:443/ci-example256m:latest\",\"imageID\":\"\",\"lastState\":{},\"name\":\"ci-example256m\",\"ready\":false,\"restartCount\":0,\"started\":false,\"state\":{\"waiting\":{\"message\":\"rpc error: code = Unknown desc = ttrpc: closed\",\"reason\":\"ErrImagePull\"}}}]}}"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.615412  199956 status_manager.go:694] "Status for pod updated successfully" pod="default/unsigned-unencrypted-cc-1" statusVersion=4 status={Phase:Pending Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.13 PodIPs:[{IP:10.244.0.13}] StartTime:2023-01-20 12:41:08 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:ci-example256m State:{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = ttrpc: closed,} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest ImageID: ContainerID: Started:0xc0017472ae}] QOSClass:Guaranteed EphemeralContainerStatuses:[]}
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.615967  199956 kubelet.go:2073] "SyncLoop RECONCILE" source="api" pods=[default/unsigned-unencrypted-cc-1]
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER avahi-daemon[896]: Interface veth187c5243.IPv6 no longer relevant for mDNS.
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER avahi-daemon[896]: Leaving mDNS multicast group on interface veth187c5243.IPv6 with address fe80::6c2c:6aff:febc:ec59.
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kernel: cni0: port 6(veth187c5243) entered disabled state
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kernel: device veth187c5243 left promiscuous mode
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kernel: cni0: port 6(veth187c5243) entered disabled state
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.664762  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="default/unsigned-unencrypted-cc-1" volumeName="kube-api-access-qcb8g" volumeSpecName="kube-api-access-qcb8g"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.674899  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") Volume is already mounted to pod, but remount was requested." pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.675103  199956 projected.go:183] Setting up volume kube-api-access-qcb8g for pod 4f97314d-815b-4787-b174-5c158cd28c9d at /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.675531  199956 atomic_writer.go:161] pod default/unsigned-unencrypted-cc-1 volume kube-api-access-qcb8g: no update required for target directory /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.675605  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") " pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER avahi-daemon[896]: Withdrawing address record for fe80::6c2c:6aff:febc:ec59 on veth187c5243.
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER NetworkManager[905]: <info>  [1674236601.7008] device (veth187c5243): released from master device cni0
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER gnome-shell[1450]: Removing a network device that was not added
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:43:21.731667179-05:00" level=info msg="TearDown network for sandbox \"38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603\" successfully"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:43:21.731751570-05:00" level=info msg="StopPodSandbox for \"38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603\" returns successfully"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.732196  199956 kuberuntime_manager.go:785] "Creating PodSandbox for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.732502  199956 kuberuntime_sandbox.go:63] "Running pod with runtime handler" pod="default/unsigned-unencrypted-cc-1" runtimeHandler="kata-qemu"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:43:21.733143932-05:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:unsigned-unencrypted-cc-1,Uid:4f97314d-815b-4787-b174-5c158cd28c9d,Namespace:default,Attempt:2,}"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER systemd[1]: run-netns-cni\x2d69f7cac6\x2d4023\x2da09b\x2d980b\x2db9a67db44670.mount: Succeeded.
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER systemd[193112]: run-netns-cni\x2d69f7cac6\x2d4023\x2da09b\x2d980b\x2db9a67db44670.mount: Succeeded.
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER systemd[1066]: run-netns-cni\x2d69f7cac6\x2d4023\x2da09b\x2d980b\x2db9a67db44670.mount: Succeeded.
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER systemd-udevd[207415]: ethtool: autonegotiation is unset or enabled, the speed and duplex are not writable.
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER systemd-udevd[207415]: Using default interface naming scheme 'v245'.
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER systemd-udevd[207415]: veth35c14a32: Could not generate persistent MAC: No data available
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER NetworkManager[905]: <info>  [1674236601.7547] manager: (veth35c14a32): new Veth device (/org/freedesktop/NetworkManager/Devices/124)
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kernel: cni0: port 6(veth35c14a32) entered blocking state
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kernel: cni0: port 6(veth35c14a32) entered disabled state
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kernel: device veth35c14a32 entered promiscuous mode
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kernel: cni0: port 6(veth35c14a32) entered blocking state
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kernel: cni0: port 6(veth35c14a32) entered forwarding state
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER NetworkManager[905]: <info>  [1674236601.7624] device (veth35c14a32): carrier: link connected
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kernel: IPv6: ADDRCONF(NETDEV_CHANGE): veth35c14a32: link becomes ready
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER containerd[201983]: map[string]interface {}{"cniVersion":"0.3.1", "hairpinMode":true, "ipMasq":false, "ipam":map[string]interface {}{"ranges":[][]map[string]interface {}{[]map[string]interface {}{map[string]interface {}{"subnet":"10.244.0.0/24"}}}, "routes":[]types.Route{types.Route{Dst:net.IPNet{IP:net.IP{0xa, 0xf4, 0x0, 0x0}, Mask:net.IPMask{0xff, 0xff, 0x0, 0x0}}, GW:net.IP(nil)}}, "type":"host-local"}, "isDefaultGateway":true, "isGateway":true, "mtu":(*uint)(0xc000018938), "name":"cbr0", "type":"bridge"}
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:21.804416  199956 factory.go:258] Using factory "containerd" for container "/kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615"
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER kernel: eth0: Caught tx_queue_len zero misconfig
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER virtiofsd[207483]: zcy-Z390-AORUS-MASTER virtiofsd[207483]: Use of deprecated flag '-f': This flag has no effect, please remove it
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER virtiofsd[207483]: zcy-Z390-AORUS-MASTER virtiofsd[207483]: Use of deprecated option format '-o': Please specify options without it (e.g., '--cache auto' instead of '-o cache=auto')
Jan 20 12:43:21 zcy-Z390-AORUS-MASTER virtiofsd[207487]: zcy-Z390-AORUS-MASTER virtiofsd[207483]: Waiting for vhost-user socket connection...
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER virtiofsd[207487]: zcy-Z390-AORUS-MASTER virtiofsd[207483]: Client connected, servicing requests
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.483568  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.483587  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.486212  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[229c8b23-91c6-47fa-ac04-4e4e256f42dd] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:22 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008e2ee0 2 [] false false map[] 0xc001f36000 0xc0017eca50}
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.486245  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.528350  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.528360  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.528521  199956 interface.go:209] Interface eno2 is up
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.528551  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.528561  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.528573  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.528576  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.528580  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.528854  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.528862  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.528866  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.528870  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.589374  199956 manager.go:988] Added container: "/kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615" (aliases: [f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 /kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615], namespace: "containerd")
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.589474  199956 handler.go:325] Added event &{/kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 2023-01-20 12:43:21.79787848 -0500 EST containerCreation {<nil>}}
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.589526  199956 container.go:530] Start housekeeping for container "/kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615"
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.589718  199956 factory.go:258] Using factory "containerd" for container "/kata_overhead/f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615"
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER containerd[201983]: {"cniVersion":"0.3.1","hairpinMode":true,"ipMasq":false,"ipam":{"ranges":[[{"subnet":"10.244.0.0/24"}]],"routes":[{"dst":"10.244.0.0/16"}],"type":"host-local"},"isDefaultGateway":true,"isGateway":true,"mtu":1450,"name":"cbr0","type":"bridge"}time="2023-01-20T12:43:22.589954085-05:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:unsigned-unencrypted-cc-1,Uid:4f97314d-815b-4787-b174-5c158cd28c9d,Namespace:default,Attempt:2,} returns sandbox id \"f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615\""
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.590049  199956 kuberuntime_manager.go:823] "Created PodSandbox for pod" podSandboxID="f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.590216  199956 kuberuntime_manager.go:846] "Determined the ip for pod after sandbox changed" IPs=[10.244.0.14] pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.590301  199956 kuberuntime_manager.go:889] "Creating container in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.590503  199956 manager.go:988] Added container: "/kata_overhead/f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615" (aliases: [f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 /kata_overhead/f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615], namespace: "containerd")
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.590512  199956 kuberuntime_manager.go:903] "Container start failed in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1" containerMessage="Back-off pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\"" err="ImagePullBackOff"
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.590528  199956 kubelet.go:1503] "syncPod exit" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d isTerminal=false
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: E0120 12:43:22.590537  199956 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ci-example256m\" with ImagePullBackOff: \"Back-off pulling image \\\"ngcn-registry.sh.intel.com:443/ci-example256m:latest\\\"\"" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.590540  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Normal" reason="BackOff" message="Back-off pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\""
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.590549  199956 pod_workers.go:988] "Processing pod event done" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.590550  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Warning" reason="Failed" message="Error: ImagePullBackOff"
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.590598  199956 handler.go:325] Added event &{/kata_overhead/f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 2023-01-20 12:43:21.83787848 -0500 EST containerCreation {<nil>}}
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.590617  199956 container.go:530] Start housekeeping for container "/kata_overhead/f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615"
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.689387  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.689457  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.690571  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:22 GMT]] 0xc0008e3fa0 2 [] true false map[] 0xc000fcd700 <nil>}
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.690679  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.695896  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.695955  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.697005  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:22 GMT]] 0xc0008e3fe0 2 [] true false map[] 0xc001716900 <nil>}
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.697104  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.710329  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.710392  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.710397  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.710458  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.711385  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:22 GMT]] 0xc0009a1ce0 2 [] true false map[] 0xc001847d00 <nil>}
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.711412  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:22 GMT]] 0xc0003206c0 2 [] true false map[] 0xc001716b00 <nil>}
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.711490  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.711515  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER avahi-daemon[896]: Joining mDNS multicast group on interface veth35c14a32.IPv6 with address fe80::788f:5dff:fe7d:6cc7.
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER avahi-daemon[896]: New relevant interface veth35c14a32.IPv6 for mDNS.
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER avahi-daemon[896]: Registering new address record for fe80::788f:5dff:fe7d:6cc7 on veth35c14a32.*.
Jan 20 12:43:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:22.829865  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.484250  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.484324  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.492754  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[08a50f09-8ff0-4bdf-85e4-d8ebf1f0d370] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:23 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0020fc340 2 [] false false map[] 0xc0000dd500 0xc001c48000}
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.492782  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.581421  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.586897  199956 generic.go:155] "GenericPLEG" podUID=4f97314d-815b-4787-b174-5c158cd28c9d containerID="f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615" oldState=non-existent newState=running
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.587337  199956 kuberuntime_manager.go:1037] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603] pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.587544  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.587557  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.587564  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.587850  199956 generic.go:421] "PLEG: Write status" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.587971  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.588019  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.588026  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.588034  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.588045  199956 kubelet.go:2098] "SyncLoop (PLEG): event for pod" pod="default/unsigned-unencrypted-cc-1" event=&{ID:4f97314d-815b-4787-b174-5c158cd28c9d Type:ContainerStarted Data:f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615}
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.588058  199956 pod_workers.go:888] "Processing pod event" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.588064  199956 kubelet.go:1501] "syncPod enter" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.588071  199956 kubelet_pods.go:1435] "Generating pod status" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.588087  199956 kubelet_pods.go:1447] "Got phase for pod" pod="default/unsigned-unencrypted-cc-1" oldPhase=Pending phase=Pending
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.588169  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.588183  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.588192  199956 kuberuntime_manager.go:638] "Container of pod is not in the desired state and shall be started" containerName="ci-example256m" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.588202  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 Attempt:2 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.588260  199956 kuberuntime_manager.go:889] "Creating container in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.588441  199956 kuberuntime_manager.go:903] "Container start failed in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1" containerMessage="Back-off pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\"" err="ImagePullBackOff"
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.588455  199956 kubelet.go:1503] "syncPod exit" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d isTerminal=false
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: E0120 12:43:23.588463  199956 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ci-example256m\" with ImagePullBackOff: \"Back-off pulling image \\\"ngcn-registry.sh.intel.com:443/ci-example256m:latest\\\"\"" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.588473  199956 pod_workers.go:988] "Processing pod event done" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.588479  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Normal" reason="BackOff" message="Back-off pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\""
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.588488  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Warning" reason="Failed" message="Error: ImagePullBackOff"
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.592438  199956 status_manager.go:685] "Patch status for pod" pod="default/unsigned-unencrypted-cc-1" patch="{\"metadata\":{\"uid\":\"4f97314d-815b-4787-b174-5c158cd28c9d\"},\"status\":{\"$setElementOrder/podIPs\":[{\"ip\":\"10.244.0.14\"}],\"containerStatuses\":[{\"image\":\"ngcn-registry.sh.intel.com:443/ci-example256m:latest\",\"imageID\":\"\",\"lastState\":{},\"name\":\"ci-example256m\",\"ready\":false,\"restartCount\":0,\"started\":false,\"state\":{\"waiting\":{\"message\":\"Back-off pulling image \\\"ngcn-registry.sh.intel.com:443/ci-example256m:latest\\\"\",\"reason\":\"ImagePullBackOff\"}}}],\"podIP\":\"10.244.0.14\",\"podIPs\":[{\"ip\":\"10.244.0.14\"},{\"$patch\":\"delete\",\"ip\":\"10.244.0.13\"}]}}"
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.592487  199956 status_manager.go:694] "Status for pod updated successfully" pod="default/unsigned-unencrypted-cc-1" statusVersion=5 status={Phase:Pending Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.14 PodIPs:[{IP:10.244.0.14}] StartTime:2023-01-20 12:41:08 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:ci-example256m State:{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "ngcn-registry.sh.intel.com:443/ci-example256m:latest",} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest ImageID: ContainerID: Started:0xc0021832d9}] QOSClass:Guaranteed EphemeralContainerStatuses:[]}
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.592441  199956 config.go:279] "Setting pods for source" source="api"
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.592807  199956 kubelet.go:2073] "SyncLoop RECONCILE" source="api" pods=[default/unsigned-unencrypted-cc-1]
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.678352  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="default/unsigned-unencrypted-cc-1" volumeName="kube-api-access-qcb8g" volumeSpecName="kube-api-access-qcb8g"
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.688411  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") Volume is already mounted to pod, but remount was requested." pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.688603  199956 projected.go:183] Setting up volume kube-api-access-qcb8g for pod 4f97314d-815b-4787-b174-5c158cd28c9d at /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.689057  199956 atomic_writer.go:161] pod default/unsigned-unencrypted-cc-1 volume kube-api-access-qcb8g: no update required for target directory /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:43:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:23.689126  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") " pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:24.483763  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:24.483836  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:24.491673  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[372ec372-69f9-4ef9-aa26-24bc091fbfee] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:24 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0020fcae0 2 [] false false map[] 0xc000cf6200 0xc001f2b600}
Jan 20 12:43:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:24.491704  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:25.483315  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:25.483385  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:25.491864  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[43beedae-768d-4345-b1cf-d288c9046195] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:25 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0020fcb60 2 [] false false map[] 0xc000fcde00 0xc001f973f0}
Jan 20 12:43:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:25.491894  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:25.581497  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:43:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:25.587603  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:43:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:25.587712  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:43:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:25.589093  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:43:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:25.589461  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:43:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:25.589511  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:43:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:25.589518  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:43:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:25.589527  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:43:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:26.164626  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:43:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:26.164718  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:26.167334  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:26 GMT] X-Content-Type-Options:[nosniff]] 0xc000321060 2 [] false false map[] 0xc0016aa100 0xc0013616b0}
Jan 20 12:43:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:26.167361  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:43:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:26.272320  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:43:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:26.272384  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:26.273673  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:26 GMT]] 0xc0009a01e0 29 [] true false map[] 0xc0016aa400 <nil>}
Jan 20 12:43:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:26.273780  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:43:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:26.483513  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:26.483577  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:26.491479  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[6af9d3ec-a49c-4cc6-bb73-375643f7fe2c] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:26 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000321320 2 [] false false map[] 0xc000fcc500 0xc0013618c0}
Jan 20 12:43:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:26.491509  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:26.895863  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:43:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:26.895931  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:26.903043  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:26 GMT] X-Content-Type-Options:[nosniff]] 0xc000321860 2 [] false false map[] 0xc0016ab000 0xc000f8d600}
Jan 20 12:43:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:26.903071  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.483664  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.483729  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.491811  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[ff2cea84-9c1c-49a9-8e29-c19f6cac669a] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:27 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e95c0 2 [] false false map[] 0xc000fcc800 0xc00161d6b0}
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.491843  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.526357  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/etcd.yaml"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.527186  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver.yaml"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.528210  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.529222  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-scheduler.yaml"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.529782  199956 config.go:279] "Setting pods for source" source="file"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.538954  199956 kubelet_getters.go:176] "Pod status updated" pod="kube-system/etcd-zcy-z390-aorus-master" status=Running
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.538994  199956 kubelet_getters.go:176] "Pod status updated" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" status=Running
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.539006  199956 kubelet_getters.go:176] "Pod status updated" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" status=Running
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.539015  199956 kubelet_getters.go:176] "Pod status updated" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" status=Running
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.581488  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.587396  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.587408  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.587415  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.587985  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.588032  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.588039  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.588047  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629377  199956 factory.go:262] Factory "containerd" was unable to handle container "/init.scope"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629388  199956 factory.go:262] Factory "systemd" was unable to handle container "/init.scope"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629393  199956 factory.go:255] Factory "raw" can handle container "/init.scope", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629399  199956 manager.go:925] ignoring container "/init.scope"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629402  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2dd678d9bc\\x2d8fcb\\x2d9fbe\\x2d1142\\x2ddede54862bab.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629406  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2dd678d9bc\\x2d8fcb\\x2d9fbe\\x2d1142\\x2ddede54862bab.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629411  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2dd678d9bc\\x2d8fcb\\x2d9fbe\\x2d1142\\x2ddede54862bab.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629414  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-1000.slice"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629417  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-1000.slice"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629420  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-1000.slice", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629424  199956 manager.go:925] ignoring container "/user.slice/user-1000.slice"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629427  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-0f9ae677fe935afcf43e145281deae0d663ba95fda6759ff24f0dd3e1cee17a9-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629431  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-0f9ae677fe935afcf43e145281deae0d663ba95fda6759ff24f0dd3e1cee17a9-rootfs.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629436  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-0f9ae677fe935afcf43e145281deae0d663ba95fda6759ff24f0dd3e1cee17a9-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629440  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-d2688d45\\x2d2487\\x2d46e7\\x2daecb\\x2de3479626909d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d4pnfq.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629444  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-d2688d45\\x2d2487\\x2d46e7\\x2daecb\\x2de3479626909d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d4pnfq.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629450  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-d2688d45\\x2d2487\\x2d46e7\\x2daecb\\x2de3479626909d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d4pnfq.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629454  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629458  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-rootfs.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629463  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629467  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-fs-fuse-connections.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629470  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-fs-fuse-connections.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629474  199956 manager.go:925] ignoring container "/system.slice/sys-fs-fuse-connections.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629477  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-random-seed.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629480  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-random-seed.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629483  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-random-seed.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629487  199956 manager.go:925] ignoring container "/system.slice/systemd-random-seed.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629490  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-7af065b7\\x2d9095\\x2d4d91\\x2d9b9e\\x2d2644e7b1f4bf-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dx6vjr.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629494  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-7af065b7\\x2d9095\\x2d4d91\\x2d9b9e\\x2d2644e7b1f4bf-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dx6vjr.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629500  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-7af065b7\\x2d9095\\x2d4d91\\x2d9b9e\\x2d2644e7b1f4bf-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dx6vjr.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629504  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-timesyncd.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629507  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-timesyncd.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629511  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-timesyncd.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629515  199956 manager.go:925] ignoring container "/system.slice/systemd-timesyncd.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629517  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-kernel-config.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629521  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-kernel-config.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629525  199956 manager.go:925] ignoring container "/system.slice/sys-kernel-config.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629528  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-remount-fs.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629530  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-remount-fs.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629534  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-remount-fs.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629538  199956 manager.go:925] ignoring container "/system.slice/systemd-remount-fs.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629541  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-04e472cb\\x2db6f9\\x2d4065\\x2db509\\x2dd7e1579fc48d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dhqj8d.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629545  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-04e472cb\\x2db6f9\\x2d4065\\x2db509\\x2dd7e1579fc48d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dhqj8d.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629551  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-04e472cb\\x2db6f9\\x2d4065\\x2db509\\x2dd7e1579fc48d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dhqj8d.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629555  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/keyboard-setup.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629557  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/keyboard-setup.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629560  199956 factory.go:255] Factory "raw" can handle container "/system.slice/keyboard-setup.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629564  199956 manager.go:925] ignoring container "/system.slice/keyboard-setup.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629567  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-logind.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629570  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-logind.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629573  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-logind.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629577  199956 manager.go:925] ignoring container "/system.slice/systemd-logind.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629580  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-systemd\\x2dfsck.slice/systemd-fsck@dev-disk-by\\x2duuid-4B5A\\x2dDC89.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629583  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-systemd\\x2dfsck.slice/systemd-fsck@dev-disk-by\\x2duuid-4B5A\\x2dDC89.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629587  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-systemd\\x2dfsck.slice/systemd-fsck@dev-disk-by\\x2duuid-4B5A\\x2dDC89.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629592  199956 manager.go:925] ignoring container "/system.slice/system-systemd\\x2dfsck.slice/systemd-fsck@dev-disk-by\\x2duuid-4B5A\\x2dDC89.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629595  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/ufw.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629598  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/ufw.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629601  199956 factory.go:255] Factory "raw" can handle container "/system.slice/ufw.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629605  199956 manager.go:925] ignoring container "/system.slice/ufw.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629608  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/cups.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629611  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/cups.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629614  199956 factory.go:255] Factory "raw" can handle container "/system.slice/cups.socket", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629618  199956 manager.go:925] ignoring container "/system.slice/cups.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629621  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-kernel-debug-tracing.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629624  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-kernel-debug-tracing.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629628  199956 manager.go:925] ignoring container "/system.slice/sys-kernel-debug-tracing.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629631  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f8443d2868215ec42d3fa335663976e33df166c5e98369aa3f9d7ddb9235bead-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629635  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f8443d2868215ec42d3fa335663976e33df166c5e98369aa3f9d7ddb9235bead-rootfs.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629640  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f8443d2868215ec42d3fa335663976e33df166c5e98369aa3f9d7ddb9235bead-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629644  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/boot-efi.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629647  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/boot-efi.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629651  199956 manager.go:925] ignoring container "/system.slice/boot-efi.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629653  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-kernel-tracing.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629657  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-kernel-tracing.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629661  199956 manager.go:925] ignoring container "/system.slice/sys-kernel-tracing.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629664  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/kmod-static-nodes.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629667  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/kmod-static-nodes.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629671  199956 factory.go:255] Factory "raw" can handle container "/system.slice/kmod-static-nodes.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629675  199956 manager.go:925] ignoring container "/system.slice/kmod-static-nodes.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629677  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-udevd.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629680  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-udevd.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629683  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-udevd.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629688  199956 manager.go:925] ignoring container "/system.slice/systemd-udevd.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629691  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e10efaa459a32161059fda75d4bcaf3bbdb896781f772b508e43fe00bc7c9003-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629695  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e10efaa459a32161059fda75d4bcaf3bbdb896781f772b508e43fe00bc7c9003-rootfs.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629700  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e10efaa459a32161059fda75d4bcaf3bbdb896781f772b508e43fe00bc7c9003-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629703  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629706  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629709  199956 factory.go:255] Factory "raw" can handle container "/system.slice", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629712  199956 manager.go:925] ignoring container "/system.slice"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629715  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/ModemManager.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629718  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/ModemManager.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629721  199956 factory.go:255] Factory "raw" can handle container "/system.slice/ModemManager.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629725  199956 manager.go:925] ignoring container "/system.slice/ModemManager.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629728  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f78ed441ff85d669a403a76c0987f2d86913431bd96d454b1a3605bdcf0474f2-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629732  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f78ed441ff85d669a403a76c0987f2d86913431bd96d454b1a3605bdcf0474f2-rootfs.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629736  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f78ed441ff85d669a403a76c0987f2d86913431bd96d454b1a3605bdcf0474f2-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629740  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629744  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-rootfs.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629749  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629752  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/proc-sys-fs-binfmt_misc.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629756  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/proc-sys-fs-binfmt_misc.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629760  199956 manager.go:925] ignoring container "/system.slice/proc-sys-fs-binfmt_misc.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629763  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/session-40.scope"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629766  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/session-40.scope"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629769  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/session-40.scope", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629773  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/session-40.scope"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629776  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/thermald.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629779  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/thermald.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629782  199956 factory.go:255] Factory "raw" can handle container "/system.slice/thermald.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629786  199956 manager.go:925] ignoring container "/system.slice/thermald.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629789  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-shm.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629793  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-shm.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629798  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-shm.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629801  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-gtk\\x2dcommon\\x2dthemes-1535.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629805  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-gtk\\x2dcommon\\x2dthemes-1535.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629809  199956 manager.go:925] ignoring container "/system.slice/snap-gtk\\x2dcommon\\x2dthemes-1535.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629812  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/wpa_supplicant.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629815  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/wpa_supplicant.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629818  199956 factory.go:255] Factory "raw" can handle container "/system.slice/wpa_supplicant.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629822  199956 manager.go:925] ignoring container "/system.slice/wpa_supplicant.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629824  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-shm.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629828  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-shm.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629833  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-shm.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629837  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629841  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-rootfs.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629846  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629850  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-186424b47c7b9022ecfe8996dc73cd9101f7539259cd65914279c84c9a02a288-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629854  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-186424b47c7b9022ecfe8996dc73cd9101f7539259cd65914279c84c9a02a288-rootfs.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629859  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-186424b47c7b9022ecfe8996dc73cd9101f7539259cd65914279c84c9a02a288-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629862  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snapd.apparmor.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629865  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/snapd.apparmor.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629868  199956 factory.go:255] Factory "raw" can handle container "/system.slice/snapd.apparmor.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629872  199956 manager.go:925] ignoring container "/system.slice/snapd.apparmor.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629875  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/docker.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629878  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/docker.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629881  199956 factory.go:255] Factory "raw" can handle container "/system.slice/docker.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629885  199956 manager.go:925] ignoring container "/system.slice/docker.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629888  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/lvm2-lvmpolld.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629891  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/lvm2-lvmpolld.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629894  199956 factory.go:255] Factory "raw" can handle container "/system.slice/lvm2-lvmpolld.socket", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629898  199956 manager.go:925] ignoring container "/system.slice/lvm2-lvmpolld.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629901  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-initctl.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629903  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-initctl.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629906  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-initctl.socket", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629910  199956 manager.go:925] ignoring container "/system.slice/systemd-initctl.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629913  199956 factory.go:262] Factory "containerd" was unable to handle container "/docker"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629916  199956 factory.go:262] Factory "systemd" was unable to handle container "/docker"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629919  199956 factory.go:255] Factory "raw" can handle container "/docker", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629923  199956 manager.go:925] ignoring container "/docker"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629925  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/blk-availability.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629928  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/blk-availability.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629931  199956 factory.go:255] Factory "raw" can handle container "/system.slice/blk-availability.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629935  199956 manager.go:925] ignoring container "/system.slice/blk-availability.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629938  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b3af3b285c950aba4fcd0176473261f7f4700aeaafe9c73ae15b15a7d6304092-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629942  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b3af3b285c950aba4fcd0176473261f7f4700aeaafe9c73ae15b15a7d6304092-rootfs.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629947  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b3af3b285c950aba4fcd0176473261f7f4700aeaafe9c73ae15b15a7d6304092-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629950  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/upower.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629953  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/upower.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629961  199956 factory.go:255] Factory "raw" can handle container "/system.slice/upower.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629965  199956 manager.go:925] ignoring container "/system.slice/upower.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629968  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-lvm2\\x2dpvscan.slice"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629970  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-lvm2\\x2dpvscan.slice"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629974  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-lvm2\\x2dpvscan.slice", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629978  199956 manager.go:925] ignoring container "/system.slice/system-lvm2\\x2dpvscan.slice"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629981  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e489181a7f95431b6d92f037dbe3f788b46a5e024df7aaf29e0115dab1168ab1-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629985  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e489181a7f95431b6d92f037dbe3f788b46a5e024df7aaf29e0115dab1168ab1-rootfs.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629989  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e489181a7f95431b6d92f037dbe3f788b46a5e024df7aaf29e0115dab1168ab1-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629993  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/syslog.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629996  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/syslog.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.629999  199956 factory.go:255] Factory "raw" can handle container "/system.slice/syslog.socket", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630003  199956 manager.go:925] ignoring container "/system.slice/syslog.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630006  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/colord.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630008  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/colord.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630012  199956 factory.go:255] Factory "raw" can handle container "/system.slice/colord.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630015  199956 manager.go:925] ignoring container "/system.slice/colord.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630018  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-shm.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630022  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-shm.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630027  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-shm.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630031  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/openvpn.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630034  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/openvpn.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630037  199956 factory.go:255] Factory "raw" can handle container "/system.slice/openvpn.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630041  199956 manager.go:925] ignoring container "/system.slice/openvpn.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630044  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-resolved.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630046  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-resolved.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630049  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-resolved.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630053  199956 manager.go:925] ignoring container "/system.slice/systemd-resolved.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630056  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-bare-5.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630059  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-bare-5.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630063  199956 manager.go:925] ignoring container "/system.slice/snap-bare-5.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630066  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-fsckd.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630070  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-fsckd.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630073  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-fsckd.socket", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630077  199956 manager.go:925] ignoring container "/system.slice/systemd-fsckd.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630080  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2d719b045b\\x2df019\\x2d025c\\x2d185d\\x2da976163f375a.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630083  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2d719b045b\\x2df019\\x2d025c\\x2d185d\\x2da976163f375a.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630088  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2d719b045b\\x2df019\\x2d025c\\x2d185d\\x2da976163f375a.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630091  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-tmpfiles-setup.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630094  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-tmpfiles-setup.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630097  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-tmpfiles-setup.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630101  199956 manager.go:925] ignoring container "/system.slice/systemd-tmpfiles-setup.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630104  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-d94e1927\\x2dd22d\\x2d4831\\x2da764\\x2d0170eae346a1-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d9qh7j.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630108  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-d94e1927\\x2dd22d\\x2d4831\\x2da764\\x2d0170eae346a1-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d9qh7j.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630114  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-d94e1927\\x2dd22d\\x2d4831\\x2da764\\x2d0170eae346a1-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d9qh7j.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630119  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/apparmor.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630121  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/apparmor.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630125  199956 factory.go:255] Factory "raw" can handle container "/system.slice/apparmor.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630129  199956 manager.go:925] ignoring container "/system.slice/apparmor.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630132  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/accounts-daemon.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630134  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/accounts-daemon.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630137  199956 factory.go:255] Factory "raw" can handle container "/system.slice/accounts-daemon.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630141  199956 manager.go:925] ignoring container "/system.slice/accounts-daemon.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630145  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-cf25b9ea\\x2d6823\\x2d4eff\\x2db30d\\x2d36a09f9d897e-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dtqzsm.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630149  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-cf25b9ea\\x2d6823\\x2d4eff\\x2db30d\\x2d36a09f9d897e-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dtqzsm.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630154  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-cf25b9ea\\x2d6823\\x2d4eff\\x2db30d\\x2d36a09f9d897e-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dtqzsm.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630158  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/cron.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630161  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/cron.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630164  199956 factory.go:255] Factory "raw" can handle container "/system.slice/cron.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630168  199956 manager.go:925] ignoring container "/system.slice/cron.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630171  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/polkit.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630173  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/polkit.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630176  199956 factory.go:255] Factory "raw" can handle container "/system.slice/polkit.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630180  199956 manager.go:925] ignoring container "/system.slice/polkit.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630184  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-snapd-16292.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630187  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-snapd-16292.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630192  199956 manager.go:925] ignoring container "/system.slice/snap-snapd-16292.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630194  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/whoopsie.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630197  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/whoopsie.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630200  199956 factory.go:255] Factory "raw" can handle container "/system.slice/whoopsie.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630204  199956 manager.go:925] ignoring container "/system.slice/whoopsie.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630207  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/unattended-upgrades.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630210  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/unattended-upgrades.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630213  199956 factory.go:255] Factory "raw" can handle container "/system.slice/unattended-upgrades.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630217  199956 manager.go:925] ignoring container "/system.slice/unattended-upgrades.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630220  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-13149330f3752d0ff65bcac86c7b522b2040811e815fbc87e56b40b612875d5a-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630224  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-13149330f3752d0ff65bcac86c7b522b2040811e815fbc87e56b40b612875d5a-rootfs.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630229  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-13149330f3752d0ff65bcac86c7b522b2040811e815fbc87e56b40b612875d5a-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630232  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-sysctl.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630235  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-sysctl.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630238  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-sysctl.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630242  199956 manager.go:925] ignoring container "/system.slice/systemd-sysctl.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630245  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/acpid.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630248  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/acpid.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630251  199956 factory.go:255] Factory "raw" can handle container "/system.slice/acpid.socket", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630255  199956 manager.go:925] ignoring container "/system.slice/acpid.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630258  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630262  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-rootfs.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630266  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630270  199956 factory.go:262] Factory "containerd" was unable to handle container "/kata_overhead"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630273  199956 factory.go:262] Factory "systemd" was unable to handle container "/kata_overhead"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630275  199956 factory.go:255] Factory "raw" can handle container "/kata_overhead", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630279  199956 manager.go:925] ignoring container "/kata_overhead"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630282  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/-.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630285  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/-.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630289  199956 manager.go:925] ignoring container "/system.slice/-.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630293  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630297  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-rootfs.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630302  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630305  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dbus.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630308  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/dbus.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630311  199956 factory.go:255] Factory "raw" can handle container "/system.slice/dbus.socket", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630315  199956 manager.go:925] ignoring container "/system.slice/dbus.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630317  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630320  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630323  199956 factory.go:255] Factory "raw" can handle container "/user.slice", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630327  199956 manager.go:925] ignoring container "/user.slice"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630329  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-modprobe.slice"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630332  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-modprobe.slice"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630335  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-modprobe.slice", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630339  199956 manager.go:925] ignoring container "/system.slice/system-modprobe.slice"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630342  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dev-mqueue.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630345  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/dev-mqueue.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630349  199956 manager.go:925] ignoring container "/system.slice/dev-mqueue.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630352  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-update-utmp.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630355  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-update-utmp.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630358  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-update-utmp.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630362  199956 manager.go:925] ignoring container "/system.slice/systemd-update-utmp.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630364  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-systemd\\x2dfsck.slice"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630367  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-systemd\\x2dfsck.slice"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630370  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-systemd\\x2dfsck.slice", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630374  199956 manager.go:925] ignoring container "/system.slice/system-systemd\\x2dfsck.slice"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630377  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/uuidd.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630380  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/uuidd.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630383  199956 factory.go:255] Factory "raw" can handle container "/system.slice/uuidd.socket", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630387  199956 manager.go:925] ignoring container "/system.slice/uuidd.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630389  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/containerd.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630392  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/containerd.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630396  199956 factory.go:255] Factory "raw" can handle container "/system.slice/containerd.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630400  199956 manager.go:925] ignoring container "/system.slice/containerd.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630403  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630407  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-rootfs.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630412  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630415  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/NetworkManager.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630418  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/NetworkManager.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630421  199956 factory.go:255] Factory "raw" can handle container "/system.slice/NetworkManager.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630425  199956 manager.go:925] ignoring container "/system.slice/NetworkManager.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630428  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/switcheroo-control.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630431  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/switcheroo-control.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630434  199956 factory.go:255] Factory "raw" can handle container "/system.slice/switcheroo-control.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630438  199956 manager.go:925] ignoring container "/system.slice/switcheroo-control.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630441  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-shm.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630445  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-shm.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630449  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-shm.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630453  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/user@0.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630456  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/user@0.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630460  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/user@0.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630464  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/user@0.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630466  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-lvm2\\x2dpvscan.slice/lvm2-pvscan@8:10.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630469  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-lvm2\\x2dpvscan.slice/lvm2-pvscan@8:10.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630473  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-lvm2\\x2dpvscan.slice/lvm2-pvscan@8:10.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630477  199956 manager.go:925] ignoring container "/system.slice/system-lvm2\\x2dpvscan.slice/lvm2-pvscan@8:10.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630480  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2d562f9e70\\x2db2b7\\x2d3ac4\\x2d2311\\x2db91510a5a9ac.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630484  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2d562f9e70\\x2db2b7\\x2d3ac4\\x2d2311\\x2db91510a5a9ac.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630488  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2d562f9e70\\x2db2b7\\x2d3ac4\\x2d2311\\x2db91510a5a9ac.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630491  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630495  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-rootfs.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630500  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630504  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-machine-id-commit.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630506  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-machine-id-commit.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630511  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-machine-id-commit.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630515  199956 manager.go:925] ignoring container "/system.slice/systemd-machine-id-commit.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630518  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/user-runtime-dir@0.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630520  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/user-runtime-dir@0.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630524  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/user-runtime-dir@0.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630528  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/user-runtime-dir@0.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630531  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2d8b7da23a\\x2d3511\\x2dfe06\\x2d72d7\\x2d26a549eb607d.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630534  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2d8b7da23a\\x2d3511\\x2dfe06\\x2d72d7\\x2d26a549eb607d.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630539  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2d8b7da23a\\x2d3511\\x2dfe06\\x2d72d7\\x2d26a549eb607d.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630542  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-b0713fbc\\x2defc5\\x2d4044\\x2d9d08\\x2d2326a0752f87-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dgcgm6.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630547  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-b0713fbc\\x2defc5\\x2d4044\\x2d9d08\\x2d2326a0752f87-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dgcgm6.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630552  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-b0713fbc\\x2defc5\\x2d4044\\x2d9d08\\x2d2326a0752f87-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dgcgm6.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630556  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/irqbalance.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630561  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/irqbalance.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630564  199956 factory.go:255] Factory "raw" can handle container "/system.slice/irqbalance.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630569  199956 manager.go:925] ignoring container "/system.slice/irqbalance.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630572  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/console-setup.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630574  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/console-setup.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630577  199956 factory.go:255] Factory "raw" can handle container "/system.slice/console-setup.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630581  199956 manager.go:925] ignoring container "/system.slice/console-setup.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630584  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-tmpfiles-setup-dev.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630587  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-tmpfiles-setup-dev.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630590  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-tmpfiles-setup-dev.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630594  199956 manager.go:925] ignoring container "/system.slice/systemd-tmpfiles-setup-dev.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630597  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/alsa-restore.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630599  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/alsa-restore.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630602  199956 factory.go:255] Factory "raw" can handle container "/system.slice/alsa-restore.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630606  199956 manager.go:925] ignoring container "/system.slice/alsa-restore.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630609  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-shm.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630613  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-shm.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630618  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-shm.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630622  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-1000.slice/session-1.scope"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630626  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-1000.slice/session-1.scope"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630629  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-1000.slice/session-1.scope", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630633  199956 manager.go:925] ignoring container "/user.slice/user-1000.slice/session-1.scope"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630636  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-shm.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630640  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-shm.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630645  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-shm.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630648  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/cups-browsed.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630651  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/cups-browsed.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630654  199956 factory.go:255] Factory "raw" can handle container "/system.slice/cups-browsed.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630658  199956 manager.go:925] ignoring container "/system.slice/cups-browsed.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630661  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/docker.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630663  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/docker.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630667  199956 factory.go:255] Factory "raw" can handle container "/system.slice/docker.socket", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630670  199956 manager.go:925] ignoring container "/system.slice/docker.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630673  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/cups.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630676  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/cups.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630679  199956 factory.go:255] Factory "raw" can handle container "/system.slice/cups.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630683  199956 manager.go:925] ignoring container "/system.slice/cups.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630686  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journald-audit.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630690  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journald-audit.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630693  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journald-audit.socket", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630697  199956 manager.go:925] ignoring container "/system.slice/systemd-journald-audit.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630700  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/acpid.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630702  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/acpid.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630705  199956 factory.go:255] Factory "raw" can handle container "/system.slice/acpid.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630709  199956 manager.go:925] ignoring container "/system.slice/acpid.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630712  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/NetworkManager-wait-online.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630715  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/NetworkManager-wait-online.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630718  199956 factory.go:255] Factory "raw" can handle container "/system.slice/NetworkManager-wait-online.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630722  199956 manager.go:925] ignoring container "/system.slice/NetworkManager-wait-online.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630725  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-snapd-ns-snap\\x2dstore.mnt.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630728  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-snapd-ns-snap\\x2dstore.mnt.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630733  199956 manager.go:925] ignoring container "/system.slice/run-snapd-ns-snap\\x2dstore.mnt.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630736  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-core20-1611.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630739  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-core20-1611.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630743  199956 manager.go:925] ignoring container "/system.slice/snap-core20-1611.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630746  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630749  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630752  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630755  199956 manager.go:925] ignoring container "/user.slice/user-0.slice"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630758  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-shm.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630762  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-shm.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630767  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-shm.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630771  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630775  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-rootfs.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630780  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630784  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630788  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-rootfs.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630793  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630796  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-snap\\x2dstore-558.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630800  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-snap\\x2dstore-558.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630804  199956 manager.go:925] ignoring container "/system.slice/snap-snap\\x2dstore-558.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630806  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journald.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630810  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journald.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630813  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journald.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630817  199956 manager.go:925] ignoring container "/system.slice/systemd-journald.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630819  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snapd.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630822  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/snapd.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630825  199956 factory.go:255] Factory "raw" can handle container "/system.slice/snapd.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630829  199956 manager.go:925] ignoring container "/system.slice/snapd.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630833  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/rsyslog.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630836  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/rsyslog.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630839  199956 factory.go:255] Factory "raw" can handle container "/system.slice/rsyslog.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630842  199956 manager.go:925] ignoring container "/system.slice/rsyslog.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630846  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-shm.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630850  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-shm.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630855  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-shm.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630858  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-user-sessions.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630861  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-user-sessions.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630865  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-user-sessions.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630869  199956 manager.go:925] ignoring container "/system.slice/systemd-user-sessions.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630872  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dm-event.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630874  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/dm-event.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630877  199956 factory.go:255] Factory "raw" can handle container "/system.slice/dm-event.socket", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630881  199956 manager.go:925] ignoring container "/system.slice/dm-event.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630884  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/avahi-daemon.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630887  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/avahi-daemon.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630890  199956 factory.go:255] Factory "raw" can handle container "/system.slice/avahi-daemon.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630894  199956 manager.go:925] ignoring container "/system.slice/avahi-daemon.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630896  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/networkd-dispatcher.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630899  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/networkd-dispatcher.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630902  199956 factory.go:255] Factory "raw" can handle container "/system.slice/networkd-dispatcher.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630906  199956 manager.go:925] ignoring container "/system.slice/networkd-dispatcher.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630909  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/udisks2.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630912  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/udisks2.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630915  199956 factory.go:255] Factory "raw" can handle container "/system.slice/udisks2.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630919  199956 manager.go:925] ignoring container "/system.slice/udisks2.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630921  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-1000.slice/user@1000.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630924  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-1000.slice/user@1000.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630927  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-1000.slice/user@1000.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630932  199956 manager.go:925] ignoring container "/user.slice/user-1000.slice/user@1000.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630934  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/rtkit-daemon.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630937  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/rtkit-daemon.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630940  199956 factory.go:255] Factory "raw" can handle container "/system.slice/rtkit-daemon.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630944  199956 manager.go:925] ignoring container "/system.slice/rtkit-daemon.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630948  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dbus.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630952  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/dbus.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630955  199956 factory.go:255] Factory "raw" can handle container "/system.slice/dbus.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630959  199956 manager.go:925] ignoring container "/system.slice/dbus.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630962  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/kerneloops.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630964  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/kerneloops.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630967  199956 factory.go:255] Factory "raw" can handle container "/system.slice/kerneloops.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630971  199956 manager.go:925] ignoring container "/system.slice/kerneloops.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630974  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630978  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-rootfs.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630983  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630986  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-getty.slice"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630989  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-getty.slice"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630992  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-getty.slice", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630996  199956 manager.go:925] ignoring container "/system.slice/system-getty.slice"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.630999  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/bluetooth.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631002  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/bluetooth.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631005  199956 factory.go:255] Factory "raw" can handle container "/system.slice/bluetooth.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631009  199956 manager.go:925] ignoring container "/system.slice/bluetooth.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631011  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-shm.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631015  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-shm.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631020  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-shm.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631024  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/uuidd.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631026  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/uuidd.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631029  199956 factory.go:255] Factory "raw" can handle container "/system.slice/uuidd.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631033  199956 manager.go:925] ignoring container "/system.slice/uuidd.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631036  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-user-1000.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631039  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-user-1000.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631043  199956 manager.go:925] ignoring container "/system.slice/run-user-1000.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631046  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journald-dev-log.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631049  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journald-dev-log.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631052  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journald-dev-log.socket", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631056  199956 manager.go:925] ignoring container "/system.slice/systemd-journald-dev-log.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631060  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-shm.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631064  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-shm.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631068  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-shm.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631072  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-sysusers.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631075  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-sysusers.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631078  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-sysusers.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631082  199956 manager.go:925] ignoring container "/system.slice/systemd-sysusers.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631085  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-udevd-control.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631089  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-udevd-control.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631092  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-udevd-control.socket", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631096  199956 manager.go:925] ignoring container "/system.slice/systemd-udevd-control.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631099  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-rfkill.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631101  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-rfkill.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631105  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-rfkill.socket", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631109  199956 manager.go:925] ignoring container "/system.slice/systemd-rfkill.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631111  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/setvtrgb.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631114  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/setvtrgb.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631117  199956 factory.go:255] Factory "raw" can handle container "/system.slice/setvtrgb.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631121  199956 manager.go:925] ignoring container "/system.slice/setvtrgb.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631124  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-shm.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631128  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-shm.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631133  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-shm.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631136  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/gdm.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631139  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/gdm.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631142  199956 factory.go:255] Factory "raw" can handle container "/system.slice/gdm.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631145  199956 manager.go:925] ignoring container "/system.slice/gdm.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631148  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/lvm2-monitor.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631151  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/lvm2-monitor.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631154  199956 factory.go:255] Factory "raw" can handle container "/system.slice/lvm2-monitor.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631158  199956 manager.go:925] ignoring container "/system.slice/lvm2-monitor.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631161  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-user-1000-gvfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631164  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-user-1000-gvfs.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631169  199956 manager.go:925] ignoring container "/system.slice/run-user-1000-gvfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631172  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snapd.seeded.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631175  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/snapd.seeded.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631178  199956 factory.go:255] Factory "raw" can handle container "/system.slice/snapd.seeded.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631182  199956 manager.go:925] ignoring container "/system.slice/snapd.seeded.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631185  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/session-44.scope"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631187  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/session-44.scope"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631190  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/session-44.scope", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631194  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/session-44.scope"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631197  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-1000.slice/user-runtime-dir@1000.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631200  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-1000.slice/user-runtime-dir@1000.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631205  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-1000.slice/user-runtime-dir@1000.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631209  199956 manager.go:925] ignoring container "/user.slice/user-1000.slice/user-runtime-dir@1000.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631212  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2d58469e7a\\x2dc331\\x2d785c\\x2dc760\\x2d93c7232334bd.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631216  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2d58469e7a\\x2dc331\\x2d785c\\x2dc760\\x2d93c7232334bd.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631220  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2d58469e7a\\x2dc331\\x2d785c\\x2dc760\\x2d93c7232334bd.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631223  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-1cb44bdbde0b41852e382d7dab7c184af8dd4e5b25d5cfe6a10a9c8ab601659d-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631227  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-1cb44bdbde0b41852e382d7dab7c184af8dd4e5b25d5cfe6a10a9c8ab601659d-rootfs.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631232  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-1cb44bdbde0b41852e382d7dab7c184af8dd4e5b25d5cfe6a10a9c8ab601659d-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631236  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journald.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631238  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journald.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631241  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journald.socket", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631245  199956 manager.go:925] ignoring container "/system.slice/systemd-journald.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631248  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-9b11acac1b891eafc7ff2b244f1c42938f71a575ce81ff917e3b7ebf61d2e045-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631252  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-9b11acac1b891eafc7ff2b244f1c42938f71a575ce81ff917e3b7ebf61d2e045-rootfs.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631257  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-9b11acac1b891eafc7ff2b244f1c42938f71a575ce81ff917e3b7ebf61d2e045-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631260  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-1c057a0e\\x2d3ee3\\x2d44f3\\x2db294\\x2d434acc8f9491-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d2sbqp.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631264  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-1c057a0e\\x2d3ee3\\x2d44f3\\x2db294\\x2d434acc8f9491-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d2sbqp.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631270  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-1c057a0e\\x2d3ee3\\x2d44f3\\x2db294\\x2d434acc8f9491-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d2sbqp.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631274  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-kernel-debug.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631278  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-kernel-debug.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631282  199956 manager.go:925] ignoring container "/system.slice/sys-kernel-debug.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631285  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/ssh.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631288  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/ssh.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631291  199956 factory.go:255] Factory "raw" can handle container "/system.slice/ssh.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631295  199956 manager.go:925] ignoring container "/system.slice/ssh.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631297  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631302  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-rootfs.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631307  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631311  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/user@0.service/dbus.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631314  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/user@0.service/dbus.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631317  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/user@0.service/dbus.socket", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631321  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/user@0.service/dbus.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631324  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-snapd-ns.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631327  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-snapd-ns.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631331  199956 manager.go:925] ignoring container "/system.slice/run-snapd-ns.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631334  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-modules-load.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631337  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-modules-load.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631340  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-modules-load.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631344  199956 manager.go:925] ignoring container "/system.slice/systemd-modules-load.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631347  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e0c5b991f81d5128566686552e45a9bbc8c584e4f6c94909edb3a42720dacc90-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631351  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e0c5b991f81d5128566686552e45a9bbc8c584e4f6c94909edb3a42720dacc90-rootfs.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631356  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e0c5b991f81d5128566686552e45a9bbc8c584e4f6c94909edb3a42720dacc90-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631360  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/session-42.scope"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631363  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/session-42.scope"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631366  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/session-42.scope", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631370  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/session-42.scope"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631373  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journal-flush.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631375  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journal-flush.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631380  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journal-flush.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631384  199956 manager.go:925] ignoring container "/system.slice/systemd-journal-flush.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631387  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-udevd-kernel.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631389  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-udevd-kernel.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631392  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-udevd-kernel.socket", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631397  199956 manager.go:925] ignoring container "/system.slice/systemd-udevd-kernel.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631400  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-gnome\\x2d3\\x2d38\\x2d2004-115.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631403  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-gnome\\x2d3\\x2d38\\x2d2004-115.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631407  199956 manager.go:925] ignoring container "/system.slice/snap-gnome\\x2d3\\x2d38\\x2d2004-115.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631410  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-udev-trigger.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631413  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-udev-trigger.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631416  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-udev-trigger.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631420  199956 manager.go:925] ignoring container "/system.slice/systemd-udev-trigger.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631423  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snapd.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631426  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/snapd.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631429  199956 factory.go:255] Factory "raw" can handle container "/system.slice/snapd.socket", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631432  199956 manager.go:925] ignoring container "/system.slice/snapd.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631435  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-user-0.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631438  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-user-0.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631442  199956 manager.go:925] ignoring container "/system.slice/run-user-0.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631445  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/user@0.service/dbus.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631448  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/user@0.service/dbus.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631451  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/user@0.service/dbus.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631455  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/user@0.service/dbus.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631458  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-5900ac5c1383a321a6ae837b57d6d29d7a660a102c0806210a9ea57290673062-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631462  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-5900ac5c1383a321a6ae837b57d6d29d7a660a102c0806210a9ea57290673062-rootfs.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631467  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-5900ac5c1383a321a6ae837b57d6d29d7a660a102c0806210a9ea57290673062-rootfs.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631470  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/avahi-daemon.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631473  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/avahi-daemon.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631476  199956 factory.go:255] Factory "raw" can handle container "/system.slice/avahi-daemon.socket", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631480  199956 manager.go:925] ignoring container "/system.slice/avahi-daemon.socket"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631483  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/apport.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631486  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/apport.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631489  199956 factory.go:255] Factory "raw" can handle container "/system.slice/apport.service", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631493  199956 manager.go:925] ignoring container "/system.slice/apport.service"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631497  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dev-hugepages.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631500  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/dev-hugepages.mount", but ignoring.
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.631504  199956 manager.go:925] ignoring container "/system.slice/dev-hugepages.mount"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.645426  199956 qos_container_manager_linux.go:379] "Updated QoS cgroup configuration"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.663266  199956 kuberuntime_gc.go:171] "Removing sandbox" sandboxID="38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:43:27.663376630-05:00" level=info msg="StopPodSandbox for \"38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603\""
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:43:27.666939049-05:00" level=info msg="TearDown network for sandbox \"38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603\" successfully"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:43:27.666965379-05:00" level=info msg="StopPodSandbox for \"38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603\" returns successfully"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:43:27.667196098-05:00" level=info msg="RemovePodSandbox for \"38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603\""
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:43:27.667228041-05:00" level=info msg="Forcibly stopping sandbox \"38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603\""
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:43:27.670730239-05:00" level=info msg="TearDown network for sandbox \"38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603\" successfully"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:43:27.673908004-05:00" level=info msg="RemovePodSandbox \"38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603\" returns successfully"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.674430  199956 kubelet.go:1280] "Container garbage collection succeeded"
Jan 20 12:43:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:27.830871  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:43:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:28.483222  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:28.483285  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:28.496499  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[6d09123d-b06a-42a2-a5cd-7d638e413ac4] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:28 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0009a1d20 2 [] false false map[] 0xc001f36800 0xc0022edef0}
Jan 20 12:43:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:28.496630  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:28.598515  199956 generic.go:155] "GenericPLEG" podUID=4f97314d-815b-4787-b174-5c158cd28c9d containerID="38a7014edc5f74af8dcb68f2722714191ba45cbb88d63276f62c7d6a23613603" oldState=exited newState=non-existent
Jan 20 12:43:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:28.599357  199956 kuberuntime_manager.go:1037] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615] pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:28.600793  199956 generic.go:421] "PLEG: Write status" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:28.879246  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:43:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:28.879313  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:28.880436  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:28 GMT] X-Content-Type-Options:[nosniff]] 0xc0004c1ca0 2 [] true false map[] 0xc001a4b900 <nil>}
Jan 20 12:43:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:28.880552  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:43:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:29.483819  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:29.483888  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:29.491843  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[065208dd-31f0-470f-84de-55d102d3279d] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:29 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0004c1cc0 2 [] false false map[] 0xc0013cef00 0xc0019eadc0}
Jan 20 12:43:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:29.491875  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:29.581638  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:43:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:29.587420  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:43:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:29.587433  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:43:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:29.588170  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:43:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:29.588597  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:43:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:29.588645  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:43:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:29.588651  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:43:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:29.588660  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:43:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:30.108261  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:43:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:30.116417  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:43:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:30.126420  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59521688Ki" capacity="65586124Ki" time="2023-01-20 12:43:30.110376601 -0500 EST m=+722.666151493"
Jan 20 12:43:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:30.126433  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64393368Ki" capacity="65061836Ki" time="2023-01-20 12:43:30.126352914 -0500 EST m=+722.682127739"
Jan 20 12:43:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:30.126440  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902003116Ki" capacity="981310056Ki" time="2023-01-20 12:43:30.110376601 -0500 EST m=+722.666151493"
Jan 20 12:43:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:30.126447  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:43:30.110376601 -0500 EST m=+722.666151493"
Jan 20 12:43:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:30.126453  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902003116Ki" capacity="981310056Ki" time="2023-01-20 12:43:21.261937455 -0500 EST"
Jan 20 12:43:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:30.126458  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:43:21.261937455 -0500 EST"
Jan 20 12:43:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:30.126464  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510774" capacity="511757" time="2023-01-20 12:43:30.126091942 -0500 EST m=+722.681866767"
Jan 20 12:43:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:30.126489  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:43:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:30.483740  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:30.483811  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:30.491572  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[e1c5fec8-8fee-47f5-b06e-6b1c9d32f22a] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:30 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001486620 2 [] false false map[] 0xc001c48700 0xc001c6f3f0}
Jan 20 12:43:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:30.491602  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:31.166929  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:43:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:31.167006  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:31.174875  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[fa2f0297-df8c-4c7a-86c7-367810f807cc] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:31 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00013cc00 2 [] false false map[] 0xc001c48a00 0xc001f076b0}
Jan 20 12:43:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:31.174929  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:31.483503  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:31.483570  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:31.496596  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[b7b4e996-2d66-4edd-8eda-52c892bc7370] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:31 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001486ec0 2 [] false false map[] 0xc001c48c00 0xc0020acd10}
Jan 20 12:43:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:31.496754  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:31.581493  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:43:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:31.588584  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:43:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:31.588602  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:43:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:31.589344  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:43:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:31.589740  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:43:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:31.589788  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:43:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:31.589795  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:43:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:31.589804  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.484114  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.484184  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.497233  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[7b9c1436-3e38-4c92-ad25-42efba442917] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:32 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001487ae0 2 [] false false map[] 0xc001f36100 0xc0013b02c0}
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.497376  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.559774  199956 handler.go:293] error while reading "/proc/199956/fd/34" link: readlink /proc/199956/fd/34: no such file or directory
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.581251  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[kube-flannel/kube-flannel-ds-hprn4]
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.581332  199956 pod_workers.go:888] "Processing pod event" pod="kube-flannel/kube-flannel-ds-hprn4" podUID=04e472cb-b6f9-4065-b509-d7e1579fc48d updateType=0
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.581389  199956 kubelet.go:1501] "syncPod enter" pod="kube-flannel/kube-flannel-ds-hprn4" podUID=04e472cb-b6f9-4065-b509-d7e1579fc48d
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.581418  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.581524  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-flannel/kube-flannel-ds-hprn4" oldPhase=Running phase=Running
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.581905  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-flannel/kube-flannel-ds-hprn4" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:44 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:45 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:45 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.239.159.53 PodIPs:[{IP:10.239.159.53}] StartTime:2023-01-20 12:31:42 -0500 EST InitContainerStatuses:[{Name:install-cni-plugin State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2023-01-20 12:31:43 -0500 EST,FinishedAt:2023-01-20 12:31:43 -0500 EST,ContainerID:containerd://349eee1bf5a2d95206979822c956ea6f555dbb786434c7b9fe46524872f1a18d,}} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:docker.io/rancher/mirrored-flannelcni-flannel-cni-plugin:v1.1.0 ImageID:docker.io/rancher/mirrored-flannelcni-flannel-cni-plugin@sha256:28d3a6be9f450282bf42e4dad143d41da23e3d91f66f19c01ee7fd21fd17cb2b ContainerID:containerd://349eee1bf5a2d95206979822c956ea6f555dbb786434c7b9fe46524872f1a18d Started:<nil>} {Name:install-cni State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2023-01-20 12:31:43 -0500 EST,FinishedAt:2023-01-20 12:31:43 -0500 EST,ContainerID:containerd://85a4ede91a47b8d7df192dc39004a2a7fa45e095191a7335eaed8ec826cd9f36,}} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:docker.io/rancher/mirrored-flannelcni-flannel:v0.19.1 ImageID:docker.io/rancher/mirrored-flannelcni-flannel@sha256:e3b0f06121b0f7a11a684e910bba89b3ab49cd6e73aeb6c719f83b2456946366 ContainerID:containerd://85a4ede91a47b8d7df192dc39004a2a7fa45e095191a7335eaed8ec826cd9f36 Started:<nil>}] ContainerStatuses:[{Name:kube-flannel State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:44 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:docker.io/rancher/mirrored-flannelcni-flannel:v0.19.1 ImageID:docker.io/rancher/mirrored-flannelcni-flannel@sha256:e3b0f06121b0f7a11a684e910bba89b3ab49cd6e73aeb6c719f83b2456946366 ContainerID:containerd://0f9ae677fe935afcf43e145281deae0d663ba95fda6759ff24f0dd3e1cee17a9 Started:0xc001d74e4e}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.582238  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.582312  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.582744  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.582906  199956 kubelet.go:1503] "syncPod exit" pod="kube-flannel/kube-flannel-ds-hprn4" podUID=04e472cb-b6f9-4065-b509-d7e1579fc48d isTerminal=false
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.582957  199956 pod_workers.go:988] "Processing pod event done" pod="kube-flannel/kube-flannel-ds-hprn4" podUID=04e472cb-b6f9-4065-b509-d7e1579fc48d updateType=0
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.645693  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="run" volumeSpecName="run"
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.645804  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="cni-plugin" volumeSpecName="cni-plugin"
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.645879  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="cni" volumeSpecName="cni"
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.645956  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="flannel-cfg" volumeSpecName="flannel-cfg"
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.646023  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="xtables-lock" volumeSpecName="xtables-lock"
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.646137  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="kube-api-access-hqj8d" volumeSpecName="kube-api-access-hqj8d"
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.654918  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-hqj8d\" (UniqueName: \"kubernetes.io/projected/04e472cb-b6f9-4065-b509-d7e1579fc48d-kube-api-access-hqj8d\") pod \"kube-flannel-ds-hprn4\" (UID: \"04e472cb-b6f9-4065-b509-d7e1579fc48d\") Volume is already mounted to pod, but remount was requested." pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.655053  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"flannel-cfg\" (UniqueName: \"kubernetes.io/configmap/04e472cb-b6f9-4065-b509-d7e1579fc48d-flannel-cfg\") pod \"kube-flannel-ds-hprn4\" (UID: \"04e472cb-b6f9-4065-b509-d7e1579fc48d\") Volume is already mounted to pod, but remount was requested." pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.655190  199956 configmap.go:181] Setting up volume flannel-cfg for pod 04e472cb-b6f9-4065-b509-d7e1579fc48d at /var/lib/kubelet/pods/04e472cb-b6f9-4065-b509-d7e1579fc48d/volumes/kubernetes.io~configmap/flannel-cfg
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.655192  199956 projected.go:183] Setting up volume kube-api-access-hqj8d for pod 04e472cb-b6f9-4065-b509-d7e1579fc48d at /var/lib/kubelet/pods/04e472cb-b6f9-4065-b509-d7e1579fc48d/volumes/kubernetes.io~projected/kube-api-access-hqj8d
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.655246  199956 configmap.go:205] Received configMap kube-flannel/kube-flannel-cfg containing (2) pieces of data, 365 total bytes
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.655318  199956 quota_linux.go:271] SupportsQuotas called, but quotas disabled
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.655590  199956 atomic_writer.go:161] pod kube-flannel/kube-flannel-ds-hprn4 volume flannel-cfg: no update required for target directory /var/lib/kubelet/pods/04e472cb-b6f9-4065-b509-d7e1579fc48d/volumes/kubernetes.io~configmap/flannel-cfg
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.655651  199956 atomic_writer.go:161] pod kube-flannel/kube-flannel-ds-hprn4 volume kube-api-access-hqj8d: no update required for target directory /var/lib/kubelet/pods/04e472cb-b6f9-4065-b509-d7e1579fc48d/volumes/kubernetes.io~projected/kube-api-access-hqj8d
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.655652  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"flannel-cfg\" (UniqueName: \"kubernetes.io/configmap/04e472cb-b6f9-4065-b509-d7e1579fc48d-flannel-cfg\") pod \"kube-flannel-ds-hprn4\" (UID: \"04e472cb-b6f9-4065-b509-d7e1579fc48d\") " pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.655725  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-hqj8d\" (UniqueName: \"kubernetes.io/projected/04e472cb-b6f9-4065-b509-d7e1579fc48d-kube-api-access-hqj8d\") pod \"kube-flannel-ds-hprn4\" (UID: \"04e472cb-b6f9-4065-b509-d7e1579fc48d\") " pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.660956  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.660963  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.661144  199956 interface.go:209] Interface eno2 is up
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.661172  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.661180  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.661184  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.661188  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.661191  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.661431  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.661441  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.661445  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.661448  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.689809  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.689871  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.690879  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:32 GMT]] 0xc0012e8a40 2 [] true false map[] 0xc001f37200 <nil>}
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.690983  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.696210  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.696270  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.697401  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:32 GMT]] 0xc0012e8aa0 2 [] true false map[] 0xc001f37400 <nil>}
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.697505  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.709633  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.709693  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.709707  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.709768  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.710705  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:32 GMT]] 0xc0021a6720 2 [] true false map[] 0xc0021a2f00 <nil>}
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.710775  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:32 GMT]] 0xc0012e8b40 2 [] true false map[] 0xc001f37600 <nil>}
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.710820  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.710881  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:43:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:32.832279  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:43:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:33.483888  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:33.483959  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:33.497040  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[6e7ffcfa-704a-4fa5-b1e7-2f92086aa9ad] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:33 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000320900 2 [] false false map[] 0xc000fcc100 0xc0021440b0}
Jan 20 12:43:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:33.497176  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:33.566854  199956 reflector.go:536] vendor/k8s.io/client-go/informers/factory.go:134: Watch close - *v1.Service total 0 items received
Jan 20 12:43:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:33.582173  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:43:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:33.588354  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:43:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:33.588367  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:43:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:33.588374  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:43:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:33.589009  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:43:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:33.589055  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:43:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:33.589061  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:43:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:33.589069  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:43:34 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:34.484008  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:34 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:34.484082  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:34 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:34.491610  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[f5b18c0d-6c79-4ea1-bbd1-5c0a0293c7ad] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:34 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001703ec0 2 [] false false map[] 0xc0011c4600 0xc00133c630}
Jan 20 12:43:34 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:34.491641  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:35.483146  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:35.483217  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:35.491722  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[48034f7d-a921-445d-ad83-c7d6f651f3cf] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:35 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000321580 2 [] false false map[] 0xc000fcc700 0xc0017c8160}
Jan 20 12:43:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:35.491752  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:35.581646  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:43:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:35.588934  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:43:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:35.588996  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:43:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:35.591279  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:43:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:35.593286  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:43:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:35.593515  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:43:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:35.593547  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:43:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:35.593590  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:36.164937  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:36.165005  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:36.172103  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:36 GMT] X-Content-Type-Options:[nosniff]] 0xc000321d80 2 [] false false map[] 0xc00105bb00 0xc0017c82c0}
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:36.172154  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:36.272502  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:36.272567  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:36.273806  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:36 GMT]] 0xc001718060 29 [] true false map[] 0xc000fcca00 <nil>}
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:36.273917  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:36.483794  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:36.483860  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:36.491678  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[b9df5db2-1a76-44ba-8890-dcdfb741acaf] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:36 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0017182c0 2 [] false false map[] 0xc00105bf00 0xc001a1c0b0}
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:36.491707  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:36.581227  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[default/unsigned-unencrypted-cc-1]
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:36.581306  199956 pod_workers.go:888] "Processing pod event" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:36.581363  199956 kubelet.go:1501] "syncPod enter" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:36.581394  199956 kubelet_pods.go:1435] "Generating pod status" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:36.581488  199956 kubelet_pods.go:1447] "Got phase for pod" pod="default/unsigned-unencrypted-cc-1" oldPhase=Pending phase=Pending
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:36.581762  199956 status_manager.go:535] "Ignoring same status for pod" pod="default/unsigned-unencrypted-cc-1" status={Phase:Pending Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.14 PodIPs:[{IP:10.244.0.14}] StartTime:2023-01-20 12:41:08 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:ci-example256m State:{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "ngcn-registry.sh.intel.com:443/ci-example256m:latest",} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest ImageID: ContainerID: Started:0xc0011141ee}] QOSClass:Guaranteed EphemeralContainerStatuses:[]}
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:36.582069  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:36.582139  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:36.582187  199956 kuberuntime_manager.go:638] "Container of pod is not in the desired state and shall be started" containerName="ci-example256m" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:36.582225  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 Attempt:2 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:36.582495  199956 kuberuntime_manager.go:889] "Creating container in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:36.583647  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Normal" reason="BackOff" message="Back-off pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\""
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:36.583664  199956 kuberuntime_manager.go:903] "Container start failed in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1" containerMessage="Back-off pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\"" err="ImagePullBackOff"
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:36.583705  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Warning" reason="Failed" message="Error: ImagePullBackOff"
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:36.583743  199956 kubelet.go:1503] "syncPod exit" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d isTerminal=false
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: E0120 12:43:36.583783  199956 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ci-example256m\" with ImagePullBackOff: \"Back-off pulling image \\\"ngcn-registry.sh.intel.com:443/ci-example256m:latest\\\"\"" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:36.583831  199956 pod_workers.go:988] "Processing pod event done" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:36.676510  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="default/unsigned-unencrypted-cc-1" volumeName="kube-api-access-qcb8g" volumeSpecName="kube-api-access-qcb8g"
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:36.685652  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") Volume is already mounted to pod, but remount was requested." pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:36.685840  199956 projected.go:183] Setting up volume kube-api-access-qcb8g for pod 4f97314d-815b-4787-b174-5c158cd28c9d at /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:36.686256  199956 atomic_writer.go:161] pod default/unsigned-unencrypted-cc-1 volume kube-api-access-qcb8g: no update required for target directory /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:36.686327  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") " pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:36.895089  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:36.895160  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:36.902936  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:36 GMT] X-Content-Type-Options:[nosniff]] 0xc00123eae0 2 [] false false map[] 0xc000fccf00 0xc001c48210}
Jan 20 12:43:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:36.902965  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:43:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:37.484031  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:37.484100  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:37.491865  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[754d46c9-5b3a-48ba-8768-a88c4a1e7d58] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:37 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000707c80 2 [] false false map[] 0xc001847d00 0xc001e0e0b0}
Jan 20 12:43:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:37.491892  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:37.581569  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[kube-system/kube-scheduler-zcy-z390-aorus-master]
Jan 20 12:43:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:37.581662  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:43:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:37.581747  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 updateType=0
Jan 20 12:43:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:37.581832  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3
Jan 20 12:43:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:37.581864  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/kube-scheduler-zcy-z390-aorus-master"
Jan 20 12:43:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:37.581955  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" oldPhase=Running phase=Running
Jan 20 12:43:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:37.582684  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:27 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:41 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:41 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:27 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.239.159.53 PodIPs:[{IP:10.239.159.53}] StartTime:2023-01-20 12:31:27 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:kube-scheduler State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:22 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:6 Image:k8s.gcr.io/kube-scheduler:v1.24.0 ImageID:k8s.gcr.io/kube-scheduler@sha256:db842a7c431fd51db7e1911f6d1df27a7b6b6963ceda24852b654d2cd535b776 ContainerID:containerd://13149330f3752d0ff65bcac86c7b522b2040811e815fbc87e56b40b612875d5a Started:0xc00104f782}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:43:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:37.583050  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-scheduler-zcy-z390-aorus-master"
Jan 20 12:43:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:37.583127  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/kube-scheduler-zcy-z390-aorus-master"
Jan 20 12:43:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:37.583212  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Jan 20 12:43:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:37.583641  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/kube-scheduler-zcy-z390-aorus-master"
Jan 20 12:43:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:37.583802  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 isTerminal=false
Jan 20 12:43:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:37.583857  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 updateType=0
Jan 20 12:43:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:37.587468  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:43:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:37.587480  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:43:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:37.587486  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:43:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:37.588183  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:43:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:37.588266  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:43:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:37.588272  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:43:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:37.588281  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:43:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:37.833818  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:43:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:38.483366  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:38.483392  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:38.487794  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[39eb5a76-9213-461a-95d0-1ce320c33cb5] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:38 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123f380 2 [] false false map[] 0xc000338600 0xc00200f970}
Jan 20 12:43:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:38.487839  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:38.879110  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/healthz" timeout="1s"
Jan 20 12:43:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:38.879161  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:43:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:38.879180  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:38.879228  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:38.880327  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:38 GMT] X-Content-Type-Options:[nosniff]] 0xc001718a00 2 [] true false map[] 0xc000338a00 <nil>}
Jan 20 12:43:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:38.880436  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:43:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:38.880404  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:38 GMT] X-Content-Type-Options:[nosniff]] 0xc001718a40 2 [] true false map[] 0xc000fcd300 <nil>}
Jan 20 12:43:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:38.880517  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:43:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:39.483501  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:39.483569  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:39.491750  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[e5a2e4be-f598-4288-aa37-678c835d887f] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:39 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123fc80 2 [] false false map[] 0xc000fcd500 0xc002217c30}
Jan 20 12:43:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:39.491777  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:39.581656  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:43:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:39.589038  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:43:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:39.589102  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:43:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:39.591249  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:43:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:39.593413  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:43:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:39.593640  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:43:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:39.593672  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:43:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:39.593715  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:43:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:40.126813  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:43:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:40.134630  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:43:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:40.144848  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:43:31.262591008 -0500 EST"
Jan 20 12:43:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:40.144861  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510776" capacity="511757" time="2023-01-20 12:43:40.14452352 -0500 EST m=+732.700298345"
Jan 20 12:43:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:40.144869  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59520324Ki" capacity="65586124Ki" time="2023-01-20 12:43:40.128918126 -0500 EST m=+732.684693018"
Jan 20 12:43:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:40.144875  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64393256Ki" capacity="65061836Ki" time="2023-01-20 12:43:40.144794491 -0500 EST m=+732.700569316"
Jan 20 12:43:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:40.144881  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902003060Ki" capacity="981310056Ki" time="2023-01-20 12:43:40.128918126 -0500 EST m=+732.684693018"
Jan 20 12:43:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:40.144887  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:43:40.128918126 -0500 EST m=+732.684693018"
Jan 20 12:43:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:40.144893  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902003060Ki" capacity="981310056Ki" time="2023-01-20 12:43:31.262591008 -0500 EST"
Jan 20 12:43:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:40.144918  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:43:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:40.483477  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:40.483555  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:40.491573  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[5472d0f5-13f0-4908-9b21-1ee8395970da] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:40 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e81e0 2 [] false false map[] 0xc0002c2d00 0xc0002b6160}
Jan 20 12:43:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:40.491603  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:41.166165  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:43:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:41.166236  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:41.174885  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[2e5e062a-1d6c-487a-b898-816c87b6babd] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:41 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001718860 2 [] false false map[] 0xc0002c3300 0xc001c48210}
Jan 20 12:43:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:41.174943  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:41.483529  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:41.483590  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:41.496443  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[a68d46f9-e80a-4016-a4d0-1b36f39d5d6b] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:41 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0017188c0 2 [] false false map[] 0xc0002c3600 0xc000fd9c30}
Jan 20 12:43:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:41.496599  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:41.581632  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:43:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:41.587467  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:43:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:41.587479  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:43:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:41.587486  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:43:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:41.588161  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:43:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:41.588233  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:43:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:41.588240  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:43:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:41.588249  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:43:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:42.483976  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:42.484045  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:42.497012  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[5f2ddf76-1612-4143-b53d-a62cfc0301d4] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:42 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ce9f60 2 [] false false map[] 0xc001775300 0xc00127c2c0}
Jan 20 12:43:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:42.497159  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:42.689304  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:43:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:42.689370  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:42.690487  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:42 GMT]] 0xc0002bf120 2 [] true false map[] 0xc000338b00 <nil>}
Jan 20 12:43:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:42.690597  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:43:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:42.695804  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:43:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:42.695868  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:42.696855  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:42 GMT]] 0xc0002bf1c0 2 [] true false map[] 0xc0002c3800 <nil>}
Jan 20 12:43:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:42.696953  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:43:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:42.710160  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:43:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:42.710204  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:43:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:42.710223  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:42.710263  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:42.711434  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:42 GMT]] 0xc0002bf2c0 2 [] true false map[] 0xc0002c3a00 <nil>}
Jan 20 12:43:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:42.711459  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:42 GMT]] 0xc000ffa080 2 [] true false map[] 0xc001775600 <nil>}
Jan 20 12:43:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:42.711543  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:43:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:42.711574  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:43:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:42.835830  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:43:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:42.918795  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:43:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:42.918802  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:43:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:42.918989  199956 interface.go:209] Interface eno2 is up
Jan 20 12:43:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:42.919015  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:43:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:42.919023  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:43:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:42.919027  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:43:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:42.919031  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:43:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:42.919034  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:43:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:42.919415  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:43:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:42.919421  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:43:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:42.919426  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:43:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:42.919430  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:43:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:43.483189  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:43.483260  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:43.491355  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[c5315c05-dcee-438b-b89f-f15694cd24e4] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:43 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0014873e0 2 [] false false map[] 0xc0002c3e00 0xc0012fc210}
Jan 20 12:43:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:43.491395  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:43.582158  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:43:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:43.587585  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:43:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:43.587598  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:43:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:43.588324  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:43:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:43.588787  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:43:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:43.588831  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:43:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:43.588838  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:43:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:43.588847  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:43:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:44.483499  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:44.483577  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:44.491627  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[be9a0aef-77b7-43b1-adc6-22ccbaa5aae8] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:44 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000978660 2 [] false false map[] 0xc0011c4200 0xc0019bc000}
Jan 20 12:43:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:44.491663  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:45.483817  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:45.483886  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:45.491778  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[5feeea27-a599-45b4-b864-a6b28f799070] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:45 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0000a49e0 2 [] false false map[] 0xc0011c4500 0xc001c56000}
Jan 20 12:43:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:45.491804  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:45.581679  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:43:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:45.587438  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:43:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:45.587449  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:43:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:45.587456  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:43:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:45.588069  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:43:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:45.588153  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:43:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:45.588160  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:43:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:45.588183  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:43:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:46.164201  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:43:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:46.164273  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:46.172138  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:46 GMT] X-Content-Type-Options:[nosniff]] 0xc0009799a0 2 [] false false map[] 0xc0011c4a00 0xc00187f8c0}
Jan 20 12:43:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:46.172162  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:43:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:46.272113  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:43:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:46.272172  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:46.273396  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:46 GMT]] 0xc000707900 29 [] true false map[] 0xc001716b00 <nil>}
Jan 20 12:43:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:46.273500  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:43:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:46.483153  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:46.483217  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:46.491553  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[5361dcc4-94e2-4d16-b6c8-4e65582d25c4] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:46 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0000a5620 2 [] false false map[] 0xc001846500 0xc00187fad0}
Jan 20 12:43:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:46.491584  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:46.895077  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:43:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:46.895148  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:46.902939  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:46 GMT] X-Content-Type-Options:[nosniff]] 0xc0000a5d40 2 [] false false map[] 0xc001716d00 0xc001ff1810}
Jan 20 12:43:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:46.902968  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:43:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:47.483742  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:47.483811  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:47.491886  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[ea525b5b-c29c-4be7-9e20-18d666c30d79] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:47 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001820080 2 [] false false map[] 0xc001846800 0xc0020a94a0}
Jan 20 12:43:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:47.491916  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:47.526259  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/etcd.yaml"
Jan 20 12:43:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:47.528416  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver.yaml"
Jan 20 12:43:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:47.531170  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Jan 20 12:43:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:47.531671  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-scheduler.yaml"
Jan 20 12:43:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:47.531954  199956 config.go:279] "Setting pods for source" source="file"
Jan 20 12:43:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:47.581957  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:43:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:47.585865  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:43:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:47.585897  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:43:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:47.587344  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:43:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:47.588644  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:43:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:47.588820  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:43:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:47.588843  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:43:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:47.588871  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:43:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:47.836630  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:43:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:48.483355  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:48.483427  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:48.496525  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[ff1bb050-2bd2-4c9c-ae79-8b5435bb1896] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:48 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00129c020 2 [] false false map[] 0xc001774100 0xc0008ca9a0}
Jan 20 12:43:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:48.496677  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:48.879519  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:43:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:48.879586  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:48.880773  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:48 GMT] X-Content-Type-Options:[nosniff]] 0xc00129c060 2 [] true false map[] 0xc001846200 <nil>}
Jan 20 12:43:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:48.880884  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:43:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:49.483870  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:49.483933  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:49.491997  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[d3642eb7-f8dd-417e-bca7-a65576d77b0b] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:49 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00129c080 2 [] false false map[] 0xc001846500 0xc0013b18c0}
Jan 20 12:43:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:49.492032  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:49.582091  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:43:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:49.587517  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:43:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:49.587528  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:43:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:49.588358  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:43:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:49.588742  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:43:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:49.588786  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:43:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:49.588793  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:43:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:49.588802  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:43:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:50.145963  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:43:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:50.153548  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:43:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:50.163694  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59520268Ki" capacity="65586124Ki" time="2023-01-20 12:43:50.147913139 -0500 EST m=+742.703688030"
Jan 20 12:43:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:50.163707  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64393168Ki" capacity="65061836Ki" time="2023-01-20 12:43:50.16363608 -0500 EST m=+742.719410905"
Jan 20 12:43:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:50.163714  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902003028Ki" capacity="981310056Ki" time="2023-01-20 12:43:50.147913139 -0500 EST m=+742.703688030"
Jan 20 12:43:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:50.163721  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:43:50.147913139 -0500 EST m=+742.703688030"
Jan 20 12:43:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:50.163726  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902003028Ki" capacity="981310056Ki" time="2023-01-20 12:43:41.262184501 -0500 EST"
Jan 20 12:43:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:50.163732  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:43:41.262184501 -0500 EST"
Jan 20 12:43:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:50.163738  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510776" capacity="511757" time="2023-01-20 12:43:50.163377277 -0500 EST m=+742.719152102"
Jan 20 12:43:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:50.163763  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:43:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:50.483639  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:50.483704  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:50.496584  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[e04e573c-3cfa-4498-9cc3-b89dcb1af868] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:50 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001820e00 2 [] false false map[] 0xc001846a00 0xc0013b1ad0}
Jan 20 12:43:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:50.496742  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.166221  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.166298  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.179711  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[80a85b01-5680-4304-9620-d43bf40988e3] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:51 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0002be000 2 [] false false map[] 0xc001846d00 0xc000fd6e70}
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.179866  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.483760  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.483828  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.491759  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[43143236-eea0-4b45-94ac-900bff69538b] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:51 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001821b20 2 [] false false map[] 0xc0021a2a00 0xc001837760}
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.491789  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.581983  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[default/unsigned-unencrypted-cc-1]
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.582071  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.582273  199956 pod_workers.go:888] "Processing pod event" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.582355  199956 kubelet.go:1501] "syncPod enter" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.582388  199956 kubelet_pods.go:1435] "Generating pod status" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.582491  199956 kubelet_pods.go:1447] "Got phase for pod" pod="default/unsigned-unencrypted-cc-1" oldPhase=Pending phase=Pending
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.582775  199956 status_manager.go:535] "Ignoring same status for pod" pod="default/unsigned-unencrypted-cc-1" status={Phase:Pending Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.14 PodIPs:[{IP:10.244.0.14}] StartTime:2023-01-20 12:41:08 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:ci-example256m State:{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "ngcn-registry.sh.intel.com:443/ci-example256m:latest",} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest ImageID: ContainerID: Started:0xc00094f0f9}] QOSClass:Guaranteed EphemeralContainerStatuses:[]}
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.583115  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.583190  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.583238  199956 kuberuntime_manager.go:638] "Container of pod is not in the desired state and shall be started" containerName="ci-example256m" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.583278  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 Attempt:2 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.583410  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="default/unsigned-unencrypted-cc-1" volumeName="kube-api-access-qcb8g" volumeSpecName="kube-api-access-qcb8g"
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.583551  199956 kuberuntime_manager.go:889] "Creating container in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.584643  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Normal" reason="Pulling" message="Pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\""
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.584649  199956 provider.go:102] Refreshing cache for provider: *credentialprovider.defaultDockerConfigProvider
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.584746  199956 config.go:144] looking for config.json at /var/lib/kubelet/config.json
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.584832  199956 config.go:144] looking for config.json at /config.json
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.584862  199956 config.go:144] looking for config.json at /.docker/config.json
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.584884  199956 config.go:144] looking for config.json at /.docker/config.json
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.584913  199956 config.go:110] looking for .dockercfg at /var/lib/kubelet/.dockercfg
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.584939  199956 config.go:110] looking for .dockercfg at /.dockercfg
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.584961  199956 config.go:110] looking for .dockercfg at /.dockercfg
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.584979  199956 config.go:110] looking for .dockercfg at /.dockercfg
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.585000  199956 provider.go:82] Docker config file not found: couldn't find valid .dockercfg after checking in [/var/lib/kubelet   /]
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.585043  199956 kuberuntime_image.go:47] "Pulling image without credentials" image="ngcn-registry.sh.intel.com:443/ci-example256m:latest"
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:43:51.585580161-05:00" level=info msg="PullImage \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\""
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:43:51.585693456-05:00" level=info msg="TaskManager get ImageService succeed." id=f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.587599  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.587611  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.588124  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.588450  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.588495  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.588502  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.588511  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.594171  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") Volume is already mounted to pod, but remount was requested." pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.594229  199956 projected.go:183] Setting up volume kube-api-access-qcb8g for pod 4f97314d-815b-4787-b174-5c158cd28c9d at /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.594339  199956 atomic_writer.go:161] pod default/unsigned-unencrypted-cc-1 volume kube-api-access-qcb8g: no update required for target directory /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.594354  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") " pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:43:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:51.713461  199956 handler.go:293] error while reading "/proc/199956/fd/34" link: readlink /proc/199956/fd/34: no such file or directory
Jan 20 12:43:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:52.483840  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:52.483936  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:52.491782  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[711e4d4a-b2e8-474a-b59c-92e4c3f71ec2] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:52 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000cb2ce0 2 [] false false map[] 0xc0002c3500 0xc0019e7550}
Jan 20 12:43:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:52.491819  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:52.582257  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr]
Jan 20 12:43:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:52.582339  199956 pod_workers.go:888] "Processing pod event" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d updateType=0
Jan 20 12:43:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:52.582401  199956 kubelet.go:1501] "syncPod enter" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d
Jan 20 12:43:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:52.582434  199956 kubelet_pods.go:1435] "Generating pod status" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:43:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:52.582536  199956 kubelet_pods.go:1447] "Got phase for pod" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" oldPhase=Running phase=Running
Jan 20 12:43:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:52.582869  199956 status_manager.go:535] "Ignoring same status for pod" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:58 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:08 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:08 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:58 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.4 PodIPs:[{IP:10.244.0.4}] StartTime:2023-01-20 12:31:58 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:kube-rbac-proxy State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:59 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:gcr.io/kubebuilder/kube-rbac-proxy:v0.13.0 ImageID:gcr.io/kubebuilder/kube-rbac-proxy@sha256:d99a8d144816b951a67648c12c0b988936ccd25cf3754f3cd85ab8c01592248f ContainerID:containerd://1cb44bdbde0b41852e382d7dab7c184af8dd4e5b25d5cfe6a10a9c8ab601659d Started:0xc001bdce3a} {Name:manager State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:59 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:quay.io/confidential-containers/operator:v0.2.0 ImageID:quay.io/confidential-containers/operator@sha256:c965b55253a9abe4c2f7596c42467fa59f2cc741bfafeed1d25629ed6f8df12d ContainerID:containerd://186424b47c7b9022ecfe8996dc73cd9101f7539259cd65914279c84c9a02a288 Started:0xc001bdce3b}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:43:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:52.583193  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:43:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:52.583265  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:43:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:52.583922  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:43:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:52.584102  199956 kubelet.go:1503] "syncPod exit" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d isTerminal=false
Jan 20 12:43:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:52.584161  199956 pod_workers.go:988] "Processing pod event done" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d updateType=0
Jan 20 12:43:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:52.590747  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" volumeName="kube-api-access-4pnfq" volumeSpecName="kube-api-access-4pnfq"
Jan 20 12:43:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:52.602923  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-4pnfq\" (UniqueName: \"kubernetes.io/projected/d2688d45-2487-46e7-aecb-e3479626909d-kube-api-access-4pnfq\") pod \"cc-operator-controller-manager-79797456f6-m6znr\" (UID: \"d2688d45-2487-46e7-aecb-e3479626909d\") Volume is already mounted to pod, but remount was requested." pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:43:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:52.603137  199956 projected.go:183] Setting up volume kube-api-access-4pnfq for pod d2688d45-2487-46e7-aecb-e3479626909d at /var/lib/kubelet/pods/d2688d45-2487-46e7-aecb-e3479626909d/volumes/kubernetes.io~projected/kube-api-access-4pnfq
Jan 20 12:43:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:52.603546  199956 atomic_writer.go:161] pod confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr volume kube-api-access-4pnfq: no update required for target directory /var/lib/kubelet/pods/d2688d45-2487-46e7-aecb-e3479626909d/volumes/kubernetes.io~projected/kube-api-access-4pnfq
Jan 20 12:43:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:52.603624  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-4pnfq\" (UniqueName: \"kubernetes.io/projected/d2688d45-2487-46e7-aecb-e3479626909d-kube-api-access-4pnfq\") pod \"cc-operator-controller-manager-79797456f6-m6znr\" (UID: \"d2688d45-2487-46e7-aecb-e3479626909d\") " pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:43:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:52.689466  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:43:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:52.689532  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:52.690647  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:52 GMT]] 0xc000cb2fa0 2 [] true false map[] 0xc0000dd400 <nil>}
Jan 20 12:43:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:52.690761  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:43:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:52.696171  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:43:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:52.696230  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:52.697151  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:52 GMT]] 0xc000cb3020 2 [] true false map[] 0xc000338300 <nil>}
Jan 20 12:43:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:52.697253  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:43:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:52.709741  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:43:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:52.709800  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:52.709816  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:43:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:52.709876  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:52.710756  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:52 GMT]] 0xc000cb30e0 2 [] true false map[] 0xc000338600 <nil>}
Jan 20 12:43:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:52.710861  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:43:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:52.710934  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:52 GMT]] 0xc000320a60 2 [] true false map[] 0xc0000dd800 <nil>}
Jan 20 12:43:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:52.711042  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:43:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:52.838282  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:43:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:53.140951  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:43:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:53.140960  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:43:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:53.141171  199956 interface.go:209] Interface eno2 is up
Jan 20 12:43:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:53.141200  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:43:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:53.141208  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:43:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:53.141213  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:43:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:53.141217  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:43:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:53.141221  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:43:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:53.141498  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:43:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:53.141509  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:43:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:53.141514  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:43:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:53.141517  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:43:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:53.484138  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:53.484207  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:53.492746  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[b7f3c200-54e8-4373-80be-8cf3a6952a71] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:53 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0014878a0 2 [] false false map[] 0xc0000dda00 0xc001e0e210}
Jan 20 12:43:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:53.492774  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:53.581257  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:43:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:53.587403  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:43:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:53.587416  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:43:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:53.587423  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:43:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:53.588176  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:43:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:53.588223  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:43:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:53.588230  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:43:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:53.588238  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:43:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:54.483493  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:54.483567  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:54.491566  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[eff32b79-2432-4c7e-9e19-69921e499173] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:54 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0000a5e00 2 [] false false map[] 0xc000338a00 0xc002009d90}
Jan 20 12:43:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:54.491595  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:55.483246  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:55.483311  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:55.491920  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[f6da2e32-e922-4cda-8eaa-393e7e996970] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:55 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0020fc120 2 [] false false map[] 0xc000fcca00 0xc0020c48f0}
Jan 20 12:43:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:55.492199  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:55.581873  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:43:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:55.587494  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:43:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:55.587507  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:43:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:55.588326  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:43:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:55.588766  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:43:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:55.588812  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:43:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:55.588819  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:43:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:55.588827  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:43:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:56.164715  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:43:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:56.164790  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:56.172208  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:56 GMT] X-Content-Type-Options:[nosniff]] 0xc000707c60 2 [] false false map[] 0xc000338600 0xc000b1f550}
Jan 20 12:43:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:56.172232  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:43:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:56.272185  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:43:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:56.272249  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:56.273502  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:56 GMT]] 0xc000320e40 29 [] true false map[] 0xc000338b00 <nil>}
Jan 20 12:43:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:56.273607  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:43:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:56.484220  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:56.484292  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:56.492594  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[4edfe241-ca93-47a6-aabd-e0d6c19f5718] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:56 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001486320 2 [] false false map[] 0xc001716100 0xc001982160}
Jan 20 12:43:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:56.492623  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:56.581547  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[kube-system/kube-apiserver-zcy-z390-aorus-master]
Jan 20 12:43:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:56.581627  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd updateType=0
Jan 20 12:43:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:56.581688  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd
Jan 20 12:43:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:56.581722  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/kube-apiserver-zcy-z390-aorus-master"
Jan 20 12:43:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:56.581808  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" oldPhase=Running phase=Running
Jan 20 12:43:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:56.582087  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:28 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:41 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:41 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:28 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.239.159.53 PodIPs:[{IP:10.239.159.53}] StartTime:2023-01-20 12:31:28 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:kube-apiserver State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:22 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:6 Image:k8s.gcr.io/kube-apiserver:v1.24.0 ImageID:k8s.gcr.io/kube-apiserver@sha256:a04522b882e919de6141b47d72393fb01226c78e7388400f966198222558c955 ContainerID:containerd://5900ac5c1383a321a6ae837b57d6d29d7a660a102c0806210a9ea57290673062 Started:0xc0014f750e}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:43:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:56.582408  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-apiserver-zcy-z390-aorus-master"
Jan 20 12:43:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:56.582484  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/kube-apiserver-zcy-z390-aorus-master"
Jan 20 12:43:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:56.583604  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/kube-apiserver-zcy-z390-aorus-master"
Jan 20 12:43:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:56.583769  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd isTerminal=false
Jan 20 12:43:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:56.583824  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd updateType=0
Jan 20 12:43:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:56.619062  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" volumeName="ca-certs" volumeSpecName="ca-certs"
Jan 20 12:43:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:56.619188  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Jan 20 12:43:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:56.619268  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" volumeName="etc-pki" volumeSpecName="etc-pki"
Jan 20 12:43:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:56.619334  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Jan 20 12:43:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:56.619404  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Jan 20 12:43:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:56.619473  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Jan 20 12:43:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:56.895422  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:43:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:56.895491  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:56.903081  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:56 GMT] X-Content-Type-Options:[nosniff]] 0xc000321a20 2 [] false false map[] 0xc001a4bf00 0xc0011bf1e0}
Jan 20 12:43:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:56.903109  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:43:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:57.483160  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:57.483245  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:57.491789  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[b59ed9b1-a698-4e3d-8721-e22805ab5b64] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:57 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123e2e0 2 [] false false map[] 0xc000339400 0xc0011bf340}
Jan 20 12:43:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:57.491820  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:57.581796  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:43:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:57.587616  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:43:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:57.587628  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:43:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:57.588327  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:43:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:57.588755  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:43:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:57.588801  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:43:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:57.588807  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:43:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:57.588816  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:43:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:57.773300  199956 reflector.go:536] pkg/kubelet/config/apiserver.go:66: Watch close - *v1.Pod total 21 items received
Jan 20 12:43:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:57.839407  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:43:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:58.484068  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:58.484135  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:58.491560  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[2c24ba69-8d18-4329-8b09-fcc4a3ca4e3e] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:58 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0009a1ea0 2 [] false false map[] 0xc001847300 0xc001121290}
Jan 20 12:43:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:58.491591  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:58.879571  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:43:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:58.879634  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:58.879673  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/healthz" timeout="1s"
Jan 20 12:43:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:58.879730  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:58.880999  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:58 GMT] X-Content-Type-Options:[nosniff]] 0xc000ffa100 2 [] true false map[] 0xc001774500 <nil>}
Jan 20 12:43:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:58.881107  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:43:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:58.881071  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:58 GMT] X-Content-Type-Options:[nosniff]] 0xc00129c760 2 [] true false map[] 0xc001847600 <nil>}
Jan 20 12:43:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:58.881190  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:43:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:59.483690  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:43:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:59.483766  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:43:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:59.491781  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[1d3d2659-9985-4fb4-ae87-9134c38b19a1] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:43:59 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0020fd1c0 2 [] false false map[] 0xc000339d00 0xc0012d7080}
Jan 20 12:43:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:59.491810  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:43:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:59.581395  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:43:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:59.588787  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:43:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:59.588849  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:43:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:59.591053  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:43:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:59.593336  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:43:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:59.593563  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:43:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:59.593597  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:43:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:43:59.593639  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:44:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:00.164095  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:44:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:00.171641  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:44:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:00.181720  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64392752Ki" capacity="65061836Ki" time="2023-01-20 12:44:00.181662466 -0500 EST m=+752.737437291"
Jan 20 12:44:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:00.181734  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902002976Ki" capacity="981310056Ki" time="2023-01-20 12:44:00.166154513 -0500 EST m=+752.721929404"
Jan 20 12:44:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:00.181741  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:44:00.166154513 -0500 EST m=+752.721929404"
Jan 20 12:44:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:00.181747  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902002976Ki" capacity="981310056Ki" time="2023-01-20 12:43:51.26418537 -0500 EST"
Jan 20 12:44:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:00.181753  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:43:51.26418537 -0500 EST"
Jan 20 12:44:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:00.181759  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510776" capacity="511757" time="2023-01-20 12:44:00.181399478 -0500 EST m=+752.737174303"
Jan 20 12:44:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:00.181764  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59483656Ki" capacity="65586124Ki" time="2023-01-20 12:44:00.166154513 -0500 EST m=+752.721929404"
Jan 20 12:44:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:00.181789  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:44:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:00.483766  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:00.483834  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:00.496592  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[fda3c7ba-e595-4b6f-bb14-97718cd73604] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:00 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ffb5a0 2 [] false false map[] 0xc0021a2200 0xc001714fd0}
Jan 20 12:44:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:00.496753  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:01.166293  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:44:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:01.166369  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:01.174606  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[802ab0dc-4695-49cd-bb25-24e9c6cc37da] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:01 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001821320 2 [] false false map[] 0xc001847d00 0xc001c83760}
Jan 20 12:44:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:01.174658  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:01.484090  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:01.484155  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:01.491837  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[73b5b5e9-e89a-4750-8cc2-4a6850e1493e] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:01 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0020fdba0 2 [] false false map[] 0xc00105a500 0xc001da3810}
Jan 20 12:44:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:01.491867  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:01.582239  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[confidential-containers-system/cc-operator-daemon-install-t6mp7]
Jan 20 12:44:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:01.582324  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:44:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:01.582468  199956 pod_workers.go:888] "Processing pod event" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" podUID=7af065b7-9095-4d91-9b9e-2644e7b1f4bf updateType=0
Jan 20 12:44:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:01.582556  199956 kubelet.go:1501] "syncPod enter" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" podUID=7af065b7-9095-4d91-9b9e-2644e7b1f4bf
Jan 20 12:44:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:01.582598  199956 kubelet_pods.go:1435] "Generating pod status" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7"
Jan 20 12:44:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:01.582686  199956 kubelet_pods.go:1447] "Got phase for pod" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" oldPhase=Running phase=Running
Jan 20 12:44:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:01.582975  199956 status_manager.go:535] "Ignoring same status for pod" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:04 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:20 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:20 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:04 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.6 PodIPs:[{IP:10.244.0.6}] StartTime:2023-01-20 12:32:04 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:cc-runtime-install-pod State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:32:20 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:quay.io/confidential-containers/runtime-payload-ci:kata-containers-amd64 ImageID:quay.io/confidential-containers/runtime-payload-ci@sha256:4736ba274765c889404fb98f01de0a997e68d2d7e5acca2440488f0e1337032b ContainerID:containerd://e10efaa459a32161059fda75d4bcaf3bbdb896781f772b508e43fe00bc7c9003 Started:0xc001d75c89}] QOSClass:BestEffort EphemeralContainerStatuses:[]}
Jan 20 12:44:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:01.583319  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7"
Jan 20 12:44:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:01.583392  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7"
Jan 20 12:44:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:01.583812  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="confidential-containers-system/cc-operator-daemon-install-t6mp7"
Jan 20 12:44:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:01.583978  199956 kubelet.go:1503] "syncPod exit" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" podUID=7af065b7-9095-4d91-9b9e-2644e7b1f4bf isTerminal=false
Jan 20 12:44:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:01.584036  199956 pod_workers.go:988] "Processing pod event done" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" podUID=7af065b7-9095-4d91-9b9e-2644e7b1f4bf updateType=0
Jan 20 12:44:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:01.588435  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:44:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:01.588446  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:44:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:01.588452  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:44:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:01.589141  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:44:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:01.589186  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:44:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:01.589193  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:44:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:01.589202  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:44:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:01.657600  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" volumeName="containerd-conf" volumeSpecName="containerd-conf"
Jan 20 12:44:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:01.657706  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" volumeName="kata-artifacts" volumeSpecName="kata-artifacts"
Jan 20 12:44:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:01.657784  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" volumeName="dbus" volumeSpecName="dbus"
Jan 20 12:44:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:01.657851  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" volumeName="systemd" volumeSpecName="systemd"
Jan 20 12:44:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:01.657915  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" volumeName="local-bin" volumeSpecName="local-bin"
Jan 20 12:44:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:01.658032  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" volumeName="kube-api-access-x6vjr" volumeSpecName="kube-api-access-x6vjr"
Jan 20 12:44:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:01.672016  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-x6vjr\" (UniqueName: \"kubernetes.io/projected/7af065b7-9095-4d91-9b9e-2644e7b1f4bf-kube-api-access-x6vjr\") pod \"cc-operator-daemon-install-t6mp7\" (UID: \"7af065b7-9095-4d91-9b9e-2644e7b1f4bf\") Volume is already mounted to pod, but remount was requested." pod="confidential-containers-system/cc-operator-daemon-install-t6mp7"
Jan 20 12:44:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:01.672200  199956 projected.go:183] Setting up volume kube-api-access-x6vjr for pod 7af065b7-9095-4d91-9b9e-2644e7b1f4bf at /var/lib/kubelet/pods/7af065b7-9095-4d91-9b9e-2644e7b1f4bf/volumes/kubernetes.io~projected/kube-api-access-x6vjr
Jan 20 12:44:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:01.672614  199956 atomic_writer.go:161] pod confidential-containers-system/cc-operator-daemon-install-t6mp7 volume kube-api-access-x6vjr: no update required for target directory /var/lib/kubelet/pods/7af065b7-9095-4d91-9b9e-2644e7b1f4bf/volumes/kubernetes.io~projected/kube-api-access-x6vjr
Jan 20 12:44:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:01.672722  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-x6vjr\" (UniqueName: \"kubernetes.io/projected/7af065b7-9095-4d91-9b9e-2644e7b1f4bf-kube-api-access-x6vjr\") pod \"cc-operator-daemon-install-t6mp7\" (UID: \"7af065b7-9095-4d91-9b9e-2644e7b1f4bf\") " pod="confidential-containers-system/cc-operator-daemon-install-t6mp7"
Jan 20 12:44:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:02.483351  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:02.483419  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:02.491373  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[3c2c4a29-a329-42d4-81c5-0e06a4c81464] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:02 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0018213a0 2 [] false false map[] 0xc001efc400 0xc001f57760}
Jan 20 12:44:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:02.491403  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:02.689959  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:44:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:02.690028  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:02.691183  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:02 GMT]] 0xc0012e9920 2 [] true false map[] 0xc001f36700 <nil>}
Jan 20 12:44:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:02.691290  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:44:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:02.695385  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:44:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:02.695448  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:02.696508  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:02 GMT]] 0xc0012e9960 2 [] true false map[] 0xc001f36f00 <nil>}
Jan 20 12:44:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:02.696606  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:44:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:02.709838  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:44:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:02.709898  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:02.709911  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:44:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:02.709973  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:02.711011  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:02 GMT]] 0xc000cb2060 2 [] true false map[] 0xc001774700 <nil>}
Jan 20 12:44:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:02.711025  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:02 GMT]] 0xc0012e9a20 2 [] true false map[] 0xc001efc700 <nil>}
Jan 20 12:44:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:02.711124  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:44:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:02.711148  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:44:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:02.841035  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.157792  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.157801  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.157955  199956 interface.go:209] Interface eno2 is up
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.157981  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.157988  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.157994  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.157997  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.158001  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.158354  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.158385  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.158390  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.158393  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.483177  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.483240  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.496095  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[b343b9da-8c85-44f7-ad69-5be45f5b9eba] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:03 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000978840 2 [] false false map[] 0xc0021a3900 0xc00217c580}
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.496222  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.581769  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[kube-system/kube-controller-manager-zcy-z390-aorus-master]
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.581857  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.581922  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce updateType=0
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.582002  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.582037  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master"
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.582143  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" oldPhase=Running phase=Running
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.582433  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:28 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:37 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:37 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:28 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.239.159.53 PodIPs:[{IP:10.239.159.53}] StartTime:2023-01-20 12:31:28 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:kube-controller-manager State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:22 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:6 Image:k8s.gcr.io/kube-controller-manager:v1.24.0 ImageID:k8s.gcr.io/kube-controller-manager@sha256:df044a154e79a18f749d3cd9d958c3edde2b6a00c815176472002b7bbf956637 ContainerID:containerd://f8443d2868215ec42d3fa335663976e33df166c5e98369aa3f9d7ddb9235bead Started:0xc0011e901e}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.582796  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master"
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.582869  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master"
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.584892  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/kube-controller-manager-zcy-z390-aorus-master"
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.585274  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce isTerminal=false
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.585371  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce updateType=0
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.588173  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.588190  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.588749  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.589066  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.589111  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.589117  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.589126  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.672053  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="ca-certs" volumeSpecName="ca-certs"
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.672159  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.672238  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="etc-pki" volumeSpecName="etc-pki"
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.672308  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="flexvolume-dir" volumeSpecName="flexvolume-dir"
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.672378  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.672443  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.672509  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Jan 20 12:44:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:03.672575  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Jan 20 12:44:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:04.483173  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:04.483251  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:04.496077  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[78f2b494-1067-48ab-9aad-319f0ee06ca0] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:04 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e8aa0 2 [] false false map[] 0xc0021a2000 0xc0015f2160}
Jan 20 12:44:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:04.496212  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:05.483742  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:05.483811  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:05.491875  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[651fa32e-6ef3-4cdd-ab36-85448403ee41] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:05 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e99e0 2 [] false false map[] 0xc0021a2300 0xc002210420}
Jan 20 12:44:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:05.491907  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:05.581252  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:44:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:05.586619  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:44:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:05.586632  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:44:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:05.586639  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:44:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:05.587414  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:44:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:05.587491  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:44:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:05.587497  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:44:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:05.587506  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:44:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:06.164791  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:44:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:06.164864  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:06.175902  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:06 GMT] X-Content-Type-Options:[nosniff]] 0xc0012e9c40 2 [] false false map[] 0xc001a4b000 0xc000dce370}
Jan 20 12:44:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:06.176035  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:44:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:06.272457  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:44:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:06.272521  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:06.273753  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:06 GMT]] 0xc0007063e0 29 [] true false map[] 0xc001a4b200 <nil>}
Jan 20 12:44:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:06.273852  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:44:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:06.484101  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:06.484170  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:06.496871  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[22a6d1c6-7fca-4ee8-8eba-099fcf579f60] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:06 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00129d640 2 [] false false map[] 0xc0021a2c00 0xc000fda2c0}
Jan 20 12:44:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:06.497011  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:06.895756  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:44:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:06.895829  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:06.903213  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:06 GMT] X-Content-Type-Options:[nosniff]] 0xc00123e040 2 [] false false map[] 0xc000fcc100 0xc0023c9e40}
Jan 20 12:44:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:06.903280  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:44:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:07.483302  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:07.483370  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:07.491717  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[ff3bc3b1-dc76-4df3-824c-d55d26def690] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:07 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000707120 2 [] false false map[] 0xc001a4b500 0xc0013ffb80}
Jan 20 12:44:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:07.491750  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:07.525520  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/etcd.yaml"
Jan 20 12:44:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:07.526316  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver.yaml"
Jan 20 12:44:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:07.527385  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Jan 20 12:44:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:07.528384  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-scheduler.yaml"
Jan 20 12:44:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:07.528959  199956 config.go:279] "Setting pods for source" source="file"
Jan 20 12:44:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:07.582004  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:44:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:07.587532  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:44:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:07.587544  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:44:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:07.588340  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:44:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:07.588796  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:44:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:07.588841  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:44:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:07.588848  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:44:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:07.588856  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:44:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:07.842851  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:44:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:07.992130  199956 handler.go:293] error while reading "/proc/199956/fd/34" link: readlink /proc/199956/fd/34: no such file or directory
Jan 20 12:44:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:08.483155  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:08.483229  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:08.491455  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[e3dcd16f-eed8-4af0-8a9c-afd100147a6a] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:08 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123f6e0 2 [] false false map[] 0xc0023aaa00 0xc0018f9760}
Jan 20 12:44:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:08.491489  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:08.878698  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:44:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:08.878771  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:08.879994  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:08 GMT] X-Content-Type-Options:[nosniff]] 0xc00123f800 2 [] true false map[] 0xc000cf7e00 <nil>}
Jan 20 12:44:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:08.880106  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:44:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:09.483980  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:09.484046  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:09.491804  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[552622a2-3553-4d0e-93e8-2a85af071c85] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:09 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008e2900 2 [] false false map[] 0xc000338200 0xc001b1b970}
Jan 20 12:44:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:09.491834  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:09.581801  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:44:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:09.587522  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:44:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:09.587534  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:44:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:09.587542  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:44:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:09.588384  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:44:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:09.588432  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:44:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:09.588438  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:44:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:09.588446  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:44:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:10.182216  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:44:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:10.190656  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:44:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:10.200471  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902002932Ki" capacity="981310056Ki" time="2023-01-20 12:44:10.184314689 -0500 EST m=+762.740089583"
Jan 20 12:44:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:10.200484  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:44:10.184314689 -0500 EST m=+762.740089583"
Jan 20 12:44:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:10.200491  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902002932Ki" capacity="981310056Ki" time="2023-01-20 12:44:01.262210665 -0500 EST"
Jan 20 12:44:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:10.200497  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:44:01.262210665 -0500 EST"
Jan 20 12:44:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:10.200509  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510775" capacity="511757" time="2023-01-20 12:44:10.200160045 -0500 EST m=+762.755934869"
Jan 20 12:44:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:10.200515  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59478608Ki" capacity="65586124Ki" time="2023-01-20 12:44:10.184314689 -0500 EST m=+762.740089583"
Jan 20 12:44:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:10.200521  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64392448Ki" capacity="65061836Ki" time="2023-01-20 12:44:10.200416096 -0500 EST m=+762.756190920"
Jan 20 12:44:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:10.200546  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:44:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:10.483891  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:10.483961  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:10.491605  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[5facfe83-348b-4726-8fc8-47cc520a4655] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:10 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0020fd040 2 [] false false map[] 0xc0023aaf00 0xc001e03290}
Jan 20 12:44:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:10.491635  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:10.581561  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[kube-system/coredns-6d4b75cb6d-zdl2m]
Jan 20 12:44:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:10.581639  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e updateType=0
Jan 20 12:44:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:10.581698  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e
Jan 20 12:44:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:10.581731  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:44:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:10.581819  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/coredns-6d4b75cb6d-zdl2m" oldPhase=Running phase=Running
Jan 20 12:44:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:10.582103  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/coredns-6d4b75cb6d-zdl2m" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:44 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:44 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.3 PodIPs:[{IP:10.244.0.3}] StartTime:2023-01-20 12:31:42 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:coredns State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:44 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:k8s.gcr.io/coredns/coredns:v1.8.6 ImageID:k8s.gcr.io/coredns/coredns@sha256:5b6ec0d6de9baaf3e92d0f66cd96a25b9edbce8716f5f15dcd1a616b3abd590e ContainerID:containerd://e489181a7f95431b6d92f037dbe3f788b46a5e024df7aaf29e0115dab1168ab1 Started:0xc001747c29}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:44:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:10.582434  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:44:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:10.582503  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:44:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:10.583001  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:44:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:10.583174  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e isTerminal=false
Jan 20 12:44:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:10.583227  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e updateType=0
Jan 20 12:44:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:10.624031  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/coredns-6d4b75cb6d-zdl2m" volumeName="config-volume" volumeSpecName="config-volume"
Jan 20 12:44:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:10.624180  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/coredns-6d4b75cb6d-zdl2m" volumeName="kube-api-access-tqzsm" volumeSpecName="kube-api-access-tqzsm"
Jan 20 12:44:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:10.639269  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-tqzsm\" (UniqueName: \"kubernetes.io/projected/cf25b9ea-6823-4eff-b30d-36a09f9d897e-kube-api-access-tqzsm\") pod \"coredns-6d4b75cb6d-zdl2m\" (UID: \"cf25b9ea-6823-4eff-b30d-36a09f9d897e\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:44:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:10.639406  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/cf25b9ea-6823-4eff-b30d-36a09f9d897e-config-volume\") pod \"coredns-6d4b75cb6d-zdl2m\" (UID: \"cf25b9ea-6823-4eff-b30d-36a09f9d897e\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:44:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:10.639543  199956 configmap.go:181] Setting up volume config-volume for pod cf25b9ea-6823-4eff-b30d-36a09f9d897e at /var/lib/kubelet/pods/cf25b9ea-6823-4eff-b30d-36a09f9d897e/volumes/kubernetes.io~configmap/config-volume
Jan 20 12:44:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:10.639577  199956 projected.go:183] Setting up volume kube-api-access-tqzsm for pod cf25b9ea-6823-4eff-b30d-36a09f9d897e at /var/lib/kubelet/pods/cf25b9ea-6823-4eff-b30d-36a09f9d897e/volumes/kubernetes.io~projected/kube-api-access-tqzsm
Jan 20 12:44:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:10.639601  199956 configmap.go:205] Received configMap kube-system/coredns containing (1) pieces of data, 339 total bytes
Jan 20 12:44:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:10.639690  199956 quota_linux.go:271] SupportsQuotas called, but quotas disabled
Jan 20 12:44:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:10.639918  199956 atomic_writer.go:161] pod kube-system/coredns-6d4b75cb6d-zdl2m volume config-volume: no update required for target directory /var/lib/kubelet/pods/cf25b9ea-6823-4eff-b30d-36a09f9d897e/volumes/kubernetes.io~configmap/config-volume
Jan 20 12:44:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:10.639982  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/cf25b9ea-6823-4eff-b30d-36a09f9d897e-config-volume\") pod \"coredns-6d4b75cb6d-zdl2m\" (UID: \"cf25b9ea-6823-4eff-b30d-36a09f9d897e\") " pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:44:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:10.640032  199956 atomic_writer.go:161] pod kube-system/coredns-6d4b75cb6d-zdl2m volume kube-api-access-tqzsm: no update required for target directory /var/lib/kubelet/pods/cf25b9ea-6823-4eff-b30d-36a09f9d897e/volumes/kubernetes.io~projected/kube-api-access-tqzsm
Jan 20 12:44:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:10.640108  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-tqzsm\" (UniqueName: \"kubernetes.io/projected/cf25b9ea-6823-4eff-b30d-36a09f9d897e-kube-api-access-tqzsm\") pod \"coredns-6d4b75cb6d-zdl2m\" (UID: \"cf25b9ea-6823-4eff-b30d-36a09f9d897e\") " pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:44:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:11.166622  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:44:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:11.166698  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:11.174657  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[db47a96e-2a52-470c-a37b-8c44ce0b411d] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:11 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ffa240 2 [] false false map[] 0xc0014c6200 0xc000aff3f0}
Jan 20 12:44:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:11.174711  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:11.483140  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:11.483214  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:11.491562  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[a5334ce6-ba06-4d95-a7d2-fed4dcdde054] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:11 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00129c1e0 2 [] false false map[] 0xc0021a2100 0xc0020c53f0}
Jan 20 12:44:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:11.491591  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:11.581520  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:44:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:11.588897  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:44:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:11.588962  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:44:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:11.591294  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:44:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:11.596729  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:44:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:11.596958  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:44:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:11.596992  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:44:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:11.597036  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:44:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:12.483904  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:12.483979  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:12.491627  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[b018bf52-086b-42e6-98c5-8c01ab99d15b] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:12 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0020fc720 2 [] false false map[] 0xc0014c6a00 0xc001e03340}
Jan 20 12:44:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:12.491659  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:12.689325  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:44:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:12.689387  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:12.690511  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:12 GMT]] 0xc00129c640 2 [] true false map[] 0xc001846a00 <nil>}
Jan 20 12:44:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:12.690615  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:44:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:12.695832  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:44:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:12.695893  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:12.696979  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:12 GMT]] 0xc0000a4080 2 [] true false map[] 0xc0014c6e00 <nil>}
Jan 20 12:44:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:12.697079  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:44:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:12.710302  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:44:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:12.710363  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:12.710364  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:44:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:12.710425  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:12.711474  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:12 GMT]] 0xc0020fc800 2 [] true false map[] 0xc0014c7000 <nil>}
Jan 20 12:44:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:12.711500  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:12 GMT]] 0xc0000a42c0 2 [] true false map[] 0xc0021a2400 <nil>}
Jan 20 12:44:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:12.711578  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:44:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:12.711603  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:44:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:12.843742  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:44:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:13.395585  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:44:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:13.395594  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:44:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:13.395812  199956 interface.go:209] Interface eno2 is up
Jan 20 12:44:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:13.395870  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:44:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:13.395878  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:44:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:13.395883  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:44:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:13.395886  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:44:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:13.395890  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:44:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:13.396283  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:44:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:13.396293  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:44:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:13.396297  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:44:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:13.396301  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:44:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:13.483665  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:13.483736  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:13.491590  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[3629586f-f2c9-48ac-8ca8-75e3ed9b802d] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:13 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ffb2a0 2 [] false false map[] 0xc0014c7300 0xc0008ef3f0}
Jan 20 12:44:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:13.491618  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:13.581512  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:44:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:13.587539  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:44:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:13.587552  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:44:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:13.587558  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:44:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:13.588355  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:44:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:13.588402  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:44:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:13.588408  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:44:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:13.588416  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:44:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:14.483547  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:14.483577  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:14.488328  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[fe8e2672-d0c9-4648-825b-e5c1cc82739a] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:14 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e8800 2 [] false false map[] 0xc001775000 0xc00141c000}
Jan 20 12:44:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:14.488390  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:15.483186  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:15.483261  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:15.490946  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[584e5628-916b-4386-a2d9-f897fde6a75c] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:15 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000979020 2 [] false false map[] 0xc001847800 0xc001645970}
Jan 20 12:44:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:15.491000  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:15.581922  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[confidential-containers-system/cc-operator-pre-install-daemon-qjplj]
Jan 20 12:44:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:15.582009  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:44:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:15.582150  199956 pod_workers.go:888] "Processing pod event" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" podUID=b0713fbc-efc5-4044-9d08-2326a0752f87 updateType=0
Jan 20 12:44:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:15.582229  199956 kubelet.go:1501] "syncPod enter" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" podUID=b0713fbc-efc5-4044-9d08-2326a0752f87
Jan 20 12:44:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:15.582267  199956 kubelet_pods.go:1435] "Generating pod status" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj"
Jan 20 12:44:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:15.582361  199956 kubelet_pods.go:1447] "Got phase for pod" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" oldPhase=Running phase=Running
Jan 20 12:44:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:15.582643  199956 status_manager.go:535] "Ignoring same status for pod" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:01 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:05 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:05 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:01 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.5 PodIPs:[{IP:10.244.0.5}] StartTime:2023-01-20 12:32:01 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:cc-runtime-pre-install-pod State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:32:03 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:quay.io/confidential-containers/container-engine-for-cc-payload:1034f9fcf947b22eea080a6f77d8e164e2369849 ImageID:quay.io/confidential-containers/container-engine-for-cc-payload@sha256:f86f078b3a47026a066e65c7d836d9b9a43bf177555c276624d90f42e50279a1 ContainerID:containerd://e0c5b991f81d5128566686552e45a9bbc8c584e4f6c94909edb3a42720dacc90 Started:0xc001ea7aee}] QOSClass:BestEffort EphemeralContainerStatuses:[]}
Jan 20 12:44:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:15.583001  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj"
Jan 20 12:44:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:15.583075  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj"
Jan 20 12:44:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:15.583460  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj"
Jan 20 12:44:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:15.583625  199956 kubelet.go:1503] "syncPod exit" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" podUID=b0713fbc-efc5-4044-9d08-2326a0752f87 isTerminal=false
Jan 20 12:44:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:15.583683  199956 pod_workers.go:988] "Processing pod event done" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" podUID=b0713fbc-efc5-4044-9d08-2326a0752f87 updateType=0
Jan 20 12:44:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:15.587598  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:44:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:15.587609  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:44:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:15.588390  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:44:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:15.588746  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:44:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:15.588792  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:44:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:15.588798  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:44:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:15.588806  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:44:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:15.659729  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" volumeName="confidential-containers-artifacts" volumeSpecName="confidential-containers-artifacts"
Jan 20 12:44:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:15.659836  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" volumeName="etc-systemd-system" volumeSpecName="etc-systemd-system"
Jan 20 12:44:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:15.659920  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" volumeName="dbus" volumeSpecName="dbus"
Jan 20 12:44:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:15.659986  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" volumeName="systemd" volumeSpecName="systemd"
Jan 20 12:44:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:15.660108  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" volumeName="kube-api-access-gcgm6" volumeSpecName="kube-api-access-gcgm6"
Jan 20 12:44:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:15.677965  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-gcgm6\" (UniqueName: \"kubernetes.io/projected/b0713fbc-efc5-4044-9d08-2326a0752f87-kube-api-access-gcgm6\") pod \"cc-operator-pre-install-daemon-qjplj\" (UID: \"b0713fbc-efc5-4044-9d08-2326a0752f87\") Volume is already mounted to pod, but remount was requested." pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj"
Jan 20 12:44:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:15.678146  199956 projected.go:183] Setting up volume kube-api-access-gcgm6 for pod b0713fbc-efc5-4044-9d08-2326a0752f87 at /var/lib/kubelet/pods/b0713fbc-efc5-4044-9d08-2326a0752f87/volumes/kubernetes.io~projected/kube-api-access-gcgm6
Jan 20 12:44:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:15.678553  199956 atomic_writer.go:161] pod confidential-containers-system/cc-operator-pre-install-daemon-qjplj volume kube-api-access-gcgm6: no update required for target directory /var/lib/kubelet/pods/b0713fbc-efc5-4044-9d08-2326a0752f87/volumes/kubernetes.io~projected/kube-api-access-gcgm6
Jan 20 12:44:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:15.678625  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-gcgm6\" (UniqueName: \"kubernetes.io/projected/b0713fbc-efc5-4044-9d08-2326a0752f87-kube-api-access-gcgm6\") pod \"cc-operator-pre-install-daemon-qjplj\" (UID: \"b0713fbc-efc5-4044-9d08-2326a0752f87\") " pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj"
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.164157  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.164235  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.171243  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:16 GMT] X-Content-Type-Options:[nosniff]] 0xc0003215c0 2 [] false false map[] 0xc000338a00 0xc0018a8d10}
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.171273  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.272075  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.272138  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.273234  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:16 GMT]] 0xc000979740 29 [] true false map[] 0xc001775300 <nil>}
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.273343  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.484240  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.484311  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.492622  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[4c8b3c23-929c-46f0-acd0-830c7ee2d592] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:16 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0009798c0 2 [] false false map[] 0xc000cf7500 0xc0018e0dc0}
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.492652  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.582126  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[kube-system/kube-proxy-prhfv]
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.582204  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/kube-proxy-prhfv" podUID=1c057a0e-3ee3-44f3-b294-434acc8f9491 updateType=0
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.582260  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/kube-proxy-prhfv" podUID=1c057a0e-3ee3-44f3-b294-434acc8f9491
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.582288  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/kube-proxy-prhfv"
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.582375  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/kube-proxy-prhfv" oldPhase=Running phase=Running
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.582652  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/kube-proxy-prhfv" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:43 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:43 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.239.159.53 PodIPs:[{IP:10.239.159.53}] StartTime:2023-01-20 12:31:42 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:kube-proxy State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:43 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:k8s.gcr.io/kube-proxy:v1.24.0 ImageID:k8s.gcr.io/kube-proxy@sha256:c957d602267fa61082ab8847914b2118955d0739d592cc7b01e278513478d6a8 ContainerID:containerd://f78ed441ff85d669a403a76c0987f2d86913431bd96d454b1a3605bdcf0474f2 Started:0xc0018f7799}] QOSClass:BestEffort EphemeralContainerStatuses:[]}
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.582977  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-proxy-prhfv"
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.583052  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/kube-proxy-prhfv"
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.583457  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/kube-proxy-prhfv"
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.583625  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/kube-proxy-prhfv" podUID=1c057a0e-3ee3-44f3-b294-434acc8f9491 isTerminal=false
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.583682  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/kube-proxy-prhfv" podUID=1c057a0e-3ee3-44f3-b294-434acc8f9491 updateType=0
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.667896  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-proxy-prhfv" volumeName="kube-proxy" volumeSpecName="kube-proxy"
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.668012  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-proxy-prhfv" volumeName="xtables-lock" volumeSpecName="xtables-lock"
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.668093  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-proxy-prhfv" volumeName="lib-modules" volumeSpecName="lib-modules"
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.668209  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-proxy-prhfv" volumeName="kube-api-access-2sbqp" volumeSpecName="kube-api-access-2sbqp"
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.686068  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/1c057a0e-3ee3-44f3-b294-434acc8f9491-kube-proxy\") pod \"kube-proxy-prhfv\" (UID: \"1c057a0e-3ee3-44f3-b294-434acc8f9491\") Volume is already mounted to pod, but remount was requested." pod="kube-system/kube-proxy-prhfv"
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.686201  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-2sbqp\" (UniqueName: \"kubernetes.io/projected/1c057a0e-3ee3-44f3-b294-434acc8f9491-kube-api-access-2sbqp\") pod \"kube-proxy-prhfv\" (UID: \"1c057a0e-3ee3-44f3-b294-434acc8f9491\") Volume is already mounted to pod, but remount was requested." pod="kube-system/kube-proxy-prhfv"
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.686342  199956 projected.go:183] Setting up volume kube-api-access-2sbqp for pod 1c057a0e-3ee3-44f3-b294-434acc8f9491 at /var/lib/kubelet/pods/1c057a0e-3ee3-44f3-b294-434acc8f9491/volumes/kubernetes.io~projected/kube-api-access-2sbqp
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.686390  199956 configmap.go:181] Setting up volume kube-proxy for pod 1c057a0e-3ee3-44f3-b294-434acc8f9491 at /var/lib/kubelet/pods/1c057a0e-3ee3-44f3-b294-434acc8f9491/volumes/kubernetes.io~configmap/kube-proxy
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.686473  199956 configmap.go:205] Received configMap kube-system/kube-proxy containing (2) pieces of data, 1458 total bytes
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.686552  199956 quota_linux.go:271] SupportsQuotas called, but quotas disabled
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.686750  199956 atomic_writer.go:161] pod kube-system/kube-proxy-prhfv volume kube-api-access-2sbqp: no update required for target directory /var/lib/kubelet/pods/1c057a0e-3ee3-44f3-b294-434acc8f9491/volumes/kubernetes.io~projected/kube-api-access-2sbqp
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.686815  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-2sbqp\" (UniqueName: \"kubernetes.io/projected/1c057a0e-3ee3-44f3-b294-434acc8f9491-kube-api-access-2sbqp\") pod \"kube-proxy-prhfv\" (UID: \"1c057a0e-3ee3-44f3-b294-434acc8f9491\") " pod="kube-system/kube-proxy-prhfv"
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.686827  199956 atomic_writer.go:161] pod kube-system/kube-proxy-prhfv volume kube-proxy: no update required for target directory /var/lib/kubelet/pods/1c057a0e-3ee3-44f3-b294-434acc8f9491/volumes/kubernetes.io~configmap/kube-proxy
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.686896  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/1c057a0e-3ee3-44f3-b294-434acc8f9491-kube-proxy\") pod \"kube-proxy-prhfv\" (UID: \"1c057a0e-3ee3-44f3-b294-434acc8f9491\") " pod="kube-system/kube-proxy-prhfv"
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.895342  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.895407  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.903012  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:16 GMT] X-Content-Type-Options:[nosniff]] 0xc0004c0980 2 [] false false map[] 0xc000cf7900 0xc00196a210}
Jan 20 12:44:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:16.903040  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:44:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:17.484101  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:17.484172  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:17.491834  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[cdddb349-d7b9-4448-bab2-cb8041628ec3] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:17 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000321b80 2 [] false false map[] 0xc000cf7f00 0xc001a194a0}
Jan 20 12:44:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:17.491864  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:17.581872  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:44:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:17.587524  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:44:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:17.587537  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:44:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:17.587544  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:44:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:17.588363  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:44:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:17.588428  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:44:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:17.588434  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:44:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:17.588443  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:44:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:17.845504  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:44:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:18.483204  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:18.483276  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:18.491343  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[f8861a69-cd69-4e12-842a-dd2c3bc60c80] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:18 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0000a5d60 2 [] false false map[] 0xc001774100 0xc000de60b0}
Jan 20 12:44:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:18.491374  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:18.603382  199956 reflector.go:536] vendor/k8s.io/client-go/informers/factory.go:134: Watch close - *v1.RuntimeClass total 0 items received
Jan 20 12:44:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:18.879035  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:44:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:18.879093  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/healthz" timeout="1s"
Jan 20 12:44:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:18.879105  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:18.879156  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:18.880279  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:18 GMT] X-Content-Type-Options:[nosniff]] 0xc0008e2140 2 [] true false map[] 0xc001a4a200 <nil>}
Jan 20 12:44:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:18.880288  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:18 GMT] X-Content-Type-Options:[nosniff]] 0xc0009a1a00 2 [] true false map[] 0xc001775200 <nil>}
Jan 20 12:44:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:18.880390  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:44:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:18.880411  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:44:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:19.483112  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:19.483186  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:19.491050  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[cfcbf3fc-4249-49ff-870c-e9f40a9dd091] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:19 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0009a1a60 2 [] false false map[] 0xc001a4a400 0xc0019826e0}
Jan 20 12:44:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:19.491132  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:19.581680  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:44:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:19.587518  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:44:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:19.587530  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:44:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:19.588342  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:44:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:19.588741  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:44:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:19.588786  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:44:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:19.588792  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:44:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:19.588801  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:44:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:20.200706  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:44:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:20.208578  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:44:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:20.218622  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64392288Ki" capacity="65061836Ki" time="2023-01-20 12:44:20.218568108 -0500 EST m=+772.774342932"
Jan 20 12:44:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:20.218635  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902002876Ki" capacity="981310056Ki" time="2023-01-20 12:44:20.202848414 -0500 EST m=+772.758623305"
Jan 20 12:44:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:20.218642  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:44:20.202848414 -0500 EST m=+772.758623305"
Jan 20 12:44:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:20.218648  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902002876Ki" capacity="981310056Ki" time="2023-01-20 12:44:11.261894554 -0500 EST"
Jan 20 12:44:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:20.218655  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:44:11.261894554 -0500 EST"
Jan 20 12:44:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:20.218661  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510775" capacity="511757" time="2023-01-20 12:44:20.218301022 -0500 EST m=+772.774075848"
Jan 20 12:44:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:20.218667  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59483132Ki" capacity="65586124Ki" time="2023-01-20 12:44:20.202848414 -0500 EST m=+772.758623305"
Jan 20 12:44:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:20.218692  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:44:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:20.483684  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:20.483700  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:20.487124  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[c0a6cf24-e1da-4e23-872b-44195da8ddf9] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:20 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00129dba0 2 [] false false map[] 0xc0021a2200 0xc0014989a0}
Jan 20 12:44:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:20.487158  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:21.166584  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:44:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:21.166657  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:21.174670  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[64e205be-9929-45e6-b39d-b686d73d7f9c] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:21 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00129dd00 2 [] false false map[] 0xc0023aa400 0xc001612840}
Jan 20 12:44:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:21.174700  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:21.483426  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:21.483489  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:21.496444  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[7dec609a-3b1f-405b-ba12-980ac83dc9d6] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:21 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008e3820 2 [] false false map[] 0xc0021a2400 0xc0017c3ef0}
Jan 20 12:44:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:21.496584  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:21.581224  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:44:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:21.588610  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:44:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:21.588674  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:44:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:21.590895  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:44:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:21.593010  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:44:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:21.593229  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:44:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:21.593261  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:44:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:21.593305  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:44:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:22.483360  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:22.483431  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:22.491305  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[7a8cee09-94e9-4520-b64d-138ea414e86c] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:22 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ffb5a0 2 [] false false map[] 0xc0023aa900 0xc001aaa0b0}
Jan 20 12:44:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:22.491334  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:22.689010  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:44:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:22.689080  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:22.690283  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:22 GMT]] 0xc0008e3c80 2 [] true false map[] 0xc0023aac00 <nil>}
Jan 20 12:44:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:22.690394  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:44:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:22.695608  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:44:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:22.695672  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:22.696619  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:22 GMT]] 0xc0012e8da0 2 [] true false map[] 0xc0023aae00 <nil>}
Jan 20 12:44:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:22.696745  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:44:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:22.709914  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:44:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:22.709977  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:22.709986  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:44:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:22.710048  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:22.710894  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:22 GMT]] 0xc0008e3e40 2 [] true false map[] 0xc0023ab000 <nil>}
Jan 20 12:44:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:22.711010  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:44:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:22.711039  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:22 GMT]] 0xc0012e8e20 2 [] true false map[] 0xc0021a2d00 <nil>}
Jan 20 12:44:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:22.711143  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:44:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:22.846432  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:44:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:23.483474  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:23.483546  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:23.491772  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[c56a1f26-2eac-4f30-8bd6-660f290e0e27] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:23 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ffb880 2 [] false false map[] 0xc0021a3600 0xc001c27ad0}
Jan 20 12:44:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:23.491804  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:23.581653  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:44:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:23.587554  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:44:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:23.587566  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:44:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:23.587573  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:44:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:23.588270  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:44:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:23.588329  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:44:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:23.588336  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:44:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:23.588360  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:44:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:23.592290  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:44:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:23.592296  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:44:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:23.592460  199956 interface.go:209] Interface eno2 is up
Jan 20 12:44:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:23.592488  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:44:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:23.592495  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:44:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:23.592500  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:44:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:23.592503  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:44:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:23.592507  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:44:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:23.592715  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:44:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:23.592722  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:44:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:23.592728  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:44:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:23.592732  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:44:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:24.483215  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:24.483286  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:24.491162  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[7c67a687-bbc1-46be-a63c-376f18df705e] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:24 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0004c12c0 2 [] false false map[] 0xc001717e00 0xc001e23c30}
Jan 20 12:44:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:24.491209  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:25.483147  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:25.483220  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:25.492114  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[db6bd93e-7c73-422d-ae4c-789f7fb85eb1] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:25 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0004c0980 2 [] false false map[] 0xc0016aa000 0xc002206160}
Jan 20 12:44:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:25.492151  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:25.582104  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:44:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:25.587618  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:44:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:25.587630  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:44:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:25.588326  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:44:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:25.588746  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:44:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:25.588796  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:44:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:25.588803  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:44:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:25.588811  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:44:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:26.164168  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:44:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:26.164238  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:26.172129  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:26 GMT] X-Content-Type-Options:[nosniff]] 0xc0000a49e0 2 [] false false map[] 0xc001a4a300 0xc0002b7ce0}
Jan 20 12:44:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:26.172158  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:44:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:26.272083  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:44:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:26.272142  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:26.273339  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:26 GMT]] 0xc0000a4c60 29 [] true false map[] 0xc0021a2300 <nil>}
Jan 20 12:44:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:26.273441  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:44:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:26.483684  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:26.483748  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:26.491638  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[f01ade1a-0f3f-44e6-ae0e-66dd93df867f] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:26 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0000a4d00 2 [] false false map[] 0xc001774100 0xc000de69a0}
Jan 20 12:44:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:26.491667  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:26.895822  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:44:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:26.895890  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:26.903255  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:26 GMT] X-Content-Type-Options:[nosniff]] 0xc0000a5980 2 [] false false map[] 0xc001a4a800 0xc000b1fd90}
Jan 20 12:44:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:26.903284  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.484149  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.484224  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.492893  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[1b6bafc4-5bb9-4a2f-b333-ce0e5db0ef50] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:27 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e8200 2 [] false false map[] 0xc001a4ab00 0xc0013b09a0}
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.492924  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.526254  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/etcd.yaml"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.528392  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver.yaml"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.531226  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.531723  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-scheduler.yaml"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.532005  199956 config.go:279] "Setting pods for source" source="file"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.540081  199956 kubelet_getters.go:176] "Pod status updated" pod="kube-system/etcd-zcy-z390-aorus-master" status=Running
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.540100  199956 kubelet_getters.go:176] "Pod status updated" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" status=Running
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.540104  199956 kubelet_getters.go:176] "Pod status updated" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" status=Running
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.540108  199956 kubelet_getters.go:176] "Pod status updated" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" status=Running
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.582071  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[kube-system/etcd-zcy-z390-aorus-master]
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.582158  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.582250  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d updateType=0
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.582329  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.582361  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/etcd-zcy-z390-aorus-master"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.582451  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/etcd-zcy-z390-aorus-master" oldPhase=Running phase=Running
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.583145  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/etcd-zcy-z390-aorus-master" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:28 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:37 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:37 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:28 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.239.159.53 PodIPs:[{IP:10.239.159.53}] StartTime:2023-01-20 12:31:28 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:etcd State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:22 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:6 Image:k8s.gcr.io/etcd:3.5.3-0 ImageID:k8s.gcr.io/etcd@sha256:13f53ed1d91e2e11aac476ee9a0269fdda6cc4874eba903efd40daf50c55eee5 ContainerID:containerd://9b11acac1b891eafc7ff2b244f1c42938f71a575ce81ff917e3b7ebf61d2e045 Started:0xc0014f68f6}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.583517  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/etcd-zcy-z390-aorus-master"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.583588  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/etcd-zcy-z390-aorus-master"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.584206  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/etcd-zcy-z390-aorus-master"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.584364  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d isTerminal=false
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.584419  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d updateType=0
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.587617  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.587628  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.587634  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.588445  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.588541  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.588548  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.588557  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629549  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/wpa_supplicant.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629559  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/wpa_supplicant.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629563  199956 factory.go:255] Factory "raw" can handle container "/system.slice/wpa_supplicant.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629570  199956 manager.go:925] ignoring container "/system.slice/wpa_supplicant.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629573  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-gtk\\x2dcommon\\x2dthemes-1535.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629576  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-gtk\\x2dcommon\\x2dthemes-1535.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629581  199956 manager.go:925] ignoring container "/system.slice/snap-gtk\\x2dcommon\\x2dthemes-1535.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629584  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-modprobe.slice"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629587  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-modprobe.slice"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629590  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-modprobe.slice", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629594  199956 manager.go:925] ignoring container "/system.slice/system-modprobe.slice"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629597  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-shm.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629602  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-shm.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629607  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-shm.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629610  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-7af065b7\\x2d9095\\x2d4d91\\x2d9b9e\\x2d2644e7b1f4bf-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dx6vjr.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629615  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-7af065b7\\x2d9095\\x2d4d91\\x2d9b9e\\x2d2644e7b1f4bf-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dx6vjr.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629621  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-7af065b7\\x2d9095\\x2d4d91\\x2d9b9e\\x2d2644e7b1f4bf-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dx6vjr.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629625  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/ModemManager.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629628  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/ModemManager.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629631  199956 factory.go:255] Factory "raw" can handle container "/system.slice/ModemManager.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629635  199956 manager.go:925] ignoring container "/system.slice/ModemManager.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629638  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dm-event.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629641  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/dm-event.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629644  199956 factory.go:255] Factory "raw" can handle container "/system.slice/dm-event.socket", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629648  199956 manager.go:925] ignoring container "/system.slice/dm-event.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629651  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-tmpfiles-setup.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629653  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-tmpfiles-setup.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629656  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-tmpfiles-setup.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629661  199956 manager.go:925] ignoring container "/system.slice/systemd-tmpfiles-setup.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629663  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-snap\\x2dstore-558.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629666  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-snap\\x2dstore-558.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629671  199956 manager.go:925] ignoring container "/system.slice/snap-snap\\x2dstore-558.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629674  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journald-dev-log.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629677  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journald-dev-log.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629680  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journald-dev-log.socket", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629684  199956 manager.go:925] ignoring container "/system.slice/systemd-journald-dev-log.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629687  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-snapd-ns.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629690  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-snapd-ns.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629694  199956 manager.go:925] ignoring container "/system.slice/run-snapd-ns.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629696  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snapd.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629699  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/snapd.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629702  199956 factory.go:255] Factory "raw" can handle container "/system.slice/snapd.socket", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629706  199956 manager.go:925] ignoring container "/system.slice/snapd.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629709  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-udevd.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629712  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-udevd.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629715  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-udevd.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629719  199956 manager.go:925] ignoring container "/system.slice/systemd-udevd.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629723  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/uuidd.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629726  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/uuidd.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629729  199956 factory.go:255] Factory "raw" can handle container "/system.slice/uuidd.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629732  199956 manager.go:925] ignoring container "/system.slice/uuidd.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629735  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e0c5b991f81d5128566686552e45a9bbc8c584e4f6c94909edb3a42720dacc90-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629739  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e0c5b991f81d5128566686552e45a9bbc8c584e4f6c94909edb3a42720dacc90-rootfs.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629744  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e0c5b991f81d5128566686552e45a9bbc8c584e4f6c94909edb3a42720dacc90-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629747  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629751  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-rootfs.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629756  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629760  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journald-audit.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629763  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journald-audit.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629766  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journald-audit.socket", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629770  199956 manager.go:925] ignoring container "/system.slice/systemd-journald-audit.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629773  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/session-42.scope"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629775  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/session-42.scope"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629779  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/session-42.scope", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629783  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/session-42.scope"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629786  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/user@0.service/dbus.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629788  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/user@0.service/dbus.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629792  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/user@0.service/dbus.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629796  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/user@0.service/dbus.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629799  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/NetworkManager.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629801  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/NetworkManager.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629804  199956 factory.go:255] Factory "raw" can handle container "/system.slice/NetworkManager.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629808  199956 manager.go:925] ignoring container "/system.slice/NetworkManager.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629811  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-d94e1927\\x2dd22d\\x2d4831\\x2da764\\x2d0170eae346a1-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d9qh7j.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629816  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-d94e1927\\x2dd22d\\x2d4831\\x2da764\\x2d0170eae346a1-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d9qh7j.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629821  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-d94e1927\\x2dd22d\\x2d4831\\x2da764\\x2d0170eae346a1-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d9qh7j.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629825  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/cron.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629828  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/cron.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629831  199956 factory.go:255] Factory "raw" can handle container "/system.slice/cron.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629835  199956 manager.go:925] ignoring container "/system.slice/cron.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629838  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-initctl.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629841  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-initctl.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629844  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-initctl.socket", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629848  199956 manager.go:925] ignoring container "/system.slice/systemd-initctl.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629851  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/user-runtime-dir@0.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629854  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/user-runtime-dir@0.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629857  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/user-runtime-dir@0.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629861  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/user-runtime-dir@0.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629864  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/lvm2-lvmpolld.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629867  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/lvm2-lvmpolld.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629870  199956 factory.go:255] Factory "raw" can handle container "/system.slice/lvm2-lvmpolld.socket", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629874  199956 manager.go:925] ignoring container "/system.slice/lvm2-lvmpolld.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629876  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-modules-load.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629879  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-modules-load.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629883  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-modules-load.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629887  199956 manager.go:925] ignoring container "/system.slice/systemd-modules-load.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629890  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-logind.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629892  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-logind.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629895  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-logind.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629899  199956 manager.go:925] ignoring container "/system.slice/systemd-logind.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629902  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e489181a7f95431b6d92f037dbe3f788b46a5e024df7aaf29e0115dab1168ab1-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629906  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e489181a7f95431b6d92f037dbe3f788b46a5e024df7aaf29e0115dab1168ab1-rootfs.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629911  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e489181a7f95431b6d92f037dbe3f788b46a5e024df7aaf29e0115dab1168ab1-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629914  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journald.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629917  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journald.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629920  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journald.socket", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629924  199956 manager.go:925] ignoring container "/system.slice/systemd-journald.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629927  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-core20-1611.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629930  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-core20-1611.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629934  199956 manager.go:925] ignoring container "/system.slice/snap-core20-1611.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629937  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/irqbalance.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629940  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/irqbalance.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629943  199956 factory.go:255] Factory "raw" can handle container "/system.slice/irqbalance.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629947  199956 manager.go:925] ignoring container "/system.slice/irqbalance.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629950  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journald.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629953  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journald.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629956  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journald.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629961  199956 manager.go:925] ignoring container "/system.slice/systemd-journald.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629964  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/avahi-daemon.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629967  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/avahi-daemon.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629970  199956 factory.go:255] Factory "raw" can handle container "/system.slice/avahi-daemon.socket", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629974  199956 manager.go:925] ignoring container "/system.slice/avahi-daemon.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629976  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/ssh.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629979  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/ssh.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629982  199956 factory.go:255] Factory "raw" can handle container "/system.slice/ssh.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629986  199956 manager.go:925] ignoring container "/system.slice/ssh.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629988  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-kernel-config.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629992  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-kernel-config.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629996  199956 manager.go:925] ignoring container "/system.slice/sys-kernel-config.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.629999  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-systemd\\x2dfsck.slice"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630002  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-systemd\\x2dfsck.slice"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630005  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-systemd\\x2dfsck.slice", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630009  199956 manager.go:925] ignoring container "/system.slice/system-systemd\\x2dfsck.slice"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630013  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e10efaa459a32161059fda75d4bcaf3bbdb896781f772b508e43fe00bc7c9003-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630017  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e10efaa459a32161059fda75d4bcaf3bbdb896781f772b508e43fe00bc7c9003-rootfs.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630022  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e10efaa459a32161059fda75d4bcaf3bbdb896781f772b508e43fe00bc7c9003-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630025  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-shm.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630029  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-shm.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630034  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-shm.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630037  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630040  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630043  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630047  199956 manager.go:925] ignoring container "/user.slice/user-0.slice"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630050  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/thermald.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630052  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/thermald.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630056  199956 factory.go:255] Factory "raw" can handle container "/system.slice/thermald.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630060  199956 manager.go:925] ignoring container "/system.slice/thermald.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630062  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/NetworkManager-wait-online.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630065  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/NetworkManager-wait-online.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630069  199956 factory.go:255] Factory "raw" can handle container "/system.slice/NetworkManager-wait-online.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630073  199956 manager.go:925] ignoring container "/system.slice/NetworkManager-wait-online.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630076  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-shm.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630080  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-shm.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630084  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-shm.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630088  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/acpid.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630091  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/acpid.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630094  199956 factory.go:255] Factory "raw" can handle container "/system.slice/acpid.socket", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630098  199956 manager.go:925] ignoring container "/system.slice/acpid.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630100  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/rtkit-daemon.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630103  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/rtkit-daemon.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630106  199956 factory.go:255] Factory "raw" can handle container "/system.slice/rtkit-daemon.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630110  199956 manager.go:925] ignoring container "/system.slice/rtkit-daemon.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630113  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-1cb44bdbde0b41852e382d7dab7c184af8dd4e5b25d5cfe6a10a9c8ab601659d-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630117  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-1cb44bdbde0b41852e382d7dab7c184af8dd4e5b25d5cfe6a10a9c8ab601659d-rootfs.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630122  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-1cb44bdbde0b41852e382d7dab7c184af8dd4e5b25d5cfe6a10a9c8ab601659d-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630125  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-lvm2\\x2dpvscan.slice/lvm2-pvscan@8:10.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630128  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-lvm2\\x2dpvscan.slice/lvm2-pvscan@8:10.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630131  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-lvm2\\x2dpvscan.slice/lvm2-pvscan@8:10.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630136  199956 manager.go:925] ignoring container "/system.slice/system-lvm2\\x2dpvscan.slice/lvm2-pvscan@8:10.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630139  199956 factory.go:262] Factory "containerd" was unable to handle container "/docker"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630142  199956 factory.go:262] Factory "systemd" was unable to handle container "/docker"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630145  199956 factory.go:255] Factory "raw" can handle container "/docker", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630149  199956 manager.go:925] ignoring container "/docker"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630151  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630155  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-rootfs.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630160  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630164  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/apport.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630166  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/apport.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630169  199956 factory.go:255] Factory "raw" can handle container "/system.slice/apport.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630173  199956 manager.go:925] ignoring container "/system.slice/apport.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630176  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-1000.slice/user-runtime-dir@1000.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630179  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-1000.slice/user-runtime-dir@1000.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630182  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-1000.slice/user-runtime-dir@1000.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630186  199956 manager.go:925] ignoring container "/user.slice/user-1000.slice/user-runtime-dir@1000.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630189  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/user@0.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630192  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/user@0.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630195  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/user@0.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630199  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/user@0.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630202  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2dd678d9bc\\x2d8fcb\\x2d9fbe\\x2d1142\\x2ddede54862bab.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630206  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2dd678d9bc\\x2d8fcb\\x2d9fbe\\x2d1142\\x2ddede54862bab.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630210  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2dd678d9bc\\x2d8fcb\\x2d9fbe\\x2d1142\\x2ddede54862bab.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630213  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f78ed441ff85d669a403a76c0987f2d86913431bd96d454b1a3605bdcf0474f2-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630218  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f78ed441ff85d669a403a76c0987f2d86913431bd96d454b1a3605bdcf0474f2-rootfs.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630222  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f78ed441ff85d669a403a76c0987f2d86913431bd96d454b1a3605bdcf0474f2-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630226  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-04e472cb\\x2db6f9\\x2d4065\\x2db509\\x2dd7e1579fc48d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dhqj8d.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630230  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-04e472cb\\x2db6f9\\x2d4065\\x2db509\\x2dd7e1579fc48d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dhqj8d.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630236  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-04e472cb\\x2db6f9\\x2d4065\\x2db509\\x2dd7e1579fc48d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dhqj8d.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630240  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-shm.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630244  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-shm.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630248  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-shm.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630252  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f8443d2868215ec42d3fa335663976e33df166c5e98369aa3f9d7ddb9235bead-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630256  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f8443d2868215ec42d3fa335663976e33df166c5e98369aa3f9d7ddb9235bead-rootfs.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630261  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f8443d2868215ec42d3fa335663976e33df166c5e98369aa3f9d7ddb9235bead-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630264  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/colord.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630267  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/colord.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630270  199956 factory.go:255] Factory "raw" can handle container "/system.slice/colord.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630274  199956 manager.go:925] ignoring container "/system.slice/colord.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630277  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/gdm.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630280  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/gdm.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630283  199956 factory.go:255] Factory "raw" can handle container "/system.slice/gdm.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630287  199956 manager.go:925] ignoring container "/system.slice/gdm.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630289  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/kerneloops.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630292  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/kerneloops.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630296  199956 factory.go:255] Factory "raw" can handle container "/system.slice/kerneloops.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630300  199956 manager.go:925] ignoring container "/system.slice/kerneloops.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630302  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-shm.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630306  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-shm.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630311  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-shm.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630315  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/lvm2-monitor.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630317  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/lvm2-monitor.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630320  199956 factory.go:255] Factory "raw" can handle container "/system.slice/lvm2-monitor.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630324  199956 manager.go:925] ignoring container "/system.slice/lvm2-monitor.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630327  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-fs-fuse-connections.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630331  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-fs-fuse-connections.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630335  199956 manager.go:925] ignoring container "/system.slice/sys-fs-fuse-connections.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630337  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/ufw.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630340  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/ufw.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630343  199956 factory.go:255] Factory "raw" can handle container "/system.slice/ufw.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630347  199956 manager.go:925] ignoring container "/system.slice/ufw.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630350  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/kmod-static-nodes.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630352  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/kmod-static-nodes.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630356  199956 factory.go:255] Factory "raw" can handle container "/system.slice/kmod-static-nodes.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630360  199956 manager.go:925] ignoring container "/system.slice/kmod-static-nodes.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630363  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-1000.slice/user@1000.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630366  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-1000.slice/user@1000.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630369  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-1000.slice/user@1000.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630373  199956 manager.go:925] ignoring container "/user.slice/user-1000.slice/user@1000.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630375  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/console-setup.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630378  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/console-setup.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630382  199956 factory.go:255] Factory "raw" can handle container "/system.slice/console-setup.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630386  199956 manager.go:925] ignoring container "/system.slice/console-setup.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630388  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-shm.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630392  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-shm.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630397  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-shm.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630401  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/uuidd.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630403  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/uuidd.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630406  199956 factory.go:255] Factory "raw" can handle container "/system.slice/uuidd.socket", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630410  199956 manager.go:925] ignoring container "/system.slice/uuidd.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630413  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/containerd.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630416  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/containerd.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630419  199956 factory.go:255] Factory "raw" can handle container "/system.slice/containerd.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630423  199956 manager.go:925] ignoring container "/system.slice/containerd.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630425  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-shm.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630429  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-shm.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630434  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-shm.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630438  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-shm.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630442  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-shm.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630447  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-shm.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630451  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-snapd-ns-snap\\x2dstore.mnt.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630454  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-snapd-ns-snap\\x2dstore.mnt.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630458  199956 manager.go:925] ignoring container "/system.slice/run-snapd-ns-snap\\x2dstore.mnt.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630461  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-b0713fbc\\x2defc5\\x2d4044\\x2d9d08\\x2d2326a0752f87-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dgcgm6.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630465  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-b0713fbc\\x2defc5\\x2d4044\\x2d9d08\\x2d2326a0752f87-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dgcgm6.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630471  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-b0713fbc\\x2defc5\\x2d4044\\x2d9d08\\x2d2326a0752f87-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dgcgm6.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630475  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-timesyncd.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630478  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-timesyncd.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630481  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-timesyncd.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630485  199956 manager.go:925] ignoring container "/system.slice/systemd-timesyncd.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630488  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/blk-availability.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630491  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/blk-availability.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630494  199956 factory.go:255] Factory "raw" can handle container "/system.slice/blk-availability.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630498  199956 manager.go:925] ignoring container "/system.slice/blk-availability.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630501  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630504  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630507  199956 factory.go:255] Factory "raw" can handle container "/system.slice", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630510  199956 manager.go:925] ignoring container "/system.slice"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630513  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/switcheroo-control.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630516  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/switcheroo-control.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630519  199956 factory.go:255] Factory "raw" can handle container "/system.slice/switcheroo-control.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630523  199956 manager.go:925] ignoring container "/system.slice/switcheroo-control.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630526  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-0f9ae677fe935afcf43e145281deae0d663ba95fda6759ff24f0dd3e1cee17a9-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630530  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-0f9ae677fe935afcf43e145281deae0d663ba95fda6759ff24f0dd3e1cee17a9-rootfs.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630534  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-0f9ae677fe935afcf43e145281deae0d663ba95fda6759ff24f0dd3e1cee17a9-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630538  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-machine-id-commit.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630541  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-machine-id-commit.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630544  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-machine-id-commit.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630548  199956 manager.go:925] ignoring container "/system.slice/systemd-machine-id-commit.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630551  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/alsa-restore.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630554  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/alsa-restore.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630557  199956 factory.go:255] Factory "raw" can handle container "/system.slice/alsa-restore.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630561  199956 manager.go:925] ignoring container "/system.slice/alsa-restore.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630563  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/session-40.scope"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630566  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/session-40.scope"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630569  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/session-40.scope", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630573  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/session-40.scope"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630577  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-5900ac5c1383a321a6ae837b57d6d29d7a660a102c0806210a9ea57290673062-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630581  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-5900ac5c1383a321a6ae837b57d6d29d7a660a102c0806210a9ea57290673062-rootfs.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630586  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-5900ac5c1383a321a6ae837b57d6d29d7a660a102c0806210a9ea57290673062-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630590  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2d719b045b\\x2df019\\x2d025c\\x2d185d\\x2da976163f375a.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630593  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2d719b045b\\x2df019\\x2d025c\\x2d185d\\x2da976163f375a.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630598  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2d719b045b\\x2df019\\x2d025c\\x2d185d\\x2da976163f375a.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630601  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/keyboard-setup.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630604  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/keyboard-setup.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630607  199956 factory.go:255] Factory "raw" can handle container "/system.slice/keyboard-setup.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630611  199956 manager.go:925] ignoring container "/system.slice/keyboard-setup.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630614  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/cups-browsed.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630617  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/cups-browsed.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630620  199956 factory.go:255] Factory "raw" can handle container "/system.slice/cups-browsed.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630623  199956 manager.go:925] ignoring container "/system.slice/cups-browsed.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630626  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/udisks2.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630629  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/udisks2.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630632  199956 factory.go:255] Factory "raw" can handle container "/system.slice/udisks2.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630636  199956 manager.go:925] ignoring container "/system.slice/udisks2.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630638  199956 factory.go:262] Factory "containerd" was unable to handle container "/init.scope"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630641  199956 factory.go:262] Factory "systemd" was unable to handle container "/init.scope"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630644  199956 factory.go:255] Factory "raw" can handle container "/init.scope", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630648  199956 manager.go:925] ignoring container "/init.scope"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630650  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/cups.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630653  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/cups.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630656  199956 factory.go:255] Factory "raw" can handle container "/system.slice/cups.socket", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630660  199956 manager.go:925] ignoring container "/system.slice/cups.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630662  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-tmpfiles-setup-dev.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630665  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-tmpfiles-setup-dev.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630669  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-tmpfiles-setup-dev.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630673  199956 manager.go:925] ignoring container "/system.slice/systemd-tmpfiles-setup-dev.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630675  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630680  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-rootfs.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630685  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630689  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/acpid.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630691  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/acpid.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630694  199956 factory.go:255] Factory "raw" can handle container "/system.slice/acpid.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630698  199956 manager.go:925] ignoring container "/system.slice/acpid.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630701  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-shm.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630705  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-shm.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630710  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-shm.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630714  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-udevd-kernel.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630716  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-udevd-kernel.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630720  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-udevd-kernel.socket", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630724  199956 manager.go:925] ignoring container "/system.slice/systemd-udevd-kernel.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630727  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dbus.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630729  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/dbus.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630732  199956 factory.go:255] Factory "raw" can handle container "/system.slice/dbus.socket", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630736  199956 manager.go:925] ignoring container "/system.slice/dbus.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630739  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/proc-sys-fs-binfmt_misc.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630742  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/proc-sys-fs-binfmt_misc.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630747  199956 manager.go:925] ignoring container "/system.slice/proc-sys-fs-binfmt_misc.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630749  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-1c057a0e\\x2d3ee3\\x2d44f3\\x2db294\\x2d434acc8f9491-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d2sbqp.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630754  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-1c057a0e\\x2d3ee3\\x2d44f3\\x2db294\\x2d434acc8f9491-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d2sbqp.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630759  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-1c057a0e\\x2d3ee3\\x2d44f3\\x2db294\\x2d434acc8f9491-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d2sbqp.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630763  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dev-hugepages.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630766  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/dev-hugepages.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630770  199956 manager.go:925] ignoring container "/system.slice/dev-hugepages.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630773  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-udev-trigger.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630776  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-udev-trigger.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630779  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-udev-trigger.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630783  199956 manager.go:925] ignoring container "/system.slice/systemd-udev-trigger.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630786  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630790  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-rootfs.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630795  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630799  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-update-utmp.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630802  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-update-utmp.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630805  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-update-utmp.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630809  199956 manager.go:925] ignoring container "/system.slice/systemd-update-utmp.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630812  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630816  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-rootfs.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630821  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630824  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/upower.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630827  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/upower.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630830  199956 factory.go:255] Factory "raw" can handle container "/system.slice/upower.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630834  199956 manager.go:925] ignoring container "/system.slice/upower.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630837  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-1000.slice/session-1.scope"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630839  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-1000.slice/session-1.scope"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630842  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-1000.slice/session-1.scope", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630847  199956 manager.go:925] ignoring container "/user.slice/user-1000.slice/session-1.scope"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630849  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630853  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-rootfs.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630858  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630862  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snapd.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630864  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/snapd.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630868  199956 factory.go:255] Factory "raw" can handle container "/system.slice/snapd.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630872  199956 manager.go:925] ignoring container "/system.slice/snapd.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630874  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-random-seed.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630877  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-random-seed.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630880  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-random-seed.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630884  199956 manager.go:925] ignoring container "/system.slice/systemd-random-seed.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630887  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snapd.apparmor.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630890  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/snapd.apparmor.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630893  199956 factory.go:255] Factory "raw" can handle container "/system.slice/snapd.apparmor.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630897  199956 manager.go:925] ignoring container "/system.slice/snapd.apparmor.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630899  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-kernel-debug-tracing.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630903  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-kernel-debug-tracing.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630907  199956 manager.go:925] ignoring container "/system.slice/sys-kernel-debug-tracing.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630910  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-user-sessions.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630913  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-user-sessions.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630916  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-user-sessions.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630920  199956 manager.go:925] ignoring container "/system.slice/systemd-user-sessions.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630922  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/polkit.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630925  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/polkit.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630928  199956 factory.go:255] Factory "raw" can handle container "/system.slice/polkit.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630932  199956 manager.go:925] ignoring container "/system.slice/polkit.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630935  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/networkd-dispatcher.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630938  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/networkd-dispatcher.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630941  199956 factory.go:255] Factory "raw" can handle container "/system.slice/networkd-dispatcher.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630945  199956 manager.go:925] ignoring container "/system.slice/networkd-dispatcher.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630948  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dev-mqueue.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630951  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/dev-mqueue.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630955  199956 manager.go:925] ignoring container "/system.slice/dev-mqueue.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630957  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/boot-efi.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630960  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/boot-efi.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630965  199956 manager.go:925] ignoring container "/system.slice/boot-efi.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630967  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630971  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-rootfs.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630976  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630980  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630982  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630985  199956 factory.go:255] Factory "raw" can handle container "/user.slice", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630989  199956 manager.go:925] ignoring container "/user.slice"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630992  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.630996  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-rootfs.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631001  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631004  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b3af3b285c950aba4fcd0176473261f7f4700aeaafe9c73ae15b15a7d6304092-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631008  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b3af3b285c950aba4fcd0176473261f7f4700aeaafe9c73ae15b15a7d6304092-rootfs.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631017  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b3af3b285c950aba4fcd0176473261f7f4700aeaafe9c73ae15b15a7d6304092-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631021  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2d58469e7a\\x2dc331\\x2d785c\\x2dc760\\x2d93c7232334bd.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631025  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2d58469e7a\\x2dc331\\x2d785c\\x2dc760\\x2d93c7232334bd.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631029  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2d58469e7a\\x2dc331\\x2d785c\\x2dc760\\x2d93c7232334bd.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631033  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/syslog.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631035  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/syslog.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631038  199956 factory.go:255] Factory "raw" can handle container "/system.slice/syslog.socket", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631042  199956 manager.go:925] ignoring container "/system.slice/syslog.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631045  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-user-0.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631048  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-user-0.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631052  199956 manager.go:925] ignoring container "/system.slice/run-user-0.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631055  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/cups.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631058  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/cups.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631061  199956 factory.go:255] Factory "raw" can handle container "/system.slice/cups.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631065  199956 manager.go:925] ignoring container "/system.slice/cups.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631068  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-1000.slice"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631071  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-1000.slice"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631074  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-1000.slice", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631078  199956 manager.go:925] ignoring container "/user.slice/user-1000.slice"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631080  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-user-1000-gvfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631083  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-user-1000-gvfs.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631087  199956 manager.go:925] ignoring container "/system.slice/run-user-1000-gvfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631090  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-resolved.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631093  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-resolved.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631096  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-resolved.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631100  199956 manager.go:925] ignoring container "/system.slice/systemd-resolved.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631103  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/session-44.scope"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631106  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/session-44.scope"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631109  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/session-44.scope", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631113  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/session-44.scope"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631116  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-sysctl.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631119  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-sysctl.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631122  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-sysctl.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631126  199956 manager.go:925] ignoring container "/system.slice/systemd-sysctl.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631129  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/accounts-daemon.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631132  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/accounts-daemon.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631135  199956 factory.go:255] Factory "raw" can handle container "/system.slice/accounts-daemon.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631139  199956 manager.go:925] ignoring container "/system.slice/accounts-daemon.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631141  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2d8b7da23a\\x2d3511\\x2dfe06\\x2d72d7\\x2d26a549eb607d.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631145  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2d8b7da23a\\x2d3511\\x2dfe06\\x2d72d7\\x2d26a549eb607d.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631149  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2d8b7da23a\\x2d3511\\x2dfe06\\x2d72d7\\x2d26a549eb607d.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631153  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-systemd\\x2dfsck.slice/systemd-fsck@dev-disk-by\\x2duuid-4B5A\\x2dDC89.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631157  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-systemd\\x2dfsck.slice/systemd-fsck@dev-disk-by\\x2duuid-4B5A\\x2dDC89.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631161  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-systemd\\x2dfsck.slice/systemd-fsck@dev-disk-by\\x2duuid-4B5A\\x2dDC89.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631165  199956 manager.go:925] ignoring container "/system.slice/system-systemd\\x2dfsck.slice/systemd-fsck@dev-disk-by\\x2duuid-4B5A\\x2dDC89.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631168  199956 factory.go:262] Factory "containerd" was unable to handle container "/kata_overhead"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631171  199956 factory.go:262] Factory "systemd" was unable to handle container "/kata_overhead"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631174  199956 factory.go:255] Factory "raw" can handle container "/kata_overhead", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631178  199956 manager.go:925] ignoring container "/kata_overhead"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631181  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-shm.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631184  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-shm.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631189  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-shm.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631193  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-fsckd.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631196  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-fsckd.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631199  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-fsckd.socket", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631202  199956 manager.go:925] ignoring container "/system.slice/systemd-fsckd.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631205  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/avahi-daemon.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631208  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/avahi-daemon.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631211  199956 factory.go:255] Factory "raw" can handle container "/system.slice/avahi-daemon.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631215  199956 manager.go:925] ignoring container "/system.slice/avahi-daemon.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631218  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-13149330f3752d0ff65bcac86c7b522b2040811e815fbc87e56b40b612875d5a-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631222  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-13149330f3752d0ff65bcac86c7b522b2040811e815fbc87e56b40b612875d5a-rootfs.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631227  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-13149330f3752d0ff65bcac86c7b522b2040811e815fbc87e56b40b612875d5a-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631230  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-shm.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631234  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-shm.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631239  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-shm.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631243  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-sysusers.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631246  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-sysusers.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631249  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-sysusers.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631253  199956 manager.go:925] ignoring container "/system.slice/systemd-sysusers.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631256  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journal-flush.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631259  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journal-flush.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631262  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journal-flush.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631266  199956 manager.go:925] ignoring container "/system.slice/systemd-journal-flush.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631269  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631273  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-rootfs.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631278  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631281  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-rfkill.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631284  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-rfkill.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631287  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-rfkill.socket", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631292  199956 manager.go:925] ignoring container "/system.slice/systemd-rfkill.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631295  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/rsyslog.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631298  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/rsyslog.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631301  199956 factory.go:255] Factory "raw" can handle container "/system.slice/rsyslog.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631305  199956 manager.go:925] ignoring container "/system.slice/rsyslog.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631308  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-186424b47c7b9022ecfe8996dc73cd9101f7539259cd65914279c84c9a02a288-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631312  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-186424b47c7b9022ecfe8996dc73cd9101f7539259cd65914279c84c9a02a288-rootfs.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631317  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-186424b47c7b9022ecfe8996dc73cd9101f7539259cd65914279c84c9a02a288-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631320  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-bare-5.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631323  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-bare-5.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631327  199956 manager.go:925] ignoring container "/system.slice/snap-bare-5.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631330  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-kernel-debug.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631333  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-kernel-debug.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631337  199956 manager.go:925] ignoring container "/system.slice/sys-kernel-debug.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631340  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/unattended-upgrades.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631342  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/unattended-upgrades.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631345  199956 factory.go:255] Factory "raw" can handle container "/system.slice/unattended-upgrades.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631353  199956 manager.go:925] ignoring container "/system.slice/unattended-upgrades.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631356  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dbus.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631358  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/dbus.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631361  199956 factory.go:255] Factory "raw" can handle container "/system.slice/dbus.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631365  199956 manager.go:925] ignoring container "/system.slice/dbus.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631368  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-udevd-control.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631370  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-udevd-control.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631374  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-udevd-control.socket", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631378  199956 manager.go:925] ignoring container "/system.slice/systemd-udevd-control.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631381  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/whoopsie.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631383  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/whoopsie.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631386  199956 factory.go:255] Factory "raw" can handle container "/system.slice/whoopsie.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631390  199956 manager.go:925] ignoring container "/system.slice/whoopsie.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631393  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/-.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631396  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/-.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631401  199956 manager.go:925] ignoring container "/system.slice/-.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631404  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631408  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-rootfs.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631413  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631416  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-d2688d45\\x2d2487\\x2d46e7\\x2daecb\\x2de3479626909d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d4pnfq.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631421  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-d2688d45\\x2d2487\\x2d46e7\\x2daecb\\x2de3479626909d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d4pnfq.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631426  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-d2688d45\\x2d2487\\x2d46e7\\x2daecb\\x2de3479626909d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d4pnfq.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631430  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/docker.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631433  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/docker.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631436  199956 factory.go:255] Factory "raw" can handle container "/system.slice/docker.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631440  199956 manager.go:925] ignoring container "/system.slice/docker.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631442  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2d562f9e70\\x2db2b7\\x2d3ac4\\x2d2311\\x2db91510a5a9ac.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631446  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2d562f9e70\\x2db2b7\\x2d3ac4\\x2d2311\\x2db91510a5a9ac.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631450  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2d562f9e70\\x2db2b7\\x2d3ac4\\x2d2311\\x2db91510a5a9ac.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631454  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/docker.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631456  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/docker.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631459  199956 factory.go:255] Factory "raw" can handle container "/system.slice/docker.socket", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631463  199956 manager.go:925] ignoring container "/system.slice/docker.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631466  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/setvtrgb.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631469  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/setvtrgb.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631472  199956 factory.go:255] Factory "raw" can handle container "/system.slice/setvtrgb.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631476  199956 manager.go:925] ignoring container "/system.slice/setvtrgb.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631478  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/bluetooth.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631481  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/bluetooth.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631484  199956 factory.go:255] Factory "raw" can handle container "/system.slice/bluetooth.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631488  199956 manager.go:925] ignoring container "/system.slice/bluetooth.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631493  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-snapd-16292.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631496  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-snapd-16292.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631500  199956 manager.go:925] ignoring container "/system.slice/snap-snapd-16292.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631503  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631507  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-rootfs.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631512  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631517  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-kernel-tracing.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631520  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-kernel-tracing.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631524  199956 manager.go:925] ignoring container "/system.slice/sys-kernel-tracing.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631527  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/apparmor.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631529  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/apparmor.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631532  199956 factory.go:255] Factory "raw" can handle container "/system.slice/apparmor.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631536  199956 manager.go:925] ignoring container "/system.slice/apparmor.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631539  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-lvm2\\x2dpvscan.slice"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631542  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-lvm2\\x2dpvscan.slice"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631545  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-lvm2\\x2dpvscan.slice", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631549  199956 manager.go:925] ignoring container "/system.slice/system-lvm2\\x2dpvscan.slice"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631551  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-getty.slice"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631554  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-getty.slice"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631557  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-getty.slice", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631561  199956 manager.go:925] ignoring container "/system.slice/system-getty.slice"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631564  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/user@0.service/dbus.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631567  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/user@0.service/dbus.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631570  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/user@0.service/dbus.socket", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631574  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/user@0.service/dbus.socket"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631577  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-gnome\\x2d3\\x2d38\\x2d2004-115.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631580  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-gnome\\x2d3\\x2d38\\x2d2004-115.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631584  199956 manager.go:925] ignoring container "/system.slice/snap-gnome\\x2d3\\x2d38\\x2d2004-115.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631587  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-remount-fs.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631590  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-remount-fs.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631593  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-remount-fs.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631597  199956 manager.go:925] ignoring container "/system.slice/systemd-remount-fs.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631600  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-9b11acac1b891eafc7ff2b244f1c42938f71a575ce81ff917e3b7ebf61d2e045-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631605  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-9b11acac1b891eafc7ff2b244f1c42938f71a575ce81ff917e3b7ebf61d2e045-rootfs.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631610  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-9b11acac1b891eafc7ff2b244f1c42938f71a575ce81ff917e3b7ebf61d2e045-rootfs.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631614  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/openvpn.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631617  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/openvpn.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631620  199956 factory.go:255] Factory "raw" can handle container "/system.slice/openvpn.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631625  199956 manager.go:925] ignoring container "/system.slice/openvpn.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631628  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-cf25b9ea\\x2d6823\\x2d4eff\\x2db30d\\x2d36a09f9d897e-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dtqzsm.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631633  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-cf25b9ea\\x2d6823\\x2d4eff\\x2db30d\\x2d36a09f9d897e-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dtqzsm.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631638  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-cf25b9ea\\x2d6823\\x2d4eff\\x2db30d\\x2d36a09f9d897e-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dtqzsm.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631642  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snapd.seeded.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631645  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/snapd.seeded.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631648  199956 factory.go:255] Factory "raw" can handle container "/system.slice/snapd.seeded.service", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631652  199956 manager.go:925] ignoring container "/system.slice/snapd.seeded.service"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631654  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-user-1000.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631658  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-user-1000.mount", but ignoring.
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.631662  199956 manager.go:925] ignoring container "/system.slice/run-user-1000.mount"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.646083  199956 qos_container_manager_linux.go:379] "Updated QoS cgroup configuration"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.648154  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/etcd-zcy-z390-aorus-master" volumeName="etcd-certs" volumeSpecName="etcd-certs"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.648174  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/etcd-zcy-z390-aorus-master" volumeName="etcd-data" volumeSpecName="etcd-data"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.675755  199956 kubelet.go:1280] "Container garbage collection succeeded"
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.764202  199956 handler.go:293] error while reading "/proc/199956/fd/34" link: readlink /proc/199956/fd/34: no such file or directory
Jan 20 12:44:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:27.848192  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:44:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:28.483376  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:28.483442  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:28.491968  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[a666aa54-1254-4c15-ab81-ea43c09a304c] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:28 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00129cc80 2 [] false false map[] 0xc0014edc00 0xc0016c4370}
Jan 20 12:44:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:28.491996  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:28.879352  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:44:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:28.879420  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:28.880618  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:28 GMT] X-Content-Type-Options:[nosniff]] 0xc00129d400 2 [] true false map[] 0xc0022c8400 <nil>}
Jan 20 12:44:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:28.880739  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:44:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:29.484182  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:29.484257  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:29.492759  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[f15b5944-1a5f-4f1c-b1c6-a4a79194456a] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:29 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0009a01e0 2 [] false false map[] 0xc001a4a100 0xc00224c0b0}
Jan 20 12:44:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:29.492786  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:29.581650  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:44:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:29.588996  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:44:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:29.589058  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:44:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:29.591358  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:44:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:29.593389  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:44:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:29.593611  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:44:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:29.593643  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:44:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:29.593687  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:44:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:30.219129  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:44:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:30.226751  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:44:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:30.236677  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59475724Ki" capacity="65586124Ki" time="2023-01-20 12:44:30.221299803 -0500 EST m=+782.777074695"
Jan 20 12:44:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:30.236694  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64392144Ki" capacity="65061836Ki" time="2023-01-20 12:44:30.236616779 -0500 EST m=+782.792391604"
Jan 20 12:44:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:30.236702  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902002720Ki" capacity="981310056Ki" time="2023-01-20 12:44:30.221299803 -0500 EST m=+782.777074695"
Jan 20 12:44:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:30.236708  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:44:30.221299803 -0500 EST m=+782.777074695"
Jan 20 12:44:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:30.236714  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902002720Ki" capacity="981310056Ki" time="2023-01-20 12:44:21.261877844 -0500 EST"
Jan 20 12:44:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:30.236720  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:44:21.261877844 -0500 EST"
Jan 20 12:44:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:30.236726  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510776" capacity="511757" time="2023-01-20 12:44:30.236346064 -0500 EST m=+782.792120889"
Jan 20 12:44:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:30.236752  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:44:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:30.483707  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:30.483770  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:30.491595  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[246f3360-54bd-4965-a35e-e9518c8e12dd] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:30 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000cb3b00 2 [] false false map[] 0xc001a4a500 0xc000de7970}
Jan 20 12:44:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:30.491626  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:31.166948  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:44:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:31.167020  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:31.174476  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[7869ba78-8d3c-4b3d-a236-4e40e8b0e50d] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:31 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000320200 2 [] false false map[] 0xc0014c6300 0xc000fb8c60}
Jan 20 12:44:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:31.174508  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:31.483480  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:31.483548  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:31.491852  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[eb83ed6c-1832-4b43-a09e-0abfcff3ecba] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:31 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0020fca80 2 [] false false map[] 0xc0014c6900 0xc00110cd10}
Jan 20 12:44:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:31.491882  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:31.581693  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:44:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:31.587587  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:44:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:31.587599  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:44:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:31.587606  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:44:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:31.588331  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:44:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:31.588376  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:44:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:31.588382  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:44:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:31.588391  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:44:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:32.483234  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:32.483307  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:32.491934  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[8e251c5f-a918-4511-93e1-0d6b35f0b857] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:32 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e84c0 2 [] false false map[] 0xc001774100 0xc00105f760}
Jan 20 12:44:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:32.491964  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:32.689780  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:44:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:32.689851  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:32.691046  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:32 GMT]] 0xc000320840 2 [] true false map[] 0xc0002c3800 <nil>}
Jan 20 12:44:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:32.691158  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:44:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:32.696379  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:44:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:32.696440  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:32.697511  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:32 GMT]] 0xc0012e8680 2 [] true false map[] 0xc001a4aa00 <nil>}
Jan 20 12:44:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:32.697615  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:44:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:32.710180  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:44:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:32.710239  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:32.710257  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:44:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:32.710318  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:32.711255  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:32 GMT]] 0xc0014862a0 2 [] true false map[] 0xc001774e00 <nil>}
Jan 20 12:44:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:32.711322  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:32 GMT]] 0xc0012e8760 2 [] true false map[] 0xc001a4ac00 <nil>}
Jan 20 12:44:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:32.711368  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:44:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:32.711426  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:44:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:32.850089  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:44:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:33.483832  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:33.483905  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:33.491809  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[000cc3af-74d5-408f-a1b4-1e6bb49172f0] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:33 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000320ee0 2 [] false false map[] 0xc0000dc400 0xc0015fbc30}
Jan 20 12:44:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:33.491839  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:33.581259  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:44:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:33.588585  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:44:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:33.588645  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:44:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:33.590839  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:44:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:33.592855  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:44:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:33.593075  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:44:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:33.593106  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:44:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:33.593149  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:44:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:33.834233  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:44:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:33.834242  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:44:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:33.834457  199956 interface.go:209] Interface eno2 is up
Jan 20 12:44:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:33.834483  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:44:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:33.834491  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:44:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:33.834496  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:44:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:33.834499  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:44:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:33.834503  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:44:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:33.834828  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:44:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:33.834836  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:44:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:33.834839  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:44:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:33.834845  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:44:34 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:34.483476  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:34 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:34.483544  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:34 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:34.491858  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[9bfdec6f-0c77-4104-a58d-be57204dd8cf] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:34 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000978680 2 [] false false map[] 0xc000fcdf00 0xc0018afa20}
Jan 20 12:44:34 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:34.491889  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:35.483401  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:35.483476  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:35.491783  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[e75b5528-ee46-4f6a-98d9-d6da3c86b28d] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:35 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008ddba0 2 [] false false map[] 0xc0023ab800 0xc001c6f4a0}
Jan 20 12:44:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:35.491814  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:35.581600  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:44:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:35.587573  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:44:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:35.587586  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:44:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:35.587592  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:44:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:35.588311  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:44:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:35.588379  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:44:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:35.588385  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:44:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:35.588393  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.165060  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.165133  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.172225  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:36 GMT] X-Content-Type-Options:[nosniff]] 0xc0012e9980 2 [] false false map[] 0xc0023abb00 0xc002081290}
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.172255  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.272098  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.272156  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.273365  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:36 GMT]] 0xc0012e9a00 29 [] true false map[] 0xc0023abe00 <nil>}
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.273472  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.483219  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.483280  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.491066  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[a76c86e1-ecc2-4c5d-9c51-56469e4c9501] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:36 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e9c40 2 [] false false map[] 0xc00105a800 0xc001e7b970}
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.491090  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.581913  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=2 pods=[kube-flannel/kube-flannel-ds-hprn4 kube-system/coredns-6d4b75cb6d-48zl2]
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.581942  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 updateType=0
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.581961  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.581970  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.581981  199956 pod_workers.go:888] "Processing pod event" pod="kube-flannel/kube-flannel-ds-hprn4" podUID=04e472cb-b6f9-4065-b509-d7e1579fc48d updateType=0
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.581998  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/coredns-6d4b75cb6d-48zl2" oldPhase=Running phase=Running
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.582000  199956 kubelet.go:1501] "syncPod enter" pod="kube-flannel/kube-flannel-ds-hprn4" podUID=04e472cb-b6f9-4065-b509-d7e1579fc48d
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.582010  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.582037  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-flannel/kube-flannel-ds-hprn4" oldPhase=Running phase=Running
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.582085  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/coredns-6d4b75cb6d-48zl2" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:45 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:45 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.2 PodIPs:[{IP:10.244.0.2}] StartTime:2023-01-20 12:31:42 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:coredns State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:43 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:k8s.gcr.io/coredns/coredns:v1.8.6 ImageID:k8s.gcr.io/coredns/coredns@sha256:5b6ec0d6de9baaf3e92d0f66cd96a25b9edbce8716f5f15dcd1a616b3abd590e ContainerID:containerd://b3af3b285c950aba4fcd0176473261f7f4700aeaafe9c73ae15b15a7d6304092 Started:0xc00013fd8e}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.582173  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.582193  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.582274  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-flannel/kube-flannel-ds-hprn4" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:44 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:45 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:45 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.239.159.53 PodIPs:[{IP:10.239.159.53}] StartTime:2023-01-20 12:31:42 -0500 EST InitContainerStatuses:[{Name:install-cni-plugin State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2023-01-20 12:31:43 -0500 EST,FinishedAt:2023-01-20 12:31:43 -0500 EST,ContainerID:containerd://349eee1bf5a2d95206979822c956ea6f555dbb786434c7b9fe46524872f1a18d,}} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:docker.io/rancher/mirrored-flannelcni-flannel-cni-plugin:v1.1.0 ImageID:docker.io/rancher/mirrored-flannelcni-flannel-cni-plugin@sha256:28d3a6be9f450282bf42e4dad143d41da23e3d91f66f19c01ee7fd21fd17cb2b ContainerID:containerd://349eee1bf5a2d95206979822c956ea6f555dbb786434c7b9fe46524872f1a18d Started:<nil>} {Name:install-cni State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2023-01-20 12:31:43 -0500 EST,FinishedAt:2023-01-20 12:31:43 -0500 EST,ContainerID:containerd://85a4ede91a47b8d7df192dc39004a2a7fa45e095191a7335eaed8ec826cd9f36,}} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:docker.io/rancher/mirrored-flannelcni-flannel:v0.19.1 ImageID:docker.io/rancher/mirrored-flannelcni-flannel@sha256:e3b0f06121b0f7a11a684e910bba89b3ab49cd6e73aeb6c719f83b2456946366 ContainerID:containerd://85a4ede91a47b8d7df192dc39004a2a7fa45e095191a7335eaed8ec826cd9f36 Started:<nil>}] ContainerStatuses:[{Name:kube-flannel State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:44 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:docker.io/rancher/mirrored-flannelcni-flannel:v0.19.1 ImageID:docker.io/rancher/mirrored-flannelcni-flannel@sha256:e3b0f06121b0f7a11a684e910bba89b3ab49cd6e73aeb6c719f83b2456946366 ContainerID:containerd://0f9ae677fe935afcf43e145281deae0d663ba95fda6759ff24f0dd3e1cee17a9 Started:0xc00147fb6e}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.582319  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.582366  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 isTerminal=false
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.582372  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.582381  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 updateType=0
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.582391  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.582487  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.582527  199956 kubelet.go:1503] "syncPod exit" pod="kube-flannel/kube-flannel-ds-hprn4" podUID=04e472cb-b6f9-4065-b509-d7e1579fc48d isTerminal=false
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.582539  199956 pod_workers.go:988] "Processing pod event done" pod="kube-flannel/kube-flannel-ds-hprn4" podUID=04e472cb-b6f9-4065-b509-d7e1579fc48d updateType=0
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.611441  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="run" volumeSpecName="run"
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.611540  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="cni-plugin" volumeSpecName="cni-plugin"
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.611617  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="cni" volumeSpecName="cni"
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.611698  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="flannel-cfg" volumeSpecName="flannel-cfg"
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.611764  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="xtables-lock" volumeSpecName="xtables-lock"
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.611896  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="kube-api-access-hqj8d" volumeSpecName="kube-api-access-hqj8d"
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.612043  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/coredns-6d4b75cb6d-48zl2" volumeName="config-volume" volumeSpecName="config-volume"
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.612174  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/coredns-6d4b75cb6d-48zl2" volumeName="kube-api-access-9qh7j" volumeSpecName="kube-api-access-9qh7j"
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.635647  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"flannel-cfg\" (UniqueName: \"kubernetes.io/configmap/04e472cb-b6f9-4065-b509-d7e1579fc48d-flannel-cfg\") pod \"kube-flannel-ds-hprn4\" (UID: \"04e472cb-b6f9-4065-b509-d7e1579fc48d\") Volume is already mounted to pod, but remount was requested." pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.635765  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/d94e1927-d22d-4831-a764-0170eae346a1-config-volume\") pod \"coredns-6d4b75cb6d-48zl2\" (UID: \"d94e1927-d22d-4831-a764-0170eae346a1\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.635848  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-hqj8d\" (UniqueName: \"kubernetes.io/projected/04e472cb-b6f9-4065-b509-d7e1579fc48d-kube-api-access-hqj8d\") pod \"kube-flannel-ds-hprn4\" (UID: \"04e472cb-b6f9-4065-b509-d7e1579fc48d\") Volume is already mounted to pod, but remount was requested." pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.635904  199956 configmap.go:181] Setting up volume flannel-cfg for pod 04e472cb-b6f9-4065-b509-d7e1579fc48d at /var/lib/kubelet/pods/04e472cb-b6f9-4065-b509-d7e1579fc48d/volumes/kubernetes.io~configmap/flannel-cfg
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.635970  199956 configmap.go:181] Setting up volume config-volume for pod d94e1927-d22d-4831-a764-0170eae346a1 at /var/lib/kubelet/pods/d94e1927-d22d-4831-a764-0170eae346a1/volumes/kubernetes.io~configmap/config-volume
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.636000  199956 projected.go:183] Setting up volume kube-api-access-hqj8d for pod 04e472cb-b6f9-4065-b509-d7e1579fc48d at /var/lib/kubelet/pods/04e472cb-b6f9-4065-b509-d7e1579fc48d/volumes/kubernetes.io~projected/kube-api-access-hqj8d
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.636054  199956 configmap.go:205] Received configMap kube-system/coredns containing (1) pieces of data, 339 total bytes
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.636131  199956 quota_linux.go:271] SupportsQuotas called, but quotas disabled
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.635984  199956 configmap.go:205] Received configMap kube-flannel/kube-flannel-cfg containing (2) pieces of data, 365 total bytes
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.636276  199956 quota_linux.go:271] SupportsQuotas called, but quotas disabled
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.635922  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-9qh7j\" (UniqueName: \"kubernetes.io/projected/d94e1927-d22d-4831-a764-0170eae346a1-kube-api-access-9qh7j\") pod \"coredns-6d4b75cb6d-48zl2\" (UID: \"d94e1927-d22d-4831-a764-0170eae346a1\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.636381  199956 atomic_writer.go:161] pod kube-system/coredns-6d4b75cb6d-48zl2 volume config-volume: no update required for target directory /var/lib/kubelet/pods/d94e1927-d22d-4831-a764-0170eae346a1/volumes/kubernetes.io~configmap/config-volume
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.636486  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/d94e1927-d22d-4831-a764-0170eae346a1-config-volume\") pod \"coredns-6d4b75cb6d-48zl2\" (UID: \"d94e1927-d22d-4831-a764-0170eae346a1\") " pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.636485  199956 projected.go:183] Setting up volume kube-api-access-9qh7j for pod d94e1927-d22d-4831-a764-0170eae346a1 at /var/lib/kubelet/pods/d94e1927-d22d-4831-a764-0170eae346a1/volumes/kubernetes.io~projected/kube-api-access-9qh7j
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.636499  199956 atomic_writer.go:161] pod kube-flannel/kube-flannel-ds-hprn4 volume kube-api-access-hqj8d: no update required for target directory /var/lib/kubelet/pods/04e472cb-b6f9-4065-b509-d7e1579fc48d/volumes/kubernetes.io~projected/kube-api-access-hqj8d
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.636580  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-hqj8d\" (UniqueName: \"kubernetes.io/projected/04e472cb-b6f9-4065-b509-d7e1579fc48d-kube-api-access-hqj8d\") pod \"kube-flannel-ds-hprn4\" (UID: \"04e472cb-b6f9-4065-b509-d7e1579fc48d\") " pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.636595  199956 atomic_writer.go:161] pod kube-flannel/kube-flannel-ds-hprn4 volume flannel-cfg: no update required for target directory /var/lib/kubelet/pods/04e472cb-b6f9-4065-b509-d7e1579fc48d/volumes/kubernetes.io~configmap/flannel-cfg
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.636668  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"flannel-cfg\" (UniqueName: \"kubernetes.io/configmap/04e472cb-b6f9-4065-b509-d7e1579fc48d-flannel-cfg\") pod \"kube-flannel-ds-hprn4\" (UID: \"04e472cb-b6f9-4065-b509-d7e1579fc48d\") " pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.636946  199956 atomic_writer.go:161] pod kube-system/coredns-6d4b75cb6d-48zl2 volume kube-api-access-9qh7j: no update required for target directory /var/lib/kubelet/pods/d94e1927-d22d-4831-a764-0170eae346a1/volumes/kubernetes.io~projected/kube-api-access-9qh7j
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.637016  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-9qh7j\" (UniqueName: \"kubernetes.io/projected/d94e1927-d22d-4831-a764-0170eae346a1-kube-api-access-9qh7j\") pod \"coredns-6d4b75cb6d-48zl2\" (UID: \"d94e1927-d22d-4831-a764-0170eae346a1\") " pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.895765  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.895856  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.903146  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:36 GMT] X-Content-Type-Options:[nosniff]] 0xc000ce8ae0 2 [] false false map[] 0xc001717900 0xc002129290}
Jan 20 12:44:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:36.903186  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:44:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:37.483226  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:37.483293  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:37.491767  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[375779fe-ffb9-42d5-8298-9107fd80a093] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:37 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001718000 2 [] false false map[] 0xc001a4a100 0xc0019824d0}
Jan 20 12:44:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:37.491798  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:37.581556  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:44:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:37.587515  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:44:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:37.587527  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:44:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:37.588234  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:44:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:37.589390  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:44:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:37.589440  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:44:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:37.589446  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:44:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:37.589455  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:44:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:37.851491  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:44:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:38.483618  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:38.483688  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:38.492143  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[775c8037-a6fa-4c83-9f08-3100ec0555e8] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:38 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00129c1e0 2 [] false false map[] 0xc001a4a800 0xc0002b6fd0}
Jan 20 12:44:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:38.492175  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:38.879355  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:44:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:38.879407  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:38.879411  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/healthz" timeout="1s"
Jan 20 12:44:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:38.879456  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:38.880345  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:38 GMT] X-Content-Type-Options:[nosniff]] 0xc0002be2a0 2 [] true false map[] 0xc0023aa500 <nil>}
Jan 20 12:44:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:38.880393  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:38 GMT] X-Content-Type-Options:[nosniff]] 0xc00129c440 2 [] true false map[] 0xc001717900 <nil>}
Jan 20 12:44:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:38.880426  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:44:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:38.880472  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:44:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:39.484055  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:39.484125  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:39.491850  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[e4c79cbd-21e1-4ed6-ae03-cbca3acd14df] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:39 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123f020 2 [] false false map[] 0xc0023aa700 0xc002081970}
Jan 20 12:44:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:39.491884  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:39.581825  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:44:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:39.587626  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:44:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:39.587639  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:44:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:39.588480  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:44:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:39.588842  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:44:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:39.588889  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:44:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:39.588895  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:44:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:39.588903  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:44:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:40.237788  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:44:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:40.246021  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:44:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:40.256106  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902002668Ki" capacity="981310056Ki" time="2023-01-20 12:44:40.239986921 -0500 EST m=+792.795761814"
Jan 20 12:44:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:40.256119  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:44:40.239986921 -0500 EST m=+792.795761814"
Jan 20 12:44:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:40.256125  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902002668Ki" capacity="981310056Ki" time="2023-01-20 12:44:31.262282262 -0500 EST"
Jan 20 12:44:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:40.256131  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:44:31.262282262 -0500 EST"
Jan 20 12:44:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:40.256137  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510778" capacity="511757" time="2023-01-20 12:44:40.255782493 -0500 EST m=+792.811557318"
Jan 20 12:44:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:40.256143  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59464532Ki" capacity="65586124Ki" time="2023-01-20 12:44:40.239986921 -0500 EST m=+792.795761814"
Jan 20 12:44:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:40.256162  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64392316Ki" capacity="65061836Ki" time="2023-01-20 12:44:40.256049155 -0500 EST m=+792.811823980"
Jan 20 12:44:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:40.256191  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:44:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:40.483364  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:40.483432  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:40.491933  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[28ad0c54-3bc0-482d-bf6a-65ad59cb3410] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:40 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123ffe0 2 [] false false map[] 0xc001a4ab00 0xc001050e70}
Jan 20 12:44:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:40.491961  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:41.166819  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:44:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:41.166893  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:41.179233  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[72d8c376-c3c6-4b8c-b7cb-0fdf19e6820d] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:41 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0020fd840 2 [] false false map[] 0xc001a4ae00 0xc00183eb00}
Jan 20 12:44:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:41.179371  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:41.483942  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:41.484013  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:41.491849  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[bff147f3-2c91-4824-bf50-0b79b99a867b] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:41 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00129d440 2 [] false false map[] 0xc001a4b200 0xc001942bb0}
Jan 20 12:44:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:41.491880  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:41.581824  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:44:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:41.587610  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:44:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:41.587622  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:44:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:41.587628  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:44:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:41.588438  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:44:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:41.588484  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:44:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:41.588491  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:44:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:41.588499  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:44:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:41.946862  199956 handler.go:293] error while reading "/proc/199956/fd/34" link: readlink /proc/199956/fd/34: no such file or directory
Jan 20 12:44:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:42.483509  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:42.483582  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:42.492750  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[4a5fdb7f-907e-44b7-82da-913ba6fe1fe8] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:42 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0000a4000 2 [] false false map[] 0xc000339b00 0xc001942d10}
Jan 20 12:44:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:42.492792  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:42.689715  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:44:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:42.689790  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:42.690862  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:42 GMT]] 0xc00129c020 2 [] true false map[] 0xc0016aa100 <nil>}
Jan 20 12:44:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:42.690971  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:44:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:42.696224  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:44:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:42.696282  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:42.697341  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:42 GMT]] 0xc00129c060 2 [] true false map[] 0xc0016aa400 <nil>}
Jan 20 12:44:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:42.697444  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:44:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:42.709583  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:44:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:42.709643  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:44:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:42.709646  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:42.709701  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:42.710793  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:42 GMT]] 0xc0008e23a0 2 [] true false map[] 0xc0023aa100 <nil>}
Jan 20 12:44:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:42.710861  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:42 GMT]] 0xc000ffa160 2 [] true false map[] 0xc0016ab000 <nil>}
Jan 20 12:44:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:42.710903  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:44:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:42.710971  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:44:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:42.853394  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:44:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:43.483938  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:43.484002  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:43.491701  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[0d8e4e7b-089f-44f0-903f-13a06cb6ad05] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:43 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0000a5800 2 [] false false map[] 0xc000339700 0xc0013602c0}
Jan 20 12:44:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:43.491730  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:43.581402  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:44:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:43.587526  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:44:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:43.587538  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:44:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:43.588261  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:44:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:43.588769  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:44:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:43.588815  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:44:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:43.588822  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:44:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:43.588831  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:44:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:44.131440  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:44:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:44.131448  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:44:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:44.131634  199956 interface.go:209] Interface eno2 is up
Jan 20 12:44:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:44.131661  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:44:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:44.131668  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:44:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:44.131673  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:44:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:44.131676  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:44:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:44.131680  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:44:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:44.131935  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:44:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:44.131958  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:44:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:44.131981  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:44:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:44.131984  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:44:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:44.484041  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:44.484106  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:44.491872  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[9a0510f3-a730-489c-a182-f31fcd6f63bf] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:44 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ce8040 2 [] false false map[] 0xc000339900 0xc0006efd90}
Jan 20 12:44:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:44.491903  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:45.483602  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:45.483675  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:45.491902  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[2ecd0775-2e0b-4f70-9b22-3d394cac1e20] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:45 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0017196c0 2 [] false false map[] 0xc0023aba00 0xc001120580}
Jan 20 12:44:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:45.491934  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:45.581435  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:44:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:45.587536  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:44:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:45.587549  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:44:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:45.588147  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:44:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:45.588474  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:44:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:45.588518  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:44:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:45.588526  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:44:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:45.588534  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:44:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:46.165035  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:44:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:46.165109  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:46.172346  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:46 GMT] X-Content-Type-Options:[nosniff]] 0xc0004c0a40 2 [] false false map[] 0xc000cf6200 0xc00138fce0}
Jan 20 12:44:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:46.172396  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:44:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:46.272270  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:44:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:46.272334  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:46.273587  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:46 GMT]] 0xc000cb3500 29 [] true false map[] 0xc0014ecc00 <nil>}
Jan 20 12:44:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:46.273689  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:44:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:46.483393  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:46.483448  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:46.492779  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[72e69c77-f59c-41a2-8b6e-186571afed0c] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:46 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001703f60 2 [] false false map[] 0xc0023abe00 0xc001120790}
Jan 20 12:44:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:46.492816  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:46.895051  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:44:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:46.895122  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:46.902338  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:46 GMT] X-Content-Type-Options:[nosniff]] 0xc000cb3680 2 [] false false map[] 0xc001716100 0xc00161a580}
Jan 20 12:44:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:46.902367  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:44:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:47.300551  199956 reflector.go:536] object-"kube-flannel"/"kube-root-ca.crt": Watch close - *v1.ConfigMap total 0 items received
Jan 20 12:44:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:47.484071  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:47.484134  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:47.497159  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[fdaac496-8ccc-4213-b94c-8664c79c3305] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:47 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001719c80 2 [] false false map[] 0xc00105b500 0xc0016be370}
Jan 20 12:44:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:47.497289  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:47.526096  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/etcd.yaml"
Jan 20 12:44:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:47.528243  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver.yaml"
Jan 20 12:44:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:47.531188  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Jan 20 12:44:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:47.531717  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-scheduler.yaml"
Jan 20 12:44:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:47.531999  199956 config.go:279] "Setting pods for source" source="file"
Jan 20 12:44:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:47.581555  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:44:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:47.585390  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:44:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:47.585423  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:44:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:47.585439  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:44:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:47.586813  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:44:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:47.586964  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:44:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:47.586986  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:44:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:47.587013  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:44:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:47.854928  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:44:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:48.483110  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:48.483174  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:48.490867  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[de077b58-0eee-4024-a516-560b3a58a937] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:48 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0003208e0 2 [] false false map[] 0xc000cf7c00 0xc001bf4f20}
Jan 20 12:44:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:48.490918  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:48.879304  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:44:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:48.879372  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:48.880537  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:48 GMT] X-Content-Type-Options:[nosniff]] 0xc000320f40 2 [] true false map[] 0xc00105b800 <nil>}
Jan 20 12:44:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:48.880660  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:44:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:49.484221  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:49.484293  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:49.491928  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[adc3dacb-7fd0-4040-bb2d-cd84abba527e] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:49 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000978660 2 [] false false map[] 0xc000cf7f00 0xc001d44e70}
Jan 20 12:44:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:49.491961  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:49.581896  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:44:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:49.587642  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:44:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:49.587653  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:44:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:49.588514  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:44:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:49.588852  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:44:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:49.588895  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:44:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:49.588901  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:44:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:49.588910  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:44:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:50.257263  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:44:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:50.264997  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:44:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:50.276097  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902002640Ki" capacity="981310056Ki" time="2023-01-20 12:44:50.259392915 -0500 EST m=+802.815167806"
Jan 20 12:44:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:50.276116  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:44:50.259392915 -0500 EST m=+802.815167806"
Jan 20 12:44:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:50.276123  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902002640Ki" capacity="981310056Ki" time="2023-01-20 12:44:41.262545314 -0500 EST"
Jan 20 12:44:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:50.276135  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:44:41.262545314 -0500 EST"
Jan 20 12:44:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:50.276142  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510778" capacity="511757" time="2023-01-20 12:44:50.275675773 -0500 EST m=+802.831450598"
Jan 20 12:44:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:50.276149  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59469996Ki" capacity="65586124Ki" time="2023-01-20 12:44:50.259392915 -0500 EST m=+802.815167806"
Jan 20 12:44:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:50.276155  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64401704Ki" capacity="65061836Ki" time="2023-01-20 12:44:50.276021633 -0500 EST m=+802.831796459"
Jan 20 12:44:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:50.276188  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:44:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:50.484169  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:50.484243  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:50.491880  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[9d959632-a165-4373-b4c2-3aa44aa7a9d2] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:50 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000320200 2 [] false false map[] 0xc0016aa100 0xc001e14160}
Jan 20 12:44:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:50.491912  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:51.166167  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:44:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:51.166234  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:51.174135  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[c641b7fb-95dc-49dc-8bd7-d2b952232ec7] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:51 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000320860 2 [] false false map[] 0xc0016ab000 0xc000fd98c0}
Jan 20 12:44:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:51.174164  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:51.483919  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:51.483991  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:51.491725  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[adbe2ff3-dfaf-43ef-bc16-e8f715dfa1fe] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:51 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000320b00 2 [] false false map[] 0xc00105a800 0xc0002b69a0}
Jan 20 12:44:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:51.491760  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:51.581618  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:44:51 zcy-Z390-AORUS-MASTER kata[207475]: time="2023-01-20T12:44:51.586861124-05:00" level=error msg="agent pull image err. context deadline exceeded" name=containerd-shim-v2 pid=207475 sandbox=f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 source=virtcontainers subsystem=kata_agent
Jan 20 12:44:51 zcy-Z390-AORUS-MASTER kata[207475]: time="2023-01-20T12:44:51.587020568-05:00" level=error msg="kata runtime PullImage err. context deadline exceeded" name=containerd-shim-v2 pid=207475 sandbox=f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 source=containerd-kata-shim-v2
Jan 20 12:44:51 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:44:51.586861124-05:00" level=error msg="agent pull image err. context deadline exceeded" name=containerd-shim-v2 pid=207475 sandbox=f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 source=virtcontainers subsystem=kata_agent
Jan 20 12:44:51 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:44:51.587020568-05:00" level=error msg="kata runtime PullImage err. context deadline exceeded" name=containerd-shim-v2 pid=207475 sandbox=f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 source=containerd-kata-shim-v2
Jan 20 12:44:51 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:44:51.587394813-05:00" level=error msg="PullImage \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\" failed" error="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
Jan 20 12:44:51 zcy-Z390-AORUS-MASTER kubelet[199956]: E0120 12:44:51.587532  199956 remote_image.go:238] "PullImage from image service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" image="ngcn-registry.sh.intel.com:443/ci-example256m:latest"
Jan 20 12:44:51 zcy-Z390-AORUS-MASTER kubelet[199956]: E0120 12:44:51.587583  199956 kuberuntime_image.go:51] "Failed to pull image" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" image="ngcn-registry.sh.intel.com:443/ci-example256m:latest"
Jan 20 12:44:51 zcy-Z390-AORUS-MASTER kubelet[199956]: E0120 12:44:51.587670  199956 kuberuntime_manager.go:905] container &Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod unsigned-unencrypted-cc-1_default(4f97314d-815b-4787-b174-5c158cd28c9d): ErrImagePull: rpc error: code = DeadlineExceeded desc = context deadline exceeded
Jan 20 12:44:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:51.587683  199956 kubelet.go:1503] "syncPod exit" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d isTerminal=false
Jan 20 12:44:51 zcy-Z390-AORUS-MASTER kubelet[199956]: E0120 12:44:51.587707  199956 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ci-example256m\" with ErrImagePull: \"rpc error: code = DeadlineExceeded desc = context deadline exceeded\"" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:44:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:51.587718  199956 pod_workers.go:988] "Processing pod event done" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:44:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:51.587729  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Warning" reason="Failed" message="Failed to pull image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\": rpc error: code = DeadlineExceeded desc = context deadline exceeded"
Jan 20 12:44:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:51.587734  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Warning" reason="Failed" message="Error: ErrImagePull"
Jan 20 12:44:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:51.587782  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:44:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:51.587799  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:44:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:51.588331  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:44:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:51.588671  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:44:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:51.588719  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:44:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:51.588725  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:44:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:51.588734  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.483269  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.483338  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.491260  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[26c1ed2a-3361-4799-b4ba-0e66820b4234] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:52 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00129c9e0 2 [] false false map[] 0xc001f37b00 0xc00138e840}
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.491291  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kata[207475]: time="2023-01-20T12:44:52.591787567-05:00" level=warning msg="notify on errors" error="failed to ping agent: Failed to Check if grpc server is working: context deadline exceeded" sandbox=f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 source=virtcontainers subsystem=virtcontainers/monitor
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kata[207475]: time="2023-01-20T12:44:52.591965276-05:00" level=warning msg="write error to watcher" error="failed to ping agent: Failed to Check if grpc server is working: context deadline exceeded" sandbox=f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 source=virtcontainers subsystem=virtcontainers/monitor
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:44:52.591787567-05:00" level=warning msg="notify on errors" error="failed to ping agent: Failed to Check if grpc server is working: context deadline exceeded" sandbox=f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 source=virtcontainers subsystem=virtcontainers/monitor
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:44:52.591965276-05:00" level=warning msg="write error to watcher" error="failed to ping agent: Failed to Check if grpc server is working: context deadline exceeded" sandbox=f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 source=virtcontainers subsystem=virtcontainers/monitor
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:44:52.592089472-05:00" level=error msg="Wait for process failed" container=f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 error="ttrpc: closed" name=containerd-shim-v2 pid=f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 sandbox=f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 source=containerd-kata-shim-v2
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:44:52.592238045-05:00" level=warning msg="sandbox stopped unexpectedly" error="failed to ping agent: Failed to Check if grpc server is working: context deadline exceeded" name=containerd-shim-v2 pid=207475 sandbox=f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 source=containerd-kata-shim-v2
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kata[207475]: time="2023-01-20T12:44:52.592089472-05:00" level=error msg="Wait for process failed" container=f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 error="ttrpc: closed" name=containerd-shim-v2 pid=f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 sandbox=f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 source=containerd-kata-shim-v2
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:44:52.593088962-05:00" level=warning msg="notify on errors" error="failed to ping agent: Failed to Check if grpc server is working: Dead agent" sandbox=f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 source=virtcontainers subsystem=virtcontainers/monitor
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:44:52.593224771-05:00" level=warning msg="write error to watcher" error="failed to ping agent: Failed to Check if grpc server is working: Dead agent" sandbox=f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 source=virtcontainers subsystem=virtcontainers/monitor
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kata[207475]: time="2023-01-20T12:44:52.592238045-05:00" level=warning msg="sandbox stopped unexpectedly" error="failed to ping agent: Failed to Check if grpc server is working: context deadline exceeded" name=containerd-shim-v2 pid=207475 sandbox=f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 source=containerd-kata-shim-v2
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kata[207475]: time="2023-01-20T12:44:52.593088962-05:00" level=warning msg="notify on errors" error="failed to ping agent: Failed to Check if grpc server is working: Dead agent" sandbox=f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 source=virtcontainers subsystem=virtcontainers/monitor
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kata[207475]: time="2023-01-20T12:44:52.593224771-05:00" level=warning msg="write error to watcher" error="failed to ping agent: Failed to Check if grpc server is working: Dead agent" sandbox=f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 source=virtcontainers subsystem=virtcontainers/monitor
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kata[207475]: time="2023-01-20T12:44:52.593991932-05:00" level=warning msg="Agent did not stop sandbox" error="Dead agent" name=containerd-shim-v2 pid=207475 sandbox=f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 sandboxid=f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 source=virtcontainers subsystem=sandbox
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:44:52.593991932-05:00" level=warning msg="Agent did not stop sandbox" error="Dead agent" name=containerd-shim-v2 pid=207475 sandbox=f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 sandboxid=f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 source=virtcontainers subsystem=sandbox
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kata[207475]: time="2023-01-20T12:44:52.595583639-05:00" level=error msg="Failed to read guest console logs" console-protocol=unix console-url=/run/vc/vm/f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615/console.sock error="read unix @->/run/vc/vm/f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615/console.sock: use of closed network connection" name=containerd-shim-v2 pid=207475 sandbox=f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 source=virtcontainers subsystem=sandbox
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:44:52.595583639-05:00" level=error msg="Failed to read guest console logs" console-protocol=unix console-url=/run/vc/vm/f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615/console.sock error="read unix @->/run/vc/vm/f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615/console.sock: use of closed network connection" name=containerd-shim-v2 pid=207475 sandbox=f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 source=virtcontainers subsystem=sandbox
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER systemd[1066]: run-kata\x2dcontainers-shared-sandboxes-f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615-shared-f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615\x2d32a9940672216df6\x2dresolv.conf.mount: Succeeded.
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER systemd[1066]: run-kata\x2dcontainers-shared-sandboxes-f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615-mounts-f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615\x2d32a9940672216df6\x2dresolv.conf.mount: Succeeded.
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER systemd[1066]: run-kata\x2dcontainers-shared-sandboxes-f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615-shared-f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615-rootfs.mount: Succeeded.
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER systemd[1066]: run-kata\x2dcontainers-shared-sandboxes-f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615-mounts-f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615-rootfs.mount: Succeeded.
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER systemd[193112]: run-kata\x2dcontainers-shared-sandboxes-f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615-shared-f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615\x2d32a9940672216df6\x2dresolv.conf.mount: Succeeded.
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER systemd[193112]: run-kata\x2dcontainers-shared-sandboxes-f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615-mounts-f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615\x2d32a9940672216df6\x2dresolv.conf.mount: Succeeded.
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER systemd[193112]: run-kata\x2dcontainers-shared-sandboxes-f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615-shared-f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615-rootfs.mount: Succeeded.
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER systemd[193112]: run-kata\x2dcontainers-shared-sandboxes-f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615-mounts-f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615-rootfs.mount: Succeeded.
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER systemd[1]: run-kata\x2dcontainers-shared-sandboxes-f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615-shared-f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615\x2d32a9940672216df6\x2dresolv.conf.mount: Succeeded.
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER systemd[1]: run-kata\x2dcontainers-shared-sandboxes-f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615-mounts-f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615\x2d32a9940672216df6\x2dresolv.conf.mount: Succeeded.
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER systemd[1]: run-kata\x2dcontainers-shared-sandboxes-f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615-shared-f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615-rootfs.mount: Succeeded.
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER systemd[1]: run-kata\x2dcontainers-shared-sandboxes-f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615-mounts-f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615-rootfs.mount: Succeeded.
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.618016  199956 manager.go:1044] Destroyed container: "/kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615" (aliases: [f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 /kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615], namespace: "containerd")
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.618044  199956 handler.go:325] Added event &{/kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 2023-01-20 12:44:52.618040672 -0500 EST m=+805.173815495 containerDeletion {<nil>}}
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.619483  199956 manager.go:1044] Destroyed container: "/kata_overhead/f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615" (aliases: [f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 /kata_overhead/f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615], namespace: "containerd")
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.619498  199956 handler.go:325] Added event &{/kata_overhead/f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 2023-01-20 12:44:52.619495685 -0500 EST m=+805.175270507 containerDeletion {<nil>}}
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER systemd[1]: run-kata\x2dcontainers-shared-sandboxes-f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615-shared.mount: Succeeded.
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER systemd[193112]: run-kata\x2dcontainers-shared-sandboxes-f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615-shared.mount: Succeeded.
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER systemd[193112]: run-containerd-io.containerd.runtime.v2.task-k8s.io-f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615-rootfs.mount: Succeeded.
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615-rootfs.mount: Succeeded.
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER systemd[1066]: run-kata\x2dcontainers-shared-sandboxes-f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615-shared.mount: Succeeded.
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER systemd[1066]: run-containerd-io.containerd.runtime.v2.task-k8s.io-f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615-rootfs.mount: Succeeded.
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kata[207475]: time="2023-01-20T12:44:52.62389549-05:00" level=error msg="failed to cleanup the &{%!s(*cgroups.cgroup=&{0x55a3e10a3980 [0xc000510240 0xc0001741e0 0xc000174200 0xc000174210 0xc000174220 0xc000174230 0xc000174250 0xc000174270 0xc000174280 0xc000012378 0xc000510780 0xc000174290 0xc0001742c0 0xc00019c600] {0 0} <nil>}) /kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 %!s(*specs.LinuxCPU=&{<nil> <nil> <nil> <nil> <nil>  }) [{%!s(bool=false)  %!s(*int64=<nil>) %!s(*int64=<nil>) rwm} {%!s(bool=true) c %!s(*int64=0xc0001cb570) %!s(*int64=0xc0001cb578) rwm} {%!s(bool=true) c %!s(*int64=0xc0001cb5a0) %!s(*int64=0xc0001cb5a8) rwm} {%!s(bool=true) c %!s(*int64=0xc00019c308) %!s(*int64=0xc00019c310) rwm} {%!s(bool=true) c %!s(*int64=0xc00019c338) %!s(*int64=0xc00019c340) rwm} {%!s(bool=true) c %!s(*int64=0xc00019c398) %!s(*int64=0xc00019c3a0) rwm} {%!s(bool=true) c %!s(*int64=0xc00019c3c8) %!s(*int64=0xc00019c3d0) rwm} {%!s(bool=true) c %!s(*int64=0xc00019c3f8) %!s(*int64=0xc00019c400) rwm} {%!s(bool=true) c %!s(*int64=0xc00019c428) %!s(*int64=0xc00019c430) rwm} {%!s(bool=true) c %!s(*int64=0xc00019c488) %!s(*int64=0xc00019c490) rwm} {%!s(bool=true) c %!s(*int64=0xc00019c4b8) %!s(*int64=0xc00019c4c0) rwm} {%!s(bool=true) c %!s(*int64=0xc00019c518) %!s(*int64=0xc00019c520) rwm} {%!s(bool=true) c %!s(*int64=0xc00019c548) %!s(*int64=0xc00019c550) rwm} {%!s(bool=true) c %!s(*int64=0xc00019c578) %!s(*int64=0xc00019c580) rwm} {%!s(bool=true) c %!s(*int64=0xc0001cb748) %!s(*int64=0xc0001cb750) m} {%!s(bool=true) b %!s(*int64=0xc0001cb748) %!s(*int64=0xc0001cb750) m} {%!s(bool=true) c %!s(*int64=0xc0001cb758) %!s(*int64=0xc0001cb750) rwm} {%!s(bool=true) c %!s(*int64=0xc0001cb760) %!s(*int64=0xc0001cb768) rwm}] {%!s(int32=0) %!s(uint32=0)}} resource controllers" error="cgroups: cgroup deleted" name=containerd-shim-v2 pid=207475 sandbox=f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 source=virtcontainers subsystem=sandbox
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kata[207475]: time="2023-01-20T12:44:52.624083063-05:00" level=warning msg="Calling Cleanup() on an already cleaned up filesystem" name=containerd-shim-v2 pid=207475 sandbox=f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 source=virtcontainers subsystem="filesystem share"
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:44:52.623895490-05:00" level=error msg="failed to cleanup the &{%!s(*cgroups.cgroup=&{0x55a3e10a3980 [0xc000510240 0xc0001741e0 0xc000174200 0xc000174210 0xc000174220 0xc000174230 0xc000174250 0xc000174270 0xc000174280 0xc000012378 0xc000510780 0xc000174290 0xc0001742c0 0xc00019c600] {0 0} <nil>}) /kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 %!s(*specs.LinuxCPU=&{<nil> <nil> <nil> <nil> <nil>  }) [{%!s(bool=false)  %!s(*int64=<nil>) %!s(*int64=<nil>) rwm} {%!s(bool=true) c %!s(*int64=0xc0001cb570) %!s(*int64=0xc0001cb578) rwm} {%!s(bool=true) c %!s(*int64=0xc0001cb5a0) %!s(*int64=0xc0001cb5a8) rwm} {%!s(bool=true) c %!s(*int64=0xc00019c308) %!s(*int64=0xc00019c310) rwm} {%!s(bool=true) c %!s(*int64=0xc00019c338) %!s(*int64=0xc00019c340) rwm} {%!s(bool=true) c %!s(*int64=0xc00019c398) %!s(*int64=0xc00019c3a0) rwm} {%!s(bool=true) c %!s(*int64=0xc00019c3c8) %!s(*int64=0xc00019c3d0) rwm} {%!s(bool=true) c %!s(*int64=0xc00019c3f8) %!s(*int64=0xc00019c400) rwm} {%!s(bool=true) c %!s(*int64=0xc00019c428) %!s(*int64=0xc00019c430) rwm} {%!s(bool=true) c %!s(*int64=0xc00019c488) %!s(*int64=0xc00019c490) rwm} {%!s(bool=true) c %!s(*int64=0xc00019c4b8) %!s(*int64=0xc00019c4c0) rwm} {%!s(bool=true) c %!s(*int64=0xc00019c518) %!s(*int64=0xc00019c520) rwm} {%!s(bool=true) c %!s(*int64=0xc00019c548) %!s(*int64=0xc00019c550) rwm} {%!s(bool=true) c %!s(*int64=0xc00019c578) %!s(*int64=0xc00019c580) rwm} {%!s(bool=true) c %!s(*int64=0xc0001cb748) %!s(*int64=0xc0001cb750) m} {%!s(bool=true) b %!s(*int64=0xc0001cb748) %!s(*int64=0xc0001cb750) m} {%!s(bool=true) c %!s(*int64=0xc0001cb758) %!s(*int64=0xc0001cb750) rwm} {%!s(bool=true) c %!s(*int64=0xc0001cb760) %!s(*int64=0xc0001cb768) rwm}] {%!s(int32=0) %!s(uint32=0)}} resource controllers" error="cgroups: cgroup deleted" name=containerd-shim-v2 pid=207475 sandbox=f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 source=virtcontainers subsystem=sandbox
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:44:52.624083063-05:00" level=warning msg="Calling Cleanup() on an already cleaned up filesystem" name=containerd-shim-v2 pid=207475 sandbox=f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 source=virtcontainers subsystem="filesystem share"
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:44:52.625797051-05:00" level=error msg="error receiving message" error="read unix /run/containerd/containerd.sock.ttrpc->@: read: connection reset by peer"
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:44:52.625813749-05:00" level=info msg="shim disconnected" id=f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:44:52.625831816-05:00" level=warning msg="cleaning up after shim disconnected" id=f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 namespace=k8s.io
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:44:52.625838433-05:00" level=info msg="cleaning up dead shim"
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:44:52.635032187-05:00" level=error msg="failed to delete" cmd="/usr/local/bin/containerd-shim-kata-qemu-v2 -namespace k8s.io -address /run/containerd/containerd.sock -publish-binary /opt/confidential-containers/bin/containerd -id f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 -bundle /run/containerd/io.containerd.runtime.v2.task/k8s.io/f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 delete" error="exit status 1"
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:44:52.635063532-05:00" level=warning msg="failed to clean up after shim disconnected" error="time=\"2023-01-20T12:44:52-05:00\" level=warning msg=\"failed to cleanup container\" container=f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 error=\"open /run/vc/sbs/f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615: no such file or directory\" name=containerd-shim-v2 pid=207659 sandbox=f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 source=containerd-kata-shim-v2\nio.containerd.kata.v2: open /run/vc/sbs/f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615: no such file or directory: exit status 1" id=f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 namespace=k8s.io
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.689168  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.689238  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.690310  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:52 GMT]] 0xc00129ca60 2 [] true false map[] 0xc000fcc600 <nil>}
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.690417  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.695634  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.695700  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.696754  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:52 GMT]] 0xc00129caa0 2 [] true false map[] 0xc000fcc800 <nil>}
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.696863  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.709978  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.710009  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.710050  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.710072  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.711221  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:52 GMT]] 0xc0008e2e80 2 [] true false map[] 0xc000fcca00 <nil>}
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.711280  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:52 GMT]] 0xc0012e80a0 2 [] true false map[] 0xc0016ab800 <nil>}
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.711345  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.711409  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.837166  199956 generic.go:155] "GenericPLEG" podUID=4f97314d-815b-4787-b174-5c158cd28c9d containerID="f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615" oldState=running newState=exited
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.837950  199956 kuberuntime_manager.go:1037] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615] pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.839453  199956 generic.go:421] "PLEG: Write status" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.839540  199956 kubelet.go:2098] "SyncLoop (PLEG): event for pod" pod="default/unsigned-unencrypted-cc-1" event=&{ID:4f97314d-815b-4787-b174-5c158cd28c9d Type:ContainerDied Data:f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615}
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.839588  199956 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615"
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.839630  199956 pod_workers.go:888] "Processing pod event" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.839658  199956 kubelet.go:1501] "syncPod enter" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.839691  199956 kubelet_pods.go:1435] "Generating pod status" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.839761  199956 kubelet_pods.go:1447] "Got phase for pod" pod="default/unsigned-unencrypted-cc-1" oldPhase=Pending phase=Pending
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.840201  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.840277  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.840310  199956 kuberuntime_manager.go:488] "No ready sandbox for pod can be found. Need to start a new one" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.840356  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:true CreateSandbox:true SandboxID:f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615 Attempt:3 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.840423  199956 kuberuntime_manager.go:730] "Stopping PodSandbox for pod, will start new one" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.840617  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="" kind="Pod" apiVersion="v1" type="Normal" reason="SandboxChanged" message="Pod sandbox changed, it will be killed and re-created."
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:44:52.840891906-05:00" level=info msg="StopPodSandbox for \"f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615\""
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.843492  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="default/unsigned-unencrypted-cc-1" volumeName="kube-api-access-qcb8g" volumeSpecName="kube-api-access-qcb8g"
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER systemd[193112]: run-containerd-io.containerd.grpc.v1.cri-sandboxes-f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615-shm.mount: Succeeded.
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER systemd[1066]: run-containerd-io.containerd.grpc.v1.cri-sandboxes-f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615-shm.mount: Succeeded.
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER systemd[1]: run-containerd-io.containerd.grpc.v1.cri-sandboxes-f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615-shm.mount: Succeeded.
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.856778  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.862211  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") Volume is already mounted to pod, but remount was requested." pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.862610  199956 projected.go:183] Setting up volume kube-api-access-qcb8g for pod 4f97314d-815b-4787-b174-5c158cd28c9d at /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.862775  199956 status_manager.go:685] "Patch status for pod" pod="default/unsigned-unencrypted-cc-1" patch="{\"metadata\":{\"uid\":\"4f97314d-815b-4787-b174-5c158cd28c9d\"},\"status\":{\"containerStatuses\":[{\"image\":\"ngcn-registry.sh.intel.com:443/ci-example256m:latest\",\"imageID\":\"\",\"lastState\":{},\"name\":\"ci-example256m\",\"ready\":false,\"restartCount\":0,\"started\":false,\"state\":{\"waiting\":{\"message\":\"rpc error: code = DeadlineExceeded desc = context deadline exceeded\",\"reason\":\"ErrImagePull\"}}}]}}"
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.862960  199956 status_manager.go:694] "Status for pod updated successfully" pod="default/unsigned-unencrypted-cc-1" statusVersion=6 status={Phase:Pending Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.14 PodIPs:[{IP:10.244.0.14}] StartTime:2023-01-20 12:41:08 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:ci-example256m State:{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = DeadlineExceeded desc = context deadline exceeded,} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest ImageID: ContainerID: Started:0xc001a41679}] QOSClass:Guaranteed EphemeralContainerStatuses:[]}
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.862316  199956 config.go:279] "Setting pods for source" source="api"
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.863066  199956 atomic_writer.go:161] pod default/unsigned-unencrypted-cc-1 volume kube-api-access-qcb8g: no update required for target directory /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.863140  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") " pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.864428  199956 kubelet.go:2073] "SyncLoop RECONCILE" source="api" pods=[default/unsigned-unencrypted-cc-1]
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER avahi-daemon[896]: Interface veth35c14a32.IPv6 no longer relevant for mDNS.
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER avahi-daemon[896]: Leaving mDNS multicast group on interface veth35c14a32.IPv6 with address fe80::788f:5dff:fe7d:6cc7.
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kernel: cni0: port 6(veth35c14a32) entered disabled state
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kernel: device veth35c14a32 left promiscuous mode
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kernel: cni0: port 6(veth35c14a32) entered disabled state
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER avahi-daemon[896]: Withdrawing address record for fe80::788f:5dff:fe7d:6cc7 on veth35c14a32.
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER NetworkManager[905]: <info>  [1674236692.9370] device (veth35c14a32): released from master device cni0
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER gnome-shell[1450]: Removing a network device that was not added
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:44:52.979664817-05:00" level=info msg="TearDown network for sandbox \"f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615\" successfully"
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:44:52.980061893-05:00" level=info msg="StopPodSandbox for \"f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615\" returns successfully"
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.980724  199956 kuberuntime_manager.go:785] "Creating PodSandbox for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:52.980979  199956 kuberuntime_sandbox.go:63] "Running pod with runtime handler" pod="default/unsigned-unencrypted-cc-1" runtimeHandler="kata-qemu"
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER systemd[193112]: run-netns-cni\x2dfce84e39\x2dd9da\x2d1477\x2db3d9\x2d07c55cac6b6c.mount: Succeeded.
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER systemd[1066]: run-netns-cni\x2dfce84e39\x2dd9da\x2d1477\x2db3d9\x2d07c55cac6b6c.mount: Succeeded.
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER systemd[1]: run-netns-cni\x2dfce84e39\x2dd9da\x2d1477\x2db3d9\x2d07c55cac6b6c.mount: Succeeded.
Jan 20 12:44:52 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:44:52.985647895-05:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:unsigned-unencrypted-cc-1,Uid:4f97314d-815b-4787-b174-5c158cd28c9d,Namespace:default,Attempt:3,}"
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER systemd-udevd[207698]: ethtool: autonegotiation is unset or enabled, the speed and duplex are not writable.
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER systemd-udevd[207698]: Using default interface naming scheme 'v245'.
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER systemd-udevd[207698]: vethad7b1431: Could not generate persistent MAC: No data available
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER NetworkManager[905]: <info>  [1674236693.0026] manager: (vethad7b1431): new Veth device (/org/freedesktop/NetworkManager/Devices/125)
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kernel: cni0: port 6(vethad7b1431) entered blocking state
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kernel: cni0: port 6(vethad7b1431) entered disabled state
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kernel: device vethad7b1431 entered promiscuous mode
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kernel: cni0: port 6(vethad7b1431) entered blocking state
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kernel: cni0: port 6(vethad7b1431) entered forwarding state
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER NetworkManager[905]: <info>  [1674236693.0141] device (vethad7b1431): carrier: link connected
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kernel: IPv6: ADDRCONF(NETDEV_CHANGE): vethad7b1431: link becomes ready
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER containerd[201983]: map[string]interface {}{"cniVersion":"0.3.1", "hairpinMode":true, "ipMasq":false, "ipam":map[string]interface {}{"ranges":[][]map[string]interface {}{[]map[string]interface {}{map[string]interface {}{"subnet":"10.244.0.0/24"}}}, "routes":[]types.Route{types.Route{Dst:net.IPNet{IP:net.IP{0xa, 0xf4, 0x0, 0x0}, Mask:net.IPMask{0xff, 0xff, 0x0, 0x0}}, GW:net.IP(nil)}}, "type":"host-local"}, "isDefaultGateway":true, "isGateway":true, "mtu":(*uint)(0xc0000b48e8), "name":"cbr0", "type":"bridge"}
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.061201  199956 factory.go:258] Using factory "containerd" for container "/kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af"
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kernel: eth0: Caught tx_queue_len zero misconfig
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER virtiofsd[207768]: zcy-Z390-AORUS-MASTER virtiofsd[207768]: Use of deprecated flag '-f': This flag has no effect, please remove it
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER virtiofsd[207768]: zcy-Z390-AORUS-MASTER virtiofsd[207768]: Use of deprecated option format '-o': Please specify options without it (e.g., '--cache auto' instead of '-o cache=auto')
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER virtiofsd[207770]: zcy-Z390-AORUS-MASTER virtiofsd[207768]: Waiting for vhost-user socket connection...
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER virtiofsd[207770]: zcy-Z390-AORUS-MASTER virtiofsd[207768]: Client connected, servicing requests
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.483141  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.483162  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.486220  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[8a3b7257-b920-44d5-ad5c-7fa2ce221294] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:53 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0000a4900 2 [] false false map[] 0xc0016abc00 0xc000ff5760}
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.486254  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.581599  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr]
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.581631  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.581669  199956 pod_workers.go:888] "Processing pod event" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d updateType=0
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.581691  199956 kubelet.go:1501] "syncPod enter" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.581701  199956 kubelet_pods.go:1435] "Generating pod status" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.581732  199956 kubelet_pods.go:1447] "Got phase for pod" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" oldPhase=Running phase=Running
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.581814  199956 status_manager.go:535] "Ignoring same status for pod" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:58 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:08 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:08 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:58 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.4 PodIPs:[{IP:10.244.0.4}] StartTime:2023-01-20 12:31:58 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:kube-rbac-proxy State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:59 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:gcr.io/kubebuilder/kube-rbac-proxy:v0.13.0 ImageID:gcr.io/kubebuilder/kube-rbac-proxy@sha256:d99a8d144816b951a67648c12c0b988936ccd25cf3754f3cd85ab8c01592248f ContainerID:containerd://1cb44bdbde0b41852e382d7dab7c184af8dd4e5b25d5cfe6a10a9c8ab601659d Started:0xc0021338df} {Name:manager State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:59 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:quay.io/confidential-containers/operator:v0.2.0 ImageID:quay.io/confidential-containers/operator@sha256:c965b55253a9abe4c2f7596c42467fa59f2cc741bfafeed1d25629ed6f8df12d ContainerID:containerd://186424b47c7b9022ecfe8996dc73cd9101f7539259cd65914279c84c9a02a288 Started:0xc0021338f0}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.581893  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.581914  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.582047  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.582088  199956 kubelet.go:1503] "syncPod exit" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d isTerminal=false
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.582107  199956 pod_workers.go:988] "Processing pod event done" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d updateType=0
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.583126  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.583138  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.583145  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.583641  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.583686  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.583693  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.583701  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.649599  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" volumeName="kube-api-access-4pnfq" volumeSpecName="kube-api-access-4pnfq"
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.666619  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-4pnfq\" (UniqueName: \"kubernetes.io/projected/d2688d45-2487-46e7-aecb-e3479626909d-kube-api-access-4pnfq\") pod \"cc-operator-controller-manager-79797456f6-m6znr\" (UID: \"d2688d45-2487-46e7-aecb-e3479626909d\") Volume is already mounted to pod, but remount was requested." pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.666673  199956 projected.go:183] Setting up volume kube-api-access-4pnfq for pod d2688d45-2487-46e7-aecb-e3479626909d at /var/lib/kubelet/pods/d2688d45-2487-46e7-aecb-e3479626909d/volumes/kubernetes.io~projected/kube-api-access-4pnfq
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.666786  199956 atomic_writer.go:161] pod confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr volume kube-api-access-4pnfq: no update required for target directory /var/lib/kubelet/pods/d2688d45-2487-46e7-aecb-e3479626909d/volumes/kubernetes.io~projected/kube-api-access-4pnfq
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.666800  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-4pnfq\" (UniqueName: \"kubernetes.io/projected/d2688d45-2487-46e7-aecb-e3479626909d-kube-api-access-4pnfq\") pod \"cc-operator-controller-manager-79797456f6-m6znr\" (UID: \"d2688d45-2487-46e7-aecb-e3479626909d\") " pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER systemd[193112]: run-kata\x2dcontainers-shared-sandboxes-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af-private-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af\x2d44e89a1158ae25de\x2dresolv.conf.mount: Succeeded.
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER systemd[1]: run-kata\x2dcontainers-shared-sandboxes-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af-private-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af\x2d44e89a1158ae25de\x2dresolv.conf.mount: Succeeded.
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER systemd[1066]: run-kata\x2dcontainers-shared-sandboxes-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af-private-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af\x2d44e89a1158ae25de\x2dresolv.conf.mount: Succeeded.
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.809551  199956 manager.go:988] Added container: "/kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af" (aliases: [3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af /kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af], namespace: "containerd")
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.809638  199956 handler.go:325] Added event &{/kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af 2023-01-20 12:44:53.057879846 -0500 EST containerCreation {<nil>}}
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.809695  199956 container.go:530] Start housekeeping for container "/kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af"
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.809862  199956 factory.go:258] Using factory "containerd" for container "/kata_overhead/3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af"
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER containerd[201983]: {"cniVersion":"0.3.1","hairpinMode":true,"ipMasq":false,"ipam":{"ranges":[[{"subnet":"10.244.0.0/24"}]],"routes":[{"dst":"10.244.0.0/16"}],"type":"host-local"},"isDefaultGateway":true,"isGateway":true,"mtu":1450,"name":"cbr0","type":"bridge"}time="2023-01-20T12:44:53.935536974-05:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:unsigned-unencrypted-cc-1,Uid:4f97314d-815b-4787-b174-5c158cd28c9d,Namespace:default,Attempt:3,} returns sandbox id \"3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af\""
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.935923  199956 kuberuntime_manager.go:823] "Created PodSandbox for pod" podSandboxID="3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.936222  199956 manager.go:988] Added container: "/kata_overhead/3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af" (aliases: [3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af /kata_overhead/3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af], namespace: "containerd")
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.936673  199956 handler.go:325] Added event &{/kata_overhead/3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af 2023-01-20 12:44:53.085879847 -0500 EST containerCreation {<nil>}}
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.936767  199956 container.go:530] Start housekeeping for container "/kata_overhead/3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af"
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.936880  199956 kuberuntime_manager.go:846] "Determined the ip for pod after sandbox changed" IPs=[10.244.0.15] pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.937211  199956 kuberuntime_manager.go:889] "Creating container in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.938177  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Normal" reason="BackOff" message="Back-off pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\""
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.938181  199956 kuberuntime_manager.go:903] "Container start failed in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1" containerMessage="Back-off pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\"" err="ImagePullBackOff"
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.938225  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Warning" reason="Failed" message="Error: ImagePullBackOff"
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.938265  199956 kubelet.go:1503] "syncPod exit" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d isTerminal=false
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: E0120 12:44:53.938308  199956 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ci-example256m\" with ImagePullBackOff: \"Back-off pulling image \\\"ngcn-registry.sh.intel.com:443/ci-example256m:latest\\\"\"" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:44:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:53.938357  199956 pod_workers.go:988] "Processing pod event done" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER avahi-daemon[896]: Joining mDNS multicast group on interface vethad7b1431.IPv6 with address fe80::881e:54ff:fecc:1fb9.
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER avahi-daemon[896]: New relevant interface vethad7b1431.IPv6 for mDNS.
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER avahi-daemon[896]: Registering new address record for fe80::881e:54ff:fecc:1fb9 on vethad7b1431.*.
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.446695  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.446703  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.446913  199956 interface.go:209] Interface eno2 is up
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.446954  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.446962  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.446967  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.446970  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.446973  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.447356  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.447365  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.447369  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.447373  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.483680  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.483699  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.488030  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[952bd7b2-43dd-4de5-bb3a-3ace18e1fbe1] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:54 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0017020a0 2 [] false false map[] 0xc0023aba00 0xc001943810}
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.488078  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.843728  199956 generic.go:155] "GenericPLEG" podUID=4f97314d-815b-4787-b174-5c158cd28c9d containerID="3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af" oldState=non-existent newState=running
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.844587  199956 kuberuntime_manager.go:1037] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615] pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.846673  199956 generic.go:421] "PLEG: Write status" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.846767  199956 kubelet.go:2098] "SyncLoop (PLEG): event for pod" pod="default/unsigned-unencrypted-cc-1" event=&{ID:4f97314d-815b-4787-b174-5c158cd28c9d Type:ContainerStarted Data:3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af}
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.846826  199956 pod_workers.go:888] "Processing pod event" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.846855  199956 kubelet.go:1501] "syncPod enter" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.846886  199956 kubelet_pods.go:1435] "Generating pod status" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.846958  199956 kubelet_pods.go:1447] "Got phase for pod" pod="default/unsigned-unencrypted-cc-1" oldPhase=Pending phase=Pending
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.847367  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.847439  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.847488  199956 kuberuntime_manager.go:638] "Container of pod is not in the desired state and shall be started" containerName="ci-example256m" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.847531  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af Attempt:3 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.847804  199956 kuberuntime_manager.go:889] "Creating container in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.848782  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Normal" reason="BackOff" message="Back-off pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\""
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.848804  199956 kuberuntime_manager.go:903] "Container start failed in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1" containerMessage="Back-off pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\"" err="ImagePullBackOff"
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.848841  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Warning" reason="Failed" message="Error: ImagePullBackOff"
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.848874  199956 kubelet.go:1503] "syncPod exit" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d isTerminal=false
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: E0120 12:44:54.848913  199956 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ci-example256m\" with ImagePullBackOff: \"Back-off pulling image \\\"ngcn-registry.sh.intel.com:443/ci-example256m:latest\\\"\"" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.848963  199956 pod_workers.go:988] "Processing pod event done" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.856308  199956 config.go:279] "Setting pods for source" source="api"
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.856395  199956 status_manager.go:685] "Patch status for pod" pod="default/unsigned-unencrypted-cc-1" patch="{\"metadata\":{\"uid\":\"4f97314d-815b-4787-b174-5c158cd28c9d\"},\"status\":{\"$setElementOrder/podIPs\":[{\"ip\":\"10.244.0.15\"}],\"containerStatuses\":[{\"image\":\"ngcn-registry.sh.intel.com:443/ci-example256m:latest\",\"imageID\":\"\",\"lastState\":{},\"name\":\"ci-example256m\",\"ready\":false,\"restartCount\":0,\"started\":false,\"state\":{\"waiting\":{\"message\":\"Back-off pulling image \\\"ngcn-registry.sh.intel.com:443/ci-example256m:latest\\\"\",\"reason\":\"ImagePullBackOff\"}}}],\"podIP\":\"10.244.0.15\",\"podIPs\":[{\"ip\":\"10.244.0.15\"},{\"$patch\":\"delete\",\"ip\":\"10.244.0.14\"}]}}"
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.856435  199956 status_manager.go:694] "Status for pod updated successfully" pod="default/unsigned-unencrypted-cc-1" statusVersion=7 status={Phase:Pending Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.15 PodIPs:[{IP:10.244.0.15}] StartTime:2023-01-20 12:41:08 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:ci-example256m State:{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "ngcn-registry.sh.intel.com:443/ci-example256m:latest",} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest ImageID: ContainerID: Started:0xc0015ff43e}] QOSClass:Guaranteed EphemeralContainerStatuses:[]}
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.856637  199956 kubelet.go:2073] "SyncLoop RECONCILE" source="api" pods=[default/unsigned-unencrypted-cc-1]
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.857446  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="default/unsigned-unencrypted-cc-1" volumeName="kube-api-access-qcb8g" volumeSpecName="kube-api-access-qcb8g"
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.874802  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") Volume is already mounted to pod, but remount was requested." pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.874989  199956 projected.go:183] Setting up volume kube-api-access-qcb8g for pod 4f97314d-815b-4787-b174-5c158cd28c9d at /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.875405  199956 atomic_writer.go:161] pod default/unsigned-unencrypted-cc-1 volume kube-api-access-qcb8g: no update required for target directory /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:44:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:54.875481  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") " pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:44:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:55.483920  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:55.483994  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:55.497273  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[22609000-db72-4aac-b933-aba60914f3e3] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:55 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001486880 2 [] false false map[] 0xc000fcd200 0xc001dbeb00}
Jan 20 12:44:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:55.497410  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:55.581987  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:44:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:55.587658  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:44:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:55.587670  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:44:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:55.588220  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:44:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:55.588628  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:44:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:55.588674  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:44:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:55.588681  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:44:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:55.588696  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:44:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:56.165171  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:44:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:56.165243  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:56.172338  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:56 GMT] X-Content-Type-Options:[nosniff]] 0xc000cb2c60 2 [] false false map[] 0xc0014c6b00 0xc001ec53f0}
Jan 20 12:44:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:56.172377  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:44:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:56.272242  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:44:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:56.272302  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:56.273531  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:56 GMT]] 0xc0009a08c0 29 [] true false map[] 0xc001717b00 <nil>}
Jan 20 12:44:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:56.273634  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:44:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:56.483424  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:56.483490  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:56.491842  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[62e51034-500e-40bb-b786-83ac25fa9a6a] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:56 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123f720 2 [] false false map[] 0xc0014c6f00 0xc001ec5600}
Jan 20 12:44:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:56.491868  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:56.895482  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:44:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:56.895549  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:56.903161  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:56 GMT] X-Content-Type-Options:[nosniff]] 0xc00123fc40 2 [] false false map[] 0xc001717d00 0xc001fd9ad0}
Jan 20 12:44:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:56.903211  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:44:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:57.484101  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:57.484167  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:57.491774  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[e7cc2075-aeaa-48e8-92ca-9758ccc4e58e] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:57 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0009a1d40 2 [] false false map[] 0xc001775e00 0xc0021e89a0}
Jan 20 12:44:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:57.491805  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:57.581950  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:44:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:57.587702  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:44:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:57.587715  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:44:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:57.588361  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:44:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:57.588750  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:44:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:57.588797  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:44:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:57.588803  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:44:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:57.588812  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:44:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:57.858204  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:44:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:58.483144  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:58.483214  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:58.491275  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[b0fcfa08-2657-4b5d-8afd-174059290ed4] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:58 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000cb3400 2 [] false false map[] 0xc0014c6100 0xc000629a20}
Jan 20 12:44:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:58.491308  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:58.879726  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/healthz" timeout="1s"
Jan 20 12:44:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:58.879780  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:44:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:58.879792  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:58.879842  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:58.881165  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:58 GMT] X-Content-Type-Options:[nosniff]] 0xc0012e8be0 2 [] true false map[] 0xc0014c6a00 <nil>}
Jan 20 12:44:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:58.881209  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:58 GMT] X-Content-Type-Options:[nosniff]] 0xc001486080 2 [] true false map[] 0xc001846200 <nil>}
Jan 20 12:44:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:58.881283  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:44:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:58.881304  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:44:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:59.483159  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:44:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:59.483231  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:44:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:59.490972  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[83c17dbe-5fd0-44bc-acc4-7d0c453d5631] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:44:59 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008dc000 2 [] false false map[] 0xc001a4a300 0xc000de7970}
Jan 20 12:44:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:59.491017  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:44:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:59.581245  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[kube-system/kube-scheduler-zcy-z390-aorus-master]
Jan 20 12:44:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:59.581336  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:44:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:59.581473  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 updateType=0
Jan 20 12:44:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:59.581558  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3
Jan 20 12:44:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:59.581589  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/kube-scheduler-zcy-z390-aorus-master"
Jan 20 12:44:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:59.581692  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" oldPhase=Running phase=Running
Jan 20 12:44:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:59.581978  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:27 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:41 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:41 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:27 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.239.159.53 PodIPs:[{IP:10.239.159.53}] StartTime:2023-01-20 12:31:27 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:kube-scheduler State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:22 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:6 Image:k8s.gcr.io/kube-scheduler:v1.24.0 ImageID:k8s.gcr.io/kube-scheduler@sha256:db842a7c431fd51db7e1911f6d1df27a7b6b6963ceda24852b654d2cd535b776 ContainerID:containerd://13149330f3752d0ff65bcac86c7b522b2040811e815fbc87e56b40b612875d5a Started:0xc00164b66e}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:44:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:59.582308  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-scheduler-zcy-z390-aorus-master"
Jan 20 12:44:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:59.582375  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/kube-scheduler-zcy-z390-aorus-master"
Jan 20 12:44:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:59.582912  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/kube-scheduler-zcy-z390-aorus-master"
Jan 20 12:44:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:59.583070  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 isTerminal=false
Jan 20 12:44:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:59.583153  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 updateType=0
Jan 20 12:44:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:59.586747  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:44:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:59.586758  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:44:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:59.586764  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:44:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:59.587584  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:44:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:59.587653  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:44:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:59.587659  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:44:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:59.587667  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:44:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:44:59.590957  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Jan 20 12:45:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:00.276567  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:45:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:00.280463  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:45:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:00.290754  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59533000Ki" capacity="65586124Ki" time="2023-01-20 12:45:00.277080032 -0500 EST m=+812.832854855"
Jan 20 12:45:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:00.290768  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64401992Ki" capacity="65061836Ki" time="2023-01-20 12:45:00.29069997 -0500 EST m=+812.846474795"
Jan 20 12:45:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:00.290776  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902002484Ki" capacity="981310056Ki" time="2023-01-20 12:45:00.277080032 -0500 EST m=+812.832854855"
Jan 20 12:45:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:00.290788  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945631" capacity="62382080" time="2023-01-20 12:45:00.277080032 -0500 EST m=+812.832854855"
Jan 20 12:45:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:00.290794  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902002484Ki" capacity="981310056Ki" time="2023-01-20 12:44:51.262153169 -0500 EST"
Jan 20 12:45:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:00.290800  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945631" capacity="62382080" time="2023-01-20 12:44:51.262153169 -0500 EST"
Jan 20 12:45:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:00.290805  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510770" capacity="511757" time="2023-01-20 12:45:00.290430207 -0500 EST m=+812.846205033"
Jan 20 12:45:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:00.290833  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:45:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:00.483586  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:00.483647  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:00.496673  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[b1e8ca90-ee60-40cc-a3ae-47d413189f66] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:00 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000707980 2 [] false false map[] 0xc001a4a800 0xc0012dc840}
Jan 20 12:45:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:00.496843  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:00.776747  199956 handler.go:293] error while reading "/proc/199956/fd/34" link: readlink /proc/199956/fd/34: no such file or directory
Jan 20 12:45:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:01.166774  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:45:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:01.166849  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:01.174855  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[b50998bf-1e35-4a46-bd29-73f33f9303af] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:01 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000321360 2 [] false false map[] 0xc0014ed700 0xc0010fcb00}
Jan 20 12:45:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:01.174903  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:01.484154  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:01.484225  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:01.497249  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[280a749d-a2e2-4dbf-bdb4-f831deb3f88f] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:01 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000707b80 2 [] false false map[] 0xc001846600 0xc000fd5ce0}
Jan 20 12:45:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:01.497392  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:01.582247  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:45:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:01.587727  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:45:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:01.587740  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:45:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:01.588366  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:45:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:01.588717  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:45:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:01.588762  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:45:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:01.588768  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:45:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:01.588777  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:45:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:02.483409  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:02.483478  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:02.492020  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[f11283d4-1806-42f5-90f1-5a6bcee8851d] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:02 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008e22e0 2 [] false false map[] 0xc0021a3800 0xc0019be210}
Jan 20 12:45:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:02.492050  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:02.689673  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:45:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:02.689736  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:02.690889  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:02 GMT]] 0xc000707cc0 2 [] true false map[] 0xc0021a3b00 <nil>}
Jan 20 12:45:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:02.690990  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:45:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:02.696207  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:45:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:02.696271  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:02.697338  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:02 GMT]] 0xc000707d00 2 [] true false map[] 0xc0021a3d00 <nil>}
Jan 20 12:45:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:02.697439  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:45:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:02.709542  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:45:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:02.709600  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:02.709616  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:45:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:02.709676  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:02.710629  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:02 GMT]] 0xc0008e26a0 2 [] true false map[] 0xc001846a00 <nil>}
Jan 20 12:45:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:02.710636  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:02 GMT]] 0xc000707d40 2 [] true false map[] 0xc0021a3f00 <nil>}
Jan 20 12:45:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:02.710731  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:45:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:02.710756  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:45:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:02.859970  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:45:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:03.483858  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:03.483931  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:03.491840  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[15ddfa43-7c7a-41a0-b7b2-37a6f36b614f] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:03 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00129c4a0 2 [] false false map[] 0xc0023aa100 0xc001b1fef0}
Jan 20 12:45:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:03.491872  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:03.581732  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:45:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:03.589116  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:45:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:03.589180  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:45:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:03.591355  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:45:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:03.593502  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:45:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:03.593724  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:45:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:03.593757  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:45:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:03.593800  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:45:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:04.483602  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:04.483679  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:04.491990  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[3a2171d7-5462-4893-8e67-e1f287deb6f9] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:04 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00129d040 2 [] false false map[] 0xc0023aa400 0xc001e80000}
Jan 20 12:45:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:04.492021  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:04.615119  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:45:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:04.615126  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:45:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:04.615338  199956 interface.go:209] Interface eno2 is up
Jan 20 12:45:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:04.615364  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:45:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:04.615371  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:45:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:04.615376  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:45:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:04.615379  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:45:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:04.615382  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:45:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:04.615827  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:45:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:04.615855  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:45:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:04.615860  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:45:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:04.615863  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:45:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:05.483602  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:05.483676  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:05.496560  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[070f6fef-7cd2-461a-8cf6-49d487a5296e] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:05 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008e3a40 2 [] false false map[] 0xc0023aa800 0xc00207fef0}
Jan 20 12:45:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:05.496716  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:05.581425  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[default/unsigned-unencrypted-cc-1]
Jan 20 12:45:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:05.581520  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:45:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:05.581663  199956 pod_workers.go:888] "Processing pod event" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:45:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:05.581718  199956 kubelet.go:1501] "syncPod enter" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:45:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:05.581755  199956 kubelet_pods.go:1435] "Generating pod status" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:45:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:05.581860  199956 kubelet_pods.go:1447] "Got phase for pod" pod="default/unsigned-unencrypted-cc-1" oldPhase=Pending phase=Pending
Jan 20 12:45:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:05.582136  199956 status_manager.go:535] "Ignoring same status for pod" pod="default/unsigned-unencrypted-cc-1" status={Phase:Pending Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.15 PodIPs:[{IP:10.244.0.15}] StartTime:2023-01-20 12:41:08 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:ci-example256m State:{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "ngcn-registry.sh.intel.com:443/ci-example256m:latest",} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest ImageID: ContainerID: Started:0xc00013f769}] QOSClass:Guaranteed EphemeralContainerStatuses:[]}
Jan 20 12:45:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:05.582476  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:45:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:05.582545  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:45:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:05.582592  199956 kuberuntime_manager.go:638] "Container of pod is not in the desired state and shall be started" containerName="ci-example256m" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:45:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:05.582635  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af Attempt:3 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:45:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:05.582947  199956 kuberuntime_manager.go:889] "Creating container in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:45:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:05.583936  199956 kuberuntime_manager.go:903] "Container start failed in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1" containerMessage="Back-off pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\"" err="ImagePullBackOff"
Jan 20 12:45:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:05.583973  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Normal" reason="BackOff" message="Back-off pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\""
Jan 20 12:45:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:05.583998  199956 kubelet.go:1503] "syncPod exit" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d isTerminal=false
Jan 20 12:45:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:05.584031  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Warning" reason="Failed" message="Error: ImagePullBackOff"
Jan 20 12:45:05 zcy-Z390-AORUS-MASTER kubelet[199956]: E0120 12:45:05.584046  199956 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ci-example256m\" with ImagePullBackOff: \"Back-off pulling image \\\"ngcn-registry.sh.intel.com:443/ci-example256m:latest\\\"\"" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:45:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:05.584091  199956 pod_workers.go:988] "Processing pod event done" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:45:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:05.586799  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:45:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:05.586811  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:45:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:05.586817  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:45:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:05.587451  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:45:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:05.587523  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:45:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:05.587529  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:45:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:05.587537  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:45:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:05.633955  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="default/unsigned-unencrypted-cc-1" volumeName="kube-api-access-qcb8g" volumeSpecName="kube-api-access-qcb8g"
Jan 20 12:45:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:05.653152  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") Volume is already mounted to pod, but remount was requested." pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:45:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:05.653341  199956 projected.go:183] Setting up volume kube-api-access-qcb8g for pod 4f97314d-815b-4787-b174-5c158cd28c9d at /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:45:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:05.653758  199956 atomic_writer.go:161] pod default/unsigned-unencrypted-cc-1 volume kube-api-access-qcb8g: no update required for target directory /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:45:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:05.653828  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") " pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:45:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:06.165079  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:45:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:06.165151  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:06.172272  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:06 GMT] X-Content-Type-Options:[nosniff]] 0xc0020fd220 2 [] false false map[] 0xc000cf7000 0xc002129080}
Jan 20 12:45:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:06.172311  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:45:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:06.272215  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:45:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:06.272276  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:06.273489  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:06 GMT]] 0xc0020fd2a0 29 [] true false map[] 0xc0002c3300 <nil>}
Jan 20 12:45:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:06.273595  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:45:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:06.483446  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:06.483508  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:06.491799  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[dd4b39b2-7135-4d22-bab4-d8bc450d3545] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:06 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0009a07c0 2 [] false false map[] 0xc001774900 0xc0021e5ef0}
Jan 20 12:45:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:06.491822  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:06.895991  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:45:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:06.896058  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:06.903307  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:06 GMT] X-Content-Type-Options:[nosniff]] 0xc0020fc480 2 [] false false map[] 0xc0002c2d00 0xc000afe4d0}
Jan 20 12:45:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:06.903363  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:45:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:07.483495  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:07.483564  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:07.491714  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[cd750f55-a5f7-42da-a3a5-6d5c012b6872] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:07 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0009a0460 2 [] false false map[] 0xc0002c3100 0xc001982160}
Jan 20 12:45:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:07.491745  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:07.525908  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/etcd.yaml"
Jan 20 12:45:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:07.526707  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver.yaml"
Jan 20 12:45:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:07.527743  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Jan 20 12:45:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:07.528756  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-scheduler.yaml"
Jan 20 12:45:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:07.529321  199956 config.go:279] "Setting pods for source" source="file"
Jan 20 12:45:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:07.581754  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:45:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:07.589197  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:45:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:07.589258  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:45:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:07.591524  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:45:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:07.593523  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:45:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:07.593748  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:45:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:07.593782  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:45:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:07.593827  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:45:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:07.861137  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:45:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:08.483449  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:08.483513  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:08.492016  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[1dc64e38-5741-466a-890c-859b9bb78359] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:08 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001821ce0 2 [] false false map[] 0xc001f37400 0xc000de7970}
Jan 20 12:45:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:08.492045  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:08.879191  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:45:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:08.879278  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:08.880400  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:08 GMT] X-Content-Type-Options:[nosniff]] 0xc0020fcda0 2 [] true false map[] 0xc001f37700 <nil>}
Jan 20 12:45:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:08.880525  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:45:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:09.483423  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:09.483493  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:09.491697  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[874907b1-b52b-4086-a04d-d8fd6b80ef66] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:09 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008dcd40 2 [] false false map[] 0xc001f37a00 0xc0012053f0}
Jan 20 12:45:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:09.491728  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:09.582239  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:45:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:09.587707  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:45:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:09.587719  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:45:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:09.587726  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:45:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:09.588570  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:45:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:09.588620  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:45:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:09.588626  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:45:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:09.588635  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:45:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:10.291768  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:45:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:10.299807  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:45:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:10.309979  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945631" capacity="62382080" time="2023-01-20 12:45:10.293892041 -0500 EST m=+822.849666928"
Jan 20 12:45:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:10.309993  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902002444Ki" capacity="981310056Ki" time="2023-01-20 12:45:01.262576606 -0500 EST"
Jan 20 12:45:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:10.309999  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945631" capacity="62382080" time="2023-01-20 12:45:01.262576606 -0500 EST"
Jan 20 12:45:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:10.310006  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510770" capacity="511757" time="2023-01-20 12:45:10.309655244 -0500 EST m=+822.865430069"
Jan 20 12:45:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:10.310012  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59531660Ki" capacity="65586124Ki" time="2023-01-20 12:45:10.293892041 -0500 EST m=+822.849666928"
Jan 20 12:45:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:10.310017  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64401884Ki" capacity="65061836Ki" time="2023-01-20 12:45:10.309921704 -0500 EST m=+822.865696529"
Jan 20 12:45:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:10.310023  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902002444Ki" capacity="981310056Ki" time="2023-01-20 12:45:10.293892041 -0500 EST m=+822.849666928"
Jan 20 12:45:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:10.310050  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:45:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:10.483470  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:10.483534  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:10.492154  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[b65b6d96-4750-4e8c-9e64-f153ce4fcc2e] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:10 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e9880 2 [] false false map[] 0xc0014ed300 0xc00161de40}
Jan 20 12:45:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:10.492184  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:11.166548  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:45:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:11.166617  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:11.174909  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[7fae5fbf-af05-4a9e-8470-90096ebc4eb4] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:11 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0014865a0 2 [] false false map[] 0xc000cf7b00 0xc0018c2420}
Jan 20 12:45:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:11.174937  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:11.483736  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:11.483801  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:11.491683  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[da661f25-d245-4102-a8f3-f026327c3bae] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:11 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001486a20 2 [] false false map[] 0xc0014ed600 0xc001be7ad0}
Jan 20 12:45:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:11.491714  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:11.581629  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:45:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:11.587601  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:45:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:11.587614  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:45:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:11.588286  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:45:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:11.588667  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:45:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:11.588718  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:45:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:11.588725  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:45:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:11.588734  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:45:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:12.483178  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:12.483248  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:12.491216  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[1aa02114-8d93-4d67-82db-e53ce9a01490] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:12 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001486d40 2 [] false false map[] 0xc00105b800 0xc001cea160}
Jan 20 12:45:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:12.491261  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:12.689648  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:45:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:12.689709  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:12.690670  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:12 GMT]] 0xc0014872c0 2 [] true false map[] 0xc0014edb00 <nil>}
Jan 20 12:45:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:12.690781  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:45:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:12.695852  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:45:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:12.695912  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:12.696882  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:12 GMT]] 0xc000ce9120 2 [] true false map[] 0xc0021a3a00 <nil>}
Jan 20 12:45:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:12.696982  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:45:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:12.710451  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:45:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:12.710511  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:12.710521  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:45:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:12.710582  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:12.711489  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:12 GMT]] 0xc000ffaf40 2 [] true false map[] 0xc0021a3c00 <nil>}
Jan 20 12:45:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:12.711590  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:45:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:12.711644  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:12 GMT]] 0xc001487380 2 [] true false map[] 0xc0014edd00 <nil>}
Jan 20 12:45:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:12.711705  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:45:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:12.862916  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:45:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:13.483682  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:13.483753  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:13.496636  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[08e55612-9da9-429d-929c-172dd312270b] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:13 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001487820 2 [] false false map[] 0xc001a4a200 0xc001fed550}
Jan 20 12:45:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:13.496802  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:13.582166  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:45:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:13.587738  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:45:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:13.587751  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:45:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:13.588290  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:45:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:13.588654  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:45:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:13.588707  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:45:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:13.588714  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:45:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:13.588722  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:45:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:14.483123  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:14.483193  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:14.491413  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[a3762800-8267-480d-8571-95ef45e021c6] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:14 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ce8140 2 [] false false map[] 0xc0014edf00 0xc0022120b0}
Jan 20 12:45:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:14.491449  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:14.986816  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:45:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:14.986824  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:45:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:14.986979  199956 interface.go:209] Interface eno2 is up
Jan 20 12:45:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:14.987005  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:45:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:14.987012  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:45:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:14.987017  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:45:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:14.987020  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:45:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:14.987024  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:45:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:14.987357  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:45:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:14.987387  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:45:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:14.987392  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:45:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:14.987395  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:45:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:15.483500  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:15.483561  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:15.492584  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[855e1596-9d81-4c52-bf0d-f7916bd094d0] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:15 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ce9f40 2 [] false false map[] 0xc0023aba00 0xc000b1ee70}
Jan 20 12:45:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:15.492621  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:15.581162  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:45:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:15.586732  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:45:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:15.586744  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:45:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:15.586751  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:45:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:15.587536  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:45:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:15.587615  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:45:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:15.587621  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:45:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:15.587632  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:16.165075  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:16.165146  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:16.172346  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:16 GMT] X-Content-Type-Options:[nosniff]] 0xc000ffbfa0 2 [] false false map[] 0xc001846c00 0xc001cea790}
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:16.172390  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:16.272592  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:16.272658  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:16.273884  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:16 GMT]] 0xc0009a0180 29 [] true false map[] 0xc0021a2400 <nil>}
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:16.273987  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:16.483157  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:16.483224  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:16.491165  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[ab30595d-4c3d-4100-b31c-c3e975e683b0] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:16 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0009a01c0 2 [] false false map[] 0xc001846e00 0xc000de7970}
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:16.491192  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:16.582132  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[default/unsigned-unencrypted-cc-1]
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:16.582210  199956 pod_workers.go:888] "Processing pod event" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:16.582267  199956 kubelet.go:1501] "syncPod enter" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:16.582296  199956 kubelet_pods.go:1435] "Generating pod status" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:16.582395  199956 kubelet_pods.go:1447] "Got phase for pod" pod="default/unsigned-unencrypted-cc-1" oldPhase=Pending phase=Pending
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:16.582673  199956 status_manager.go:535] "Ignoring same status for pod" pod="default/unsigned-unencrypted-cc-1" status={Phase:Pending Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.15 PodIPs:[{IP:10.244.0.15}] StartTime:2023-01-20 12:41:08 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:ci-example256m State:{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "ngcn-registry.sh.intel.com:443/ci-example256m:latest",} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest ImageID: ContainerID: Started:0xc001201bfe}] QOSClass:Guaranteed EphemeralContainerStatuses:[]}
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:16.582978  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:16.583044  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:16.583091  199956 kuberuntime_manager.go:638] "Container of pod is not in the desired state and shall be started" containerName="ci-example256m" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:16.583134  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af Attempt:3 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:16.583405  199956 kuberuntime_manager.go:889] "Creating container in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:16.584558  199956 kuberuntime_manager.go:903] "Container start failed in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1" containerMessage="Back-off pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\"" err="ImagePullBackOff"
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:16.584567  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Normal" reason="BackOff" message="Back-off pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\""
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:16.584638  199956 kubelet.go:1503] "syncPod exit" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d isTerminal=false
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:16.584645  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Warning" reason="Failed" message="Error: ImagePullBackOff"
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: E0120 12:45:16.584681  199956 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ci-example256m\" with ImagePullBackOff: \"Back-off pulling image \\\"ngcn-registry.sh.intel.com:443/ci-example256m:latest\\\"\"" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:16.584759  199956 pod_workers.go:988] "Processing pod event done" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:16.615011  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="default/unsigned-unencrypted-cc-1" volumeName="kube-api-access-qcb8g" volumeSpecName="kube-api-access-qcb8g"
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:16.637160  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") Volume is already mounted to pod, but remount was requested." pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:16.637355  199956 projected.go:183] Setting up volume kube-api-access-qcb8g for pod 4f97314d-815b-4787-b174-5c158cd28c9d at /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:16.637759  199956 atomic_writer.go:161] pod default/unsigned-unencrypted-cc-1 volume kube-api-access-qcb8g: no update required for target directory /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:16.637831  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") " pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:16.895587  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:16.895656  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:16.903204  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:16 GMT] X-Content-Type-Options:[nosniff]] 0xc0009a0580 2 [] false false map[] 0xc0021a2f00 0xc0013b4370}
Jan 20 12:45:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:16.903230  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:45:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:17.483146  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:17.483219  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:17.490638  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[46f68b48-3d37-4041-acd5-60369f111072] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:17 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000978fe0 2 [] false false map[] 0xc001847200 0xc001617550}
Jan 20 12:45:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:17.490669  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:17.581855  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:45:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:17.587679  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:45:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:17.587692  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:45:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:17.588461  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:45:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:17.588923  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:45:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:17.588969  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:45:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:17.588975  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:45:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:17.588984  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:45:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:17.864671  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:45:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:18.483106  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:18.483157  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:18.491201  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[ccf8213e-8290-4da5-bd99-9e577d3c758a] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:18 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0020fc400 2 [] false false map[] 0xc0021a3500 0xc0018af600}
Jan 20 12:45:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:18.491231  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:18.879454  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:45:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:18.879523  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:18.879546  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/healthz" timeout="1s"
Jan 20 12:45:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:18.879606  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:18.880766  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:18 GMT] X-Content-Type-Options:[nosniff]] 0xc0020fc740 2 [] true false map[] 0xc0002c3500 <nil>}
Jan 20 12:45:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:18.880804  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:18 GMT] X-Content-Type-Options:[nosniff]] 0xc001821f80 2 [] true false map[] 0xc0021a3700 <nil>}
Jan 20 12:45:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:18.880881  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:45:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:18.880921  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:45:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:19.483524  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:19.483596  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:19.491555  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[71dd2afa-b92e-48b7-9d29-1dcc2cf1c6cf] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:19 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000cb24a0 2 [] false false map[] 0xc0002c3700 0xc00163b6b0}
Jan 20 12:45:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:19.491586  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:19.581792  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:45:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:19.587686  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:45:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:19.587698  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:45:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:19.588371  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:45:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:19.588713  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:45:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:19.588757  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:45:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:19.588763  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:45:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:19.588772  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:45:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:20.310642  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:45:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:20.318697  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:45:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:20.328972  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902002404Ki" capacity="981310056Ki" time="2023-01-20 12:45:11.261679616 -0500 EST"
Jan 20 12:45:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:20.328984  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945631" capacity="62382080" time="2023-01-20 12:45:11.261679616 -0500 EST"
Jan 20 12:45:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:20.328991  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510771" capacity="511757" time="2023-01-20 12:45:20.328643205 -0500 EST m=+832.884418030"
Jan 20 12:45:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:20.328997  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59533860Ki" capacity="65586124Ki" time="2023-01-20 12:45:20.31274417 -0500 EST m=+832.868519062"
Jan 20 12:45:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:20.329003  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64402180Ki" capacity="65061836Ki" time="2023-01-20 12:45:20.328918698 -0500 EST m=+832.884693522"
Jan 20 12:45:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:20.329009  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902002404Ki" capacity="981310056Ki" time="2023-01-20 12:45:20.31274417 -0500 EST m=+832.868519062"
Jan 20 12:45:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:20.329015  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945631" capacity="62382080" time="2023-01-20 12:45:20.31274417 -0500 EST m=+832.868519062"
Jan 20 12:45:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:20.329041  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:45:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:20.355023  199956 handler.go:293] error while reading "/proc/199956/fd/34" link: readlink /proc/199956/fd/34: no such file or directory
Jan 20 12:45:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:20.483988  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:20.484058  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:20.492100  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[eba01183-f50f-4270-b415-6468bc6b2591] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:20 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e91c0 2 [] false false map[] 0xc0021a3700 0xc001983970}
Jan 20 12:45:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:20.492129  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:21.166474  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:45:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:21.166548  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:21.174914  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[0a366589-74c4-4d6d-afe8-57f6e24a4de8] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:21 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0007067c0 2 [] false false map[] 0xc0021a3a00 0xc001573ce0}
Jan 20 12:45:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:21.174959  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:21.483642  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:21.483715  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:21.496432  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[91bbff9a-1a5c-4fc8-8aa0-a865bb91448a] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:21 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008dcd40 2 [] false false map[] 0xc0021a3d00 0xc0018afe40}
Jan 20 12:45:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:21.496577  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:21.581247  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[confidential-containers-system/cc-operator-daemon-install-t6mp7]
Jan 20 12:45:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:21.581344  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:45:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:21.581483  199956 pod_workers.go:888] "Processing pod event" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" podUID=7af065b7-9095-4d91-9b9e-2644e7b1f4bf updateType=0
Jan 20 12:45:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:21.581571  199956 kubelet.go:1501] "syncPod enter" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" podUID=7af065b7-9095-4d91-9b9e-2644e7b1f4bf
Jan 20 12:45:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:21.581607  199956 kubelet_pods.go:1435] "Generating pod status" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7"
Jan 20 12:45:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:21.581697  199956 kubelet_pods.go:1447] "Got phase for pod" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" oldPhase=Running phase=Running
Jan 20 12:45:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:21.581990  199956 status_manager.go:535] "Ignoring same status for pod" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:04 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:20 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:20 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:04 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.6 PodIPs:[{IP:10.244.0.6}] StartTime:2023-01-20 12:32:04 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:cc-runtime-install-pod State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:32:20 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:quay.io/confidential-containers/runtime-payload-ci:kata-containers-amd64 ImageID:quay.io/confidential-containers/runtime-payload-ci@sha256:4736ba274765c889404fb98f01de0a997e68d2d7e5acca2440488f0e1337032b ContainerID:containerd://e10efaa459a32161059fda75d4bcaf3bbdb896781f772b508e43fe00bc7c9003 Started:0xc001f81a6e}] QOSClass:BestEffort EphemeralContainerStatuses:[]}
Jan 20 12:45:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:21.582361  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7"
Jan 20 12:45:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:21.582437  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7"
Jan 20 12:45:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:21.582861  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="confidential-containers-system/cc-operator-daemon-install-t6mp7"
Jan 20 12:45:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:21.583032  199956 kubelet.go:1503] "syncPod exit" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" podUID=7af065b7-9095-4d91-9b9e-2644e7b1f4bf isTerminal=false
Jan 20 12:45:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:21.583092  199956 pod_workers.go:988] "Processing pod event done" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" podUID=7af065b7-9095-4d91-9b9e-2644e7b1f4bf updateType=0
Jan 20 12:45:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:21.586761  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:45:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:21.586773  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:45:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:21.586778  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:45:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:21.587455  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:45:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:21.587500  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:45:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:21.587507  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:45:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:21.587515  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:45:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:21.654974  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" volumeName="containerd-conf" volumeSpecName="containerd-conf"
Jan 20 12:45:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:21.655083  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" volumeName="kata-artifacts" volumeSpecName="kata-artifacts"
Jan 20 12:45:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:21.655156  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" volumeName="dbus" volumeSpecName="dbus"
Jan 20 12:45:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:21.655224  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" volumeName="systemd" volumeSpecName="systemd"
Jan 20 12:45:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:21.655289  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" volumeName="local-bin" volumeSpecName="local-bin"
Jan 20 12:45:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:21.655428  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" volumeName="kube-api-access-x6vjr" volumeSpecName="kube-api-access-x6vjr"
Jan 20 12:45:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:21.673355  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-x6vjr\" (UniqueName: \"kubernetes.io/projected/7af065b7-9095-4d91-9b9e-2644e7b1f4bf-kube-api-access-x6vjr\") pod \"cc-operator-daemon-install-t6mp7\" (UID: \"7af065b7-9095-4d91-9b9e-2644e7b1f4bf\") Volume is already mounted to pod, but remount was requested." pod="confidential-containers-system/cc-operator-daemon-install-t6mp7"
Jan 20 12:45:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:21.673528  199956 projected.go:183] Setting up volume kube-api-access-x6vjr for pod 7af065b7-9095-4d91-9b9e-2644e7b1f4bf at /var/lib/kubelet/pods/7af065b7-9095-4d91-9b9e-2644e7b1f4bf/volumes/kubernetes.io~projected/kube-api-access-x6vjr
Jan 20 12:45:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:21.673935  199956 atomic_writer.go:161] pod confidential-containers-system/cc-operator-daemon-install-t6mp7 volume kube-api-access-x6vjr: no update required for target directory /var/lib/kubelet/pods/7af065b7-9095-4d91-9b9e-2644e7b1f4bf/volumes/kubernetes.io~projected/kube-api-access-x6vjr
Jan 20 12:45:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:21.674009  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-x6vjr\" (UniqueName: \"kubernetes.io/projected/7af065b7-9095-4d91-9b9e-2644e7b1f4bf-kube-api-access-x6vjr\") pod \"cc-operator-daemon-install-t6mp7\" (UID: \"7af065b7-9095-4d91-9b9e-2644e7b1f4bf\") " pod="confidential-containers-system/cc-operator-daemon-install-t6mp7"
Jan 20 12:45:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:22.483852  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:22.483915  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:22.491653  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[bb3dbbd8-73ca-4a14-b9f1-a1c7fb8768c1] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:22 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e9360 2 [] false false map[] 0xc0014c6100 0xc0011e36b0}
Jan 20 12:45:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:22.491683  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:22.689440  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:45:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:22.689499  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:22.690614  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:22 GMT]] 0xc0008ddbe0 2 [] true false map[] 0xc0002c2f00 <nil>}
Jan 20 12:45:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:22.690722  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:45:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:22.695955  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:45:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:22.696019  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:22.697016  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:22 GMT]] 0xc0008ddc20 2 [] true false map[] 0xc001846400 <nil>}
Jan 20 12:45:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:22.697116  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:45:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:22.710309  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:45:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:22.710372  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:22.710381  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:45:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:22.710437  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:22.711370  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:22 GMT]] 0xc000ffa080 2 [] true false map[] 0xc001846600 <nil>}
Jan 20 12:45:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:22.711441  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:22 GMT]] 0xc0020fd700 2 [] true false map[] 0xc0002c3100 <nil>}
Jan 20 12:45:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:22.711481  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:45:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:22.711541  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:45:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:22.866200  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:45:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:23.483319  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:23.483393  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:23.490965  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[eb84e924-5aa6-43ba-bcc3-d0fa4b4681a1] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:23 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008dde40 2 [] false false map[] 0xc0014c6900 0xc0012614a0}
Jan 20 12:45:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:23.491016  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:23.581864  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:45:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:23.587668  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:45:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:23.587680  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:45:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:23.588480  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:45:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:23.588889  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:45:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:23.588935  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:45:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:23.588943  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:45:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:23.588951  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:45:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:24.483827  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:24.483845  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:24.486818  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[65932826-9c53-4c63-a3f8-55668b47dfef] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:24 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0009794c0 2 [] false false map[] 0xc0016aa100 0xc0015fa420}
Jan 20 12:45:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:24.486849  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:24.581724  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[kube-system/kube-apiserver-zcy-z390-aorus-master]
Jan 20 12:45:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:24.581801  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd updateType=0
Jan 20 12:45:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:24.581859  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd
Jan 20 12:45:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:24.581893  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/kube-apiserver-zcy-z390-aorus-master"
Jan 20 12:45:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:24.581983  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" oldPhase=Running phase=Running
Jan 20 12:45:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:24.582272  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:28 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:41 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:41 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:28 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.239.159.53 PodIPs:[{IP:10.239.159.53}] StartTime:2023-01-20 12:31:28 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:kube-apiserver State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:22 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:6 Image:k8s.gcr.io/kube-apiserver:v1.24.0 ImageID:k8s.gcr.io/kube-apiserver@sha256:a04522b882e919de6141b47d72393fb01226c78e7388400f966198222558c955 ContainerID:containerd://5900ac5c1383a321a6ae837b57d6d29d7a660a102c0806210a9ea57290673062 Started:0xc0010b4d79}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:45:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:24.582592  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-apiserver-zcy-z390-aorus-master"
Jan 20 12:45:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:24.582665  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/kube-apiserver-zcy-z390-aorus-master"
Jan 20 12:45:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:24.583814  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/kube-apiserver-zcy-z390-aorus-master"
Jan 20 12:45:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:24.583976  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd isTerminal=false
Jan 20 12:45:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:24.584032  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd updateType=0
Jan 20 12:45:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:24.679529  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" volumeName="ca-certs" volumeSpecName="ca-certs"
Jan 20 12:45:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:24.679636  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Jan 20 12:45:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:24.679709  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" volumeName="etc-pki" volumeSpecName="etc-pki"
Jan 20 12:45:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:24.679776  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Jan 20 12:45:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:24.679847  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Jan 20 12:45:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:24.679915  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Jan 20 12:45:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:25.092552  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:45:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:25.092561  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:45:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:25.092716  199956 interface.go:209] Interface eno2 is up
Jan 20 12:45:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:25.092744  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:45:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:25.092752  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:45:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:25.092757  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:45:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:25.092760  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:45:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:25.092764  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:45:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:25.093011  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:45:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:25.093020  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:45:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:25.093025  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:45:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:25.093029  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:45:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:25.483756  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:25.483829  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:25.491789  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[d4a474b3-df83-435e-b443-7b829f246ea0] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:25 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0009a1d40 2 [] false false map[] 0xc0014c6e00 0xc0018c1290}
Jan 20 12:45:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:25.491820  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:25.581313  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:45:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:25.586759  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:45:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:25.586771  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:45:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:25.586777  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:45:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:25.587593  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:45:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:25.587675  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:45:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:25.587695  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:45:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:25.587703  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:45:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:26.164519  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:45:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:26.164593  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:26.172351  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:26 GMT] X-Content-Type-Options:[nosniff]] 0xc000cb2820 2 [] false false map[] 0xc001a4b000 0xc001abe2c0}
Jan 20 12:45:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:26.172433  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:45:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:26.272232  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:45:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:26.272244  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:26.272600  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:26 GMT]] 0xc001820d20 29 [] true false map[] 0xc001a4b300 <nil>}
Jan 20 12:45:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:26.272620  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:45:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:26.483654  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:26.483715  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:26.491710  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[ad053cc1-0f4b-46e3-80ec-278ccca024af] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:26 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001821040 2 [] false false map[] 0xc0014c7300 0xc001c96c60}
Jan 20 12:45:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:26.491738  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:26.896064  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:45:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:26.896132  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:26.903333  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:26 GMT] X-Content-Type-Options:[nosniff]] 0xc000cb2ca0 2 [] false false map[] 0xc001a4b600 0xc001e3be40}
Jan 20 12:45:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:26.903387  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.483212  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.483282  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.490893  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[176fd466-06d4-4825-b2c4-a4899c9d286f] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:27 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001821340 2 [] false false map[] 0xc0014c7600 0xc001f10370}
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.490924  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.525873  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/etcd.yaml"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.527969  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver.yaml"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.530894  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.531773  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-scheduler.yaml"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.532060  199956 config.go:279] "Setting pods for source" source="file"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.540700  199956 kubelet_getters.go:176] "Pod status updated" pod="kube-system/etcd-zcy-z390-aorus-master" status=Running
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.540719  199956 kubelet_getters.go:176] "Pod status updated" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" status=Running
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.540724  199956 kubelet_getters.go:176] "Pod status updated" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" status=Running
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.540727  199956 kubelet_getters.go:176] "Pod status updated" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" status=Running
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.581987  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.585863  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.585898  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.587422  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.588711  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.588861  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.588883  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.588910  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640328  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-sysctl.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640342  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-sysctl.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640348  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-sysctl.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640353  199956 manager.go:925] ignoring container "/system.slice/systemd-sysctl.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640356  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-04e472cb\\x2db6f9\\x2d4065\\x2db509\\x2dd7e1579fc48d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dhqj8d.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640362  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-04e472cb\\x2db6f9\\x2d4065\\x2db509\\x2dd7e1579fc48d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dhqj8d.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640368  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-04e472cb\\x2db6f9\\x2d4065\\x2db509\\x2dd7e1579fc48d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dhqj8d.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640372  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-cf25b9ea\\x2d6823\\x2d4eff\\x2db30d\\x2d36a09f9d897e-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dtqzsm.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640377  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-cf25b9ea\\x2d6823\\x2d4eff\\x2db30d\\x2d36a09f9d897e-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dtqzsm.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640382  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-cf25b9ea\\x2d6823\\x2d4eff\\x2db30d\\x2d36a09f9d897e-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dtqzsm.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640386  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-snap\\x2dstore-558.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640390  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-snap\\x2dstore-558.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640394  199956 manager.go:925] ignoring container "/system.slice/snap-snap\\x2dstore-558.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640397  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-resolved.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640400  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-resolved.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640403  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-resolved.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640407  199956 manager.go:925] ignoring container "/system.slice/systemd-resolved.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640410  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/session-44.scope"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640413  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/session-44.scope"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640416  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/session-44.scope", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640420  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/session-44.scope"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640423  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f78ed441ff85d669a403a76c0987f2d86913431bd96d454b1a3605bdcf0474f2-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640427  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f78ed441ff85d669a403a76c0987f2d86913431bd96d454b1a3605bdcf0474f2-rootfs.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640432  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f78ed441ff85d669a403a76c0987f2d86913431bd96d454b1a3605bdcf0474f2-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640435  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/kerneloops.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640438  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/kerneloops.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640441  199956 factory.go:255] Factory "raw" can handle container "/system.slice/kerneloops.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640445  199956 manager.go:925] ignoring container "/system.slice/kerneloops.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640448  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/avahi-daemon.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640451  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/avahi-daemon.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640454  199956 factory.go:255] Factory "raw" can handle container "/system.slice/avahi-daemon.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640458  199956 manager.go:925] ignoring container "/system.slice/avahi-daemon.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640461  199956 factory.go:262] Factory "containerd" was unable to handle container "/kata_overhead"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640463  199956 factory.go:262] Factory "systemd" was unable to handle container "/kata_overhead"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640467  199956 factory.go:255] Factory "raw" can handle container "/kata_overhead", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640471  199956 manager.go:925] ignoring container "/kata_overhead"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640473  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-modprobe.slice"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640476  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-modprobe.slice"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640479  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-modprobe.slice", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640483  199956 manager.go:925] ignoring container "/system.slice/system-modprobe.slice"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640486  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-user-1000.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640490  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-user-1000.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640494  199956 manager.go:925] ignoring container "/system.slice/run-user-1000.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640497  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/docker.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640500  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/docker.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640503  199956 factory.go:255] Factory "raw" can handle container "/system.slice/docker.socket", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640507  199956 manager.go:925] ignoring container "/system.slice/docker.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640510  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-tmpfiles-setup.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640512  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-tmpfiles-setup.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640515  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-tmpfiles-setup.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640520  199956 manager.go:925] ignoring container "/system.slice/systemd-tmpfiles-setup.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640522  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/-.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640525  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/-.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640529  199956 manager.go:925] ignoring container "/system.slice/-.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640532  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-1cb44bdbde0b41852e382d7dab7c184af8dd4e5b25d5cfe6a10a9c8ab601659d-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640536  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-1cb44bdbde0b41852e382d7dab7c184af8dd4e5b25d5cfe6a10a9c8ab601659d-rootfs.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640541  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-1cb44bdbde0b41852e382d7dab7c184af8dd4e5b25d5cfe6a10a9c8ab601659d-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640545  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/lvm2-monitor.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640548  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/lvm2-monitor.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640551  199956 factory.go:255] Factory "raw" can handle container "/system.slice/lvm2-monitor.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640555  199956 manager.go:925] ignoring container "/system.slice/lvm2-monitor.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640558  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-gtk\\x2dcommon\\x2dthemes-1535.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640561  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-gtk\\x2dcommon\\x2dthemes-1535.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640565  199956 manager.go:925] ignoring container "/system.slice/snap-gtk\\x2dcommon\\x2dthemes-1535.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640568  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-snapd-16292.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640572  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-snapd-16292.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640576  199956 manager.go:925] ignoring container "/system.slice/snap-snapd-16292.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640579  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-user-1000-gvfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640582  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-user-1000-gvfs.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640586  199956 manager.go:925] ignoring container "/system.slice/run-user-1000-gvfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640589  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-logind.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640592  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-logind.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640595  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-logind.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640599  199956 manager.go:925] ignoring container "/system.slice/systemd-logind.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640602  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-1000.slice"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640605  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-1000.slice"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640608  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-1000.slice", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640612  199956 manager.go:925] ignoring container "/user.slice/user-1000.slice"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640615  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-shm.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640619  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-shm.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640624  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-shm.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640627  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640631  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-rootfs.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640636  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640640  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-kernel-debug-tracing.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640643  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-kernel-debug-tracing.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640647  199956 manager.go:925] ignoring container "/system.slice/sys-kernel-debug-tracing.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640650  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640654  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-rootfs.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640659  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640662  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dev-mqueue.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640666  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/dev-mqueue.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640670  199956 manager.go:925] ignoring container "/system.slice/dev-mqueue.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640673  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dbus.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640675  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/dbus.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640679  199956 factory.go:255] Factory "raw" can handle container "/system.slice/dbus.socket", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640682  199956 manager.go:925] ignoring container "/system.slice/dbus.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640685  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640698  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-rootfs.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640703  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640707  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640710  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640713  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640716  199956 manager.go:925] ignoring container "/user.slice/user-0.slice"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640719  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-udevd-control.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640722  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-udevd-control.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640725  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-udevd-control.socket", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640729  199956 manager.go:925] ignoring container "/system.slice/systemd-udevd-control.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640733  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640737  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-rootfs.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640742  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640745  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640749  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-rootfs.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640754  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640758  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-tmpfiles-setup-dev.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640761  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-tmpfiles-setup-dev.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640764  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-tmpfiles-setup-dev.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640768  199956 manager.go:925] ignoring container "/system.slice/systemd-tmpfiles-setup-dev.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640771  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-shm.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640775  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-shm.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640780  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-shm.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640783  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/acpid.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640786  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/acpid.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640789  199956 factory.go:255] Factory "raw" can handle container "/system.slice/acpid.socket", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640793  199956 manager.go:925] ignoring container "/system.slice/acpid.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640796  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/thermald.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640799  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/thermald.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640802  199956 factory.go:255] Factory "raw" can handle container "/system.slice/thermald.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640806  199956 manager.go:925] ignoring container "/system.slice/thermald.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640809  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/session-42.scope"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640812  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/session-42.scope"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640815  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/session-42.scope", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640819  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/session-42.scope"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640822  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-lvm2\\x2dpvscan.slice/lvm2-pvscan@8:10.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640825  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-lvm2\\x2dpvscan.slice/lvm2-pvscan@8:10.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640829  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-lvm2\\x2dpvscan.slice/lvm2-pvscan@8:10.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640833  199956 manager.go:925] ignoring container "/system.slice/system-lvm2\\x2dpvscan.slice/lvm2-pvscan@8:10.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640836  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/blk-availability.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640839  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/blk-availability.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640842  199956 factory.go:255] Factory "raw" can handle container "/system.slice/blk-availability.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640846  199956 manager.go:925] ignoring container "/system.slice/blk-availability.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640849  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/session-40.scope"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640851  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/session-40.scope"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640855  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/session-40.scope", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640859  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/session-40.scope"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640862  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-systemd\\x2dfsck.slice"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640865  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-systemd\\x2dfsck.slice"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640868  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-systemd\\x2dfsck.slice", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640872  199956 manager.go:925] ignoring container "/system.slice/system-systemd\\x2dfsck.slice"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640875  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-remount-fs.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640878  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-remount-fs.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640881  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-remount-fs.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640885  199956 manager.go:925] ignoring container "/system.slice/systemd-remount-fs.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640888  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640892  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-rootfs.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640897  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640901  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-bare-5.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640904  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-bare-5.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640908  199956 manager.go:925] ignoring container "/system.slice/snap-bare-5.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640911  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-gnome\\x2d3\\x2d38\\x2d2004-115.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640914  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-gnome\\x2d3\\x2d38\\x2d2004-115.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640918  199956 manager.go:925] ignoring container "/system.slice/snap-gnome\\x2d3\\x2d38\\x2d2004-115.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640921  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-fsckd.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640924  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-fsckd.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640927  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-fsckd.socket", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640931  199956 manager.go:925] ignoring container "/system.slice/systemd-fsckd.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640934  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/whoopsie.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640937  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/whoopsie.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640940  199956 factory.go:255] Factory "raw" can handle container "/system.slice/whoopsie.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640944  199956 manager.go:925] ignoring container "/system.slice/whoopsie.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640946  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/cron.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640949  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/cron.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640952  199956 factory.go:255] Factory "raw" can handle container "/system.slice/cron.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640956  199956 manager.go:925] ignoring container "/system.slice/cron.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640959  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-1000.slice/session-1.scope"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640962  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-1000.slice/session-1.scope"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640965  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-1000.slice/session-1.scope", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640969  199956 manager.go:925] ignoring container "/user.slice/user-1000.slice/session-1.scope"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640972  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-machine-id-commit.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640974  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-machine-id-commit.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640978  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-machine-id-commit.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640982  199956 manager.go:925] ignoring container "/system.slice/systemd-machine-id-commit.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640985  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/accounts-daemon.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640987  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/accounts-daemon.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640990  199956 factory.go:255] Factory "raw" can handle container "/system.slice/accounts-daemon.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640995  199956 manager.go:925] ignoring container "/system.slice/accounts-daemon.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.640997  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-13149330f3752d0ff65bcac86c7b522b2040811e815fbc87e56b40b612875d5a-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641002  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-13149330f3752d0ff65bcac86c7b522b2040811e815fbc87e56b40b612875d5a-rootfs.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641006  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-13149330f3752d0ff65bcac86c7b522b2040811e815fbc87e56b40b612875d5a-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641010  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641013  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641016  199956 factory.go:255] Factory "raw" can handle container "/system.slice", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641020  199956 manager.go:925] ignoring container "/system.slice"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641022  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-shm.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641026  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-shm.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641031  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-shm.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641035  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dbus.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641038  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/dbus.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641041  199956 factory.go:255] Factory "raw" can handle container "/system.slice/dbus.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641045  199956 manager.go:925] ignoring container "/system.slice/dbus.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641047  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journald.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641050  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journald.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641053  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journald.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641057  199956 manager.go:925] ignoring container "/system.slice/systemd-journald.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641060  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/kmod-static-nodes.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641063  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/kmod-static-nodes.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641066  199956 factory.go:255] Factory "raw" can handle container "/system.slice/kmod-static-nodes.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641070  199956 manager.go:925] ignoring container "/system.slice/kmod-static-nodes.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641073  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/syslog.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641076  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/syslog.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641079  199956 factory.go:255] Factory "raw" can handle container "/system.slice/syslog.socket", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641083  199956 manager.go:925] ignoring container "/system.slice/syslog.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641085  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-snapd-ns.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641088  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-snapd-ns.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641093  199956 manager.go:925] ignoring container "/system.slice/run-snapd-ns.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641095  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-user-0.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641099  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-user-0.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641103  199956 manager.go:925] ignoring container "/system.slice/run-user-0.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641105  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-d2688d45\\x2d2487\\x2d46e7\\x2daecb\\x2de3479626909d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d4pnfq.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641110  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-d2688d45\\x2d2487\\x2d46e7\\x2daecb\\x2de3479626909d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d4pnfq.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641116  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-d2688d45\\x2d2487\\x2d46e7\\x2daecb\\x2de3479626909d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d4pnfq.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641120  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-shm.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641124  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-shm.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641129  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-shm.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641133  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/networkd-dispatcher.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641136  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/networkd-dispatcher.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641139  199956 factory.go:255] Factory "raw" can handle container "/system.slice/networkd-dispatcher.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641143  199956 manager.go:925] ignoring container "/system.slice/networkd-dispatcher.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641146  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/rtkit-daemon.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641148  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/rtkit-daemon.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641151  199956 factory.go:255] Factory "raw" can handle container "/system.slice/rtkit-daemon.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641156  199956 manager.go:925] ignoring container "/system.slice/rtkit-daemon.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641158  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journal-flush.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641161  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journal-flush.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641164  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journal-flush.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641168  199956 manager.go:925] ignoring container "/system.slice/systemd-journal-flush.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641171  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-fs-fuse-connections.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641174  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-fs-fuse-connections.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641179  199956 manager.go:925] ignoring container "/system.slice/sys-fs-fuse-connections.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641182  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-b0713fbc\\x2defc5\\x2d4044\\x2d9d08\\x2d2326a0752f87-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dgcgm6.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641186  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-b0713fbc\\x2defc5\\x2d4044\\x2d9d08\\x2d2326a0752f87-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dgcgm6.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641192  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-b0713fbc\\x2defc5\\x2d4044\\x2d9d08\\x2d2326a0752f87-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dgcgm6.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641196  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/user-runtime-dir@0.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641199  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/user-runtime-dir@0.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641202  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/user-runtime-dir@0.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641206  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/user-runtime-dir@0.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641209  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/alsa-restore.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641212  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/alsa-restore.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641215  199956 factory.go:255] Factory "raw" can handle container "/system.slice/alsa-restore.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641219  199956 manager.go:925] ignoring container "/system.slice/alsa-restore.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641222  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2d58469e7a\\x2dc331\\x2d785c\\x2dc760\\x2d93c7232334bd.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641225  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2d58469e7a\\x2dc331\\x2d785c\\x2dc760\\x2d93c7232334bd.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641230  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2d58469e7a\\x2dc331\\x2d785c\\x2dc760\\x2d93c7232334bd.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641233  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-rfkill.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641236  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-rfkill.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641239  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-rfkill.socket", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641243  199956 manager.go:925] ignoring container "/system.slice/systemd-rfkill.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641246  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/keyboard-setup.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641249  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/keyboard-setup.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641252  199956 factory.go:255] Factory "raw" can handle container "/system.slice/keyboard-setup.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641256  199956 manager.go:925] ignoring container "/system.slice/keyboard-setup.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641259  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-kernel-config.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641262  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-kernel-config.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641266  199956 manager.go:925] ignoring container "/system.slice/sys-kernel-config.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641269  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/lvm2-lvmpolld.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641272  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/lvm2-lvmpolld.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641275  199956 factory.go:255] Factory "raw" can handle container "/system.slice/lvm2-lvmpolld.socket", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641279  199956 manager.go:925] ignoring container "/system.slice/lvm2-lvmpolld.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641282  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e10efaa459a32161059fda75d4bcaf3bbdb896781f772b508e43fe00bc7c9003-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641286  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e10efaa459a32161059fda75d4bcaf3bbdb896781f772b508e43fe00bc7c9003-rootfs.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641291  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e10efaa459a32161059fda75d4bcaf3bbdb896781f772b508e43fe00bc7c9003-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641294  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-kernel-debug.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641297  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-kernel-debug.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641301  199956 manager.go:925] ignoring container "/system.slice/sys-kernel-debug.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641304  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/wpa_supplicant.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641307  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/wpa_supplicant.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641310  199956 factory.go:255] Factory "raw" can handle container "/system.slice/wpa_supplicant.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641315  199956 manager.go:925] ignoring container "/system.slice/wpa_supplicant.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641317  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641321  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-rootfs.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641326  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641330  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-shm.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641334  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-shm.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641339  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-shm.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641342  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-core20-1611.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641346  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-core20-1611.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641350  199956 manager.go:925] ignoring container "/system.slice/snap-core20-1611.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641352  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/apparmor.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641355  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/apparmor.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641358  199956 factory.go:255] Factory "raw" can handle container "/system.slice/apparmor.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641362  199956 manager.go:925] ignoring container "/system.slice/apparmor.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641365  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641369  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-rootfs.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641374  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641378  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/acpid.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641380  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/acpid.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641383  199956 factory.go:255] Factory "raw" can handle container "/system.slice/acpid.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641387  199956 manager.go:925] ignoring container "/system.slice/acpid.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641390  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-shm.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641394  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-shm.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641399  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-shm.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641403  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/uuidd.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641406  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/uuidd.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641409  199956 factory.go:255] Factory "raw" can handle container "/system.slice/uuidd.socket", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641413  199956 manager.go:925] ignoring container "/system.slice/uuidd.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641415  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-udevd-kernel.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641418  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-udevd-kernel.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641421  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-udevd-kernel.socket", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641425  199956 manager.go:925] ignoring container "/system.slice/systemd-udevd-kernel.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641428  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2d719b045b\\x2df019\\x2d025c\\x2d185d\\x2da976163f375a.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641432  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2d719b045b\\x2df019\\x2d025c\\x2d185d\\x2da976163f375a.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641436  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2d719b045b\\x2df019\\x2d025c\\x2d185d\\x2da976163f375a.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641440  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-186424b47c7b9022ecfe8996dc73cd9101f7539259cd65914279c84c9a02a288-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641444  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-186424b47c7b9022ecfe8996dc73cd9101f7539259cd65914279c84c9a02a288-rootfs.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641449  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-186424b47c7b9022ecfe8996dc73cd9101f7539259cd65914279c84c9a02a288-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641452  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/proc-sys-fs-binfmt_misc.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641456  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/proc-sys-fs-binfmt_misc.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641460  199956 manager.go:925] ignoring container "/system.slice/proc-sys-fs-binfmt_misc.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641463  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-shm.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641467  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-shm.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641472  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-shm.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641475  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/switcheroo-control.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641478  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/switcheroo-control.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641481  199956 factory.go:255] Factory "raw" can handle container "/system.slice/switcheroo-control.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641485  199956 manager.go:925] ignoring container "/system.slice/switcheroo-control.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641488  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-modules-load.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641491  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-modules-load.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641494  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-modules-load.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641499  199956 manager.go:925] ignoring container "/system.slice/systemd-modules-load.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641502  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/cups.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641504  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/cups.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641507  199956 factory.go:255] Factory "raw" can handle container "/system.slice/cups.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641511  199956 manager.go:925] ignoring container "/system.slice/cups.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641514  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snapd.apparmor.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641517  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/snapd.apparmor.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641520  199956 factory.go:255] Factory "raw" can handle container "/system.slice/snapd.apparmor.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641524  199956 manager.go:925] ignoring container "/system.slice/snapd.apparmor.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641527  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f8443d2868215ec42d3fa335663976e33df166c5e98369aa3f9d7ddb9235bead-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641531  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f8443d2868215ec42d3fa335663976e33df166c5e98369aa3f9d7ddb9235bead-rootfs.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641536  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f8443d2868215ec42d3fa335663976e33df166c5e98369aa3f9d7ddb9235bead-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641539  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journald-audit.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641542  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journald-audit.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641545  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journald-audit.socket", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641549  199956 manager.go:925] ignoring container "/system.slice/systemd-journald-audit.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641552  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-user-sessions.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641555  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-user-sessions.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641558  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-user-sessions.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641562  199956 manager.go:925] ignoring container "/system.slice/systemd-user-sessions.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641565  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-snapd-ns-snap\\x2dstore.mnt.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641568  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-snapd-ns-snap\\x2dstore.mnt.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641572  199956 manager.go:925] ignoring container "/system.slice/run-snapd-ns-snap\\x2dstore.mnt.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641575  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/polkit.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641578  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/polkit.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641581  199956 factory.go:255] Factory "raw" can handle container "/system.slice/polkit.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641585  199956 manager.go:925] ignoring container "/system.slice/polkit.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641588  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journald-dev-log.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641591  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journald-dev-log.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641594  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journald-dev-log.socket", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641598  199956 manager.go:925] ignoring container "/system.slice/systemd-journald-dev-log.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641601  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/uuidd.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641603  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/uuidd.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641606  199956 factory.go:255] Factory "raw" can handle container "/system.slice/uuidd.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641611  199956 manager.go:925] ignoring container "/system.slice/uuidd.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641613  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641617  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-rootfs.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641622  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641626  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-shm.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641630  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-shm.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641635  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-shm.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641639  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journald.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641642  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journald.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641645  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journald.socket", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641649  199956 manager.go:925] ignoring container "/system.slice/systemd-journald.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641651  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/upower.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641654  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/upower.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641657  199956 factory.go:255] Factory "raw" can handle container "/system.slice/upower.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641661  199956 manager.go:925] ignoring container "/system.slice/upower.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641664  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-lvm2\\x2dpvscan.slice"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641667  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-lvm2\\x2dpvscan.slice"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641670  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-lvm2\\x2dpvscan.slice", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641674  199956 manager.go:925] ignoring container "/system.slice/system-lvm2\\x2dpvscan.slice"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641677  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e0c5b991f81d5128566686552e45a9bbc8c584e4f6c94909edb3a42720dacc90-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641681  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e0c5b991f81d5128566686552e45a9bbc8c584e4f6c94909edb3a42720dacc90-rootfs.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641686  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e0c5b991f81d5128566686552e45a9bbc8c584e4f6c94909edb3a42720dacc90-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641690  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-udevd.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641693  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-udevd.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641696  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-udevd.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641700  199956 manager.go:925] ignoring container "/system.slice/systemd-udevd.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641703  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-random-seed.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641706  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-random-seed.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641709  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-random-seed.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641713  199956 manager.go:925] ignoring container "/system.slice/systemd-random-seed.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641716  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-d94e1927\\x2dd22d\\x2d4831\\x2da764\\x2d0170eae346a1-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d9qh7j.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641720  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-d94e1927\\x2dd22d\\x2d4831\\x2da764\\x2d0170eae346a1-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d9qh7j.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641726  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-d94e1927\\x2dd22d\\x2d4831\\x2da764\\x2d0170eae346a1-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d9qh7j.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641730  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e489181a7f95431b6d92f037dbe3f788b46a5e024df7aaf29e0115dab1168ab1-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641734  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e489181a7f95431b6d92f037dbe3f788b46a5e024df7aaf29e0115dab1168ab1-rootfs.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641739  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e489181a7f95431b6d92f037dbe3f788b46a5e024df7aaf29e0115dab1168ab1-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641742  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-9b11acac1b891eafc7ff2b244f1c42938f71a575ce81ff917e3b7ebf61d2e045-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641746  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-9b11acac1b891eafc7ff2b244f1c42938f71a575ce81ff917e3b7ebf61d2e045-rootfs.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641751  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-9b11acac1b891eafc7ff2b244f1c42938f71a575ce81ff917e3b7ebf61d2e045-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641755  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-1c057a0e\\x2d3ee3\\x2d44f3\\x2db294\\x2d434acc8f9491-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d2sbqp.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641759  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-1c057a0e\\x2d3ee3\\x2d44f3\\x2db294\\x2d434acc8f9491-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d2sbqp.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641764  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-1c057a0e\\x2d3ee3\\x2d44f3\\x2db294\\x2d434acc8f9491-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d2sbqp.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641770  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-timesyncd.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641773  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-timesyncd.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641777  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-timesyncd.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641781  199956 manager.go:925] ignoring container "/system.slice/systemd-timesyncd.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641784  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/bluetooth.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641787  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/bluetooth.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641790  199956 factory.go:255] Factory "raw" can handle container "/system.slice/bluetooth.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641794  199956 manager.go:925] ignoring container "/system.slice/bluetooth.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641796  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/ufw.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641799  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/ufw.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641803  199956 factory.go:255] Factory "raw" can handle container "/system.slice/ufw.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641806  199956 manager.go:925] ignoring container "/system.slice/ufw.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641809  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-kernel-tracing.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641812  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-kernel-tracing.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641816  199956 manager.go:925] ignoring container "/system.slice/sys-kernel-tracing.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641819  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641823  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-rootfs.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641828  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641832  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/irqbalance.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641834  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/irqbalance.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641837  199956 factory.go:255] Factory "raw" can handle container "/system.slice/irqbalance.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641842  199956 manager.go:925] ignoring container "/system.slice/irqbalance.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641844  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-udev-trigger.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641847  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-udev-trigger.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641851  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-udev-trigger.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641855  199956 manager.go:925] ignoring container "/system.slice/systemd-udev-trigger.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641858  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snapd.seeded.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641861  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/snapd.seeded.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641864  199956 factory.go:255] Factory "raw" can handle container "/system.slice/snapd.seeded.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641868  199956 manager.go:925] ignoring container "/system.slice/snapd.seeded.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641870  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/ModemManager.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641873  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/ModemManager.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641877  199956 factory.go:255] Factory "raw" can handle container "/system.slice/ModemManager.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641881  199956 manager.go:925] ignoring container "/system.slice/ModemManager.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641884  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/NetworkManager.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641886  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/NetworkManager.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641889  199956 factory.go:255] Factory "raw" can handle container "/system.slice/NetworkManager.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641893  199956 manager.go:925] ignoring container "/system.slice/NetworkManager.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641896  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/openvpn.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641899  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/openvpn.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641902  199956 factory.go:255] Factory "raw" can handle container "/system.slice/openvpn.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641906  199956 manager.go:925] ignoring container "/system.slice/openvpn.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641909  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/apport.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641912  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/apport.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641915  199956 factory.go:255] Factory "raw" can handle container "/system.slice/apport.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641919  199956 manager.go:925] ignoring container "/system.slice/apport.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641922  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-1000.slice/user@1000.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641924  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-1000.slice/user@1000.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641927  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-1000.slice/user@1000.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641932  199956 manager.go:925] ignoring container "/user.slice/user-1000.slice/user@1000.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641934  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/containerd.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641937  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/containerd.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641940  199956 factory.go:255] Factory "raw" can handle container "/system.slice/containerd.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641944  199956 manager.go:925] ignoring container "/system.slice/containerd.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641947  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-7af065b7\\x2d9095\\x2d4d91\\x2d9b9e\\x2d2644e7b1f4bf-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dx6vjr.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641952  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-7af065b7\\x2d9095\\x2d4d91\\x2d9b9e\\x2d2644e7b1f4bf-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dx6vjr.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641957  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-7af065b7\\x2d9095\\x2d4d91\\x2d9b9e\\x2d2644e7b1f4bf-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dx6vjr.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641961  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-5900ac5c1383a321a6ae837b57d6d29d7a660a102c0806210a9ea57290673062-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641965  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-5900ac5c1383a321a6ae837b57d6d29d7a660a102c0806210a9ea57290673062-rootfs.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641970  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-5900ac5c1383a321a6ae837b57d6d29d7a660a102c0806210a9ea57290673062-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641974  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/NetworkManager-wait-online.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641977  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/NetworkManager-wait-online.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641980  199956 factory.go:255] Factory "raw" can handle container "/system.slice/NetworkManager-wait-online.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641985  199956 manager.go:925] ignoring container "/system.slice/NetworkManager-wait-online.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641987  199956 factory.go:262] Factory "containerd" was unable to handle container "/docker"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641990  199956 factory.go:262] Factory "systemd" was unable to handle container "/docker"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641993  199956 factory.go:255] Factory "raw" can handle container "/docker", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641997  199956 manager.go:925] ignoring container "/docker"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.641999  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642002  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642005  199956 factory.go:255] Factory "raw" can handle container "/user.slice", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642009  199956 manager.go:925] ignoring container "/user.slice"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642012  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/user@0.service/dbus.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642015  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/user@0.service/dbus.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642018  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/user@0.service/dbus.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642022  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/user@0.service/dbus.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642025  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-0f9ae677fe935afcf43e145281deae0d663ba95fda6759ff24f0dd3e1cee17a9-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642029  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-0f9ae677fe935afcf43e145281deae0d663ba95fda6759ff24f0dd3e1cee17a9-rootfs.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642035  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-0f9ae677fe935afcf43e145281deae0d663ba95fda6759ff24f0dd3e1cee17a9-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642038  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/docker.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642041  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/docker.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642044  199956 factory.go:255] Factory "raw" can handle container "/system.slice/docker.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642048  199956 manager.go:925] ignoring container "/system.slice/docker.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642051  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snapd.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642053  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/snapd.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642056  199956 factory.go:255] Factory "raw" can handle container "/system.slice/snapd.socket", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642060  199956 manager.go:925] ignoring container "/system.slice/snapd.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642063  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2d562f9e70\\x2db2b7\\x2d3ac4\\x2d2311\\x2db91510a5a9ac.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642067  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2d562f9e70\\x2db2b7\\x2d3ac4\\x2d2311\\x2db91510a5a9ac.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642071  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2d562f9e70\\x2db2b7\\x2d3ac4\\x2d2311\\x2db91510a5a9ac.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642075  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/cups-browsed.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642077  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/cups-browsed.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642080  199956 factory.go:255] Factory "raw" can handle container "/system.slice/cups-browsed.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642084  199956 manager.go:925] ignoring container "/system.slice/cups-browsed.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642087  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-1000.slice/user-runtime-dir@1000.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642090  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-1000.slice/user-runtime-dir@1000.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642094  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-1000.slice/user-runtime-dir@1000.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642098  199956 manager.go:925] ignoring container "/user.slice/user-1000.slice/user-runtime-dir@1000.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642101  199956 factory.go:262] Factory "containerd" was unable to handle container "/init.scope"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642103  199956 factory.go:262] Factory "systemd" was unable to handle container "/init.scope"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642106  199956 factory.go:255] Factory "raw" can handle container "/init.scope", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642110  199956 manager.go:925] ignoring container "/init.scope"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642113  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-sysusers.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642116  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-sysusers.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642119  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-sysusers.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642123  199956 manager.go:925] ignoring container "/system.slice/systemd-sysusers.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642126  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/user@0.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642129  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/user@0.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642132  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/user@0.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642136  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/user@0.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642139  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-update-utmp.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642142  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-update-utmp.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642145  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-update-utmp.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642149  199956 manager.go:925] ignoring container "/system.slice/systemd-update-utmp.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642152  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-initctl.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642155  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-initctl.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642158  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-initctl.socket", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642162  199956 manager.go:925] ignoring container "/system.slice/systemd-initctl.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642165  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-shm.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642169  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-shm.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642174  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-shm.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642177  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-shm.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642181  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-shm.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642186  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-shm.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642190  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/udisks2.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642192  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/udisks2.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642195  199956 factory.go:255] Factory "raw" can handle container "/system.slice/udisks2.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642199  199956 manager.go:925] ignoring container "/system.slice/udisks2.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642202  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dm-event.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642205  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/dm-event.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642208  199956 factory.go:255] Factory "raw" can handle container "/system.slice/dm-event.socket", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642212  199956 manager.go:925] ignoring container "/system.slice/dm-event.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642215  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642219  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-rootfs.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642224  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642228  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/rsyslog.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642230  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/rsyslog.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642234  199956 factory.go:255] Factory "raw" can handle container "/system.slice/rsyslog.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642238  199956 manager.go:925] ignoring container "/system.slice/rsyslog.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642240  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dev-hugepages.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642243  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/dev-hugepages.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642248  199956 manager.go:925] ignoring container "/system.slice/dev-hugepages.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642251  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-getty.slice"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642254  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-getty.slice"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642257  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-getty.slice", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642260  199956 manager.go:925] ignoring container "/system.slice/system-getty.slice"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642263  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/ssh.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642266  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/ssh.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642269  199956 factory.go:255] Factory "raw" can handle container "/system.slice/ssh.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642273  199956 manager.go:925] ignoring container "/system.slice/ssh.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642275  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2d8b7da23a\\x2d3511\\x2dfe06\\x2d72d7\\x2d26a549eb607d.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642279  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2d8b7da23a\\x2d3511\\x2dfe06\\x2d72d7\\x2d26a549eb607d.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642284  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2d8b7da23a\\x2d3511\\x2dfe06\\x2d72d7\\x2d26a549eb607d.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642287  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/colord.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642290  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/colord.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642293  199956 factory.go:255] Factory "raw" can handle container "/system.slice/colord.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642297  199956 manager.go:925] ignoring container "/system.slice/colord.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642300  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/gdm.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642302  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/gdm.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642305  199956 factory.go:255] Factory "raw" can handle container "/system.slice/gdm.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642309  199956 manager.go:925] ignoring container "/system.slice/gdm.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642312  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/avahi-daemon.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642315  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/avahi-daemon.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642318  199956 factory.go:255] Factory "raw" can handle container "/system.slice/avahi-daemon.socket", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642322  199956 manager.go:925] ignoring container "/system.slice/avahi-daemon.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642324  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b3af3b285c950aba4fcd0176473261f7f4700aeaafe9c73ae15b15a7d6304092-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642329  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b3af3b285c950aba4fcd0176473261f7f4700aeaafe9c73ae15b15a7d6304092-rootfs.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642334  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b3af3b285c950aba4fcd0176473261f7f4700aeaafe9c73ae15b15a7d6304092-rootfs.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642337  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-systemd\\x2dfsck.slice/systemd-fsck@dev-disk-by\\x2duuid-4B5A\\x2dDC89.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642341  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-systemd\\x2dfsck.slice/systemd-fsck@dev-disk-by\\x2duuid-4B5A\\x2dDC89.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642345  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-systemd\\x2dfsck.slice/systemd-fsck@dev-disk-by\\x2duuid-4B5A\\x2dDC89.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642350  199956 manager.go:925] ignoring container "/system.slice/system-systemd\\x2dfsck.slice/systemd-fsck@dev-disk-by\\x2duuid-4B5A\\x2dDC89.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642353  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/cups.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642356  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/cups.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642359  199956 factory.go:255] Factory "raw" can handle container "/system.slice/cups.socket", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642363  199956 manager.go:925] ignoring container "/system.slice/cups.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642365  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/unattended-upgrades.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642368  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/unattended-upgrades.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642371  199956 factory.go:255] Factory "raw" can handle container "/system.slice/unattended-upgrades.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642379  199956 manager.go:925] ignoring container "/system.slice/unattended-upgrades.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642382  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/user@0.service/dbus.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642385  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/user@0.service/dbus.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642388  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/user@0.service/dbus.socket", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642392  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/user@0.service/dbus.socket"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642395  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-shm.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642399  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-shm.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642404  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-shm.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642408  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/boot-efi.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642411  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/boot-efi.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642415  199956 manager.go:925] ignoring container "/system.slice/boot-efi.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642417  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/console-setup.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642420  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/console-setup.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642423  199956 factory.go:255] Factory "raw" can handle container "/system.slice/console-setup.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642427  199956 manager.go:925] ignoring container "/system.slice/console-setup.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642430  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2dd678d9bc\\x2d8fcb\\x2d9fbe\\x2d1142\\x2ddede54862bab.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642434  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2dd678d9bc\\x2d8fcb\\x2d9fbe\\x2d1142\\x2ddede54862bab.mount", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642439  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2dd678d9bc\\x2d8fcb\\x2d9fbe\\x2d1142\\x2ddede54862bab.mount"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642442  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snapd.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642445  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/snapd.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642448  199956 factory.go:255] Factory "raw" can handle container "/system.slice/snapd.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642452  199956 manager.go:925] ignoring container "/system.slice/snapd.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642455  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/setvtrgb.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642457  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/setvtrgb.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642460  199956 factory.go:255] Factory "raw" can handle container "/system.slice/setvtrgb.service", but ignoring.
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.642465  199956 manager.go:925] ignoring container "/system.slice/setvtrgb.service"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.647346  199956 qos_container_manager_linux.go:379] "Updated QoS cgroup configuration"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.679820  199956 kuberuntime_gc.go:171] "Removing sandbox" sandboxID="f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:45:27.680289819-05:00" level=info msg="StopPodSandbox for \"f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615\""
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:45:27.692151351-05:00" level=info msg="TearDown network for sandbox \"f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615\" successfully"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:45:27.692219554-05:00" level=info msg="StopPodSandbox for \"f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615\" returns successfully"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:45:27.692933597-05:00" level=info msg="RemovePodSandbox for \"f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615\""
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:45:27.693023772-05:00" level=info msg="Forcibly stopping sandbox \"f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615\""
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:45:27.704503005-05:00" level=info msg="TearDown network for sandbox \"f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615\" successfully"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:45:27.710470950-05:00" level=info msg="RemovePodSandbox \"f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615\" returns successfully"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.711911  199956 kubelet.go:1280] "Container garbage collection succeeded"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.867933  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.945415  199956 generic.go:155] "GenericPLEG" podUID=4f97314d-815b-4787-b174-5c158cd28c9d containerID="f272c26d889ab32ac92f7da0ef36925bdb8f93ea2d48f4844a61f5a7ac053615" oldState=exited newState=non-existent
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.946271  199956 kuberuntime_manager.go:1037] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af] pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:45:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:27.947772  199956 generic.go:421] "PLEG: Write status" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:45:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:28.483489  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:28.483562  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:28.491913  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[23087005-0aa7-440f-a05b-3afd848f3a21] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:28 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0014867a0 2 [] false false map[] 0xc000cf6300 0xc0015f20b0}
Jan 20 12:45:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:28.491943  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:28.581720  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[kube-system/kube-controller-manager-zcy-z390-aorus-master]
Jan 20 12:45:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:28.581801  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce updateType=0
Jan 20 12:45:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:28.581860  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce
Jan 20 12:45:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:28.581891  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master"
Jan 20 12:45:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:28.581977  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" oldPhase=Running phase=Running
Jan 20 12:45:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:28.582250  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:28 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:37 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:37 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:28 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.239.159.53 PodIPs:[{IP:10.239.159.53}] StartTime:2023-01-20 12:31:28 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:kube-controller-manager State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:22 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:6 Image:k8s.gcr.io/kube-controller-manager:v1.24.0 ImageID:k8s.gcr.io/kube-controller-manager@sha256:df044a154e79a18f749d3cd9d958c3edde2b6a00c815176472002b7bbf956637 ContainerID:containerd://f8443d2868215ec42d3fa335663976e33df166c5e98369aa3f9d7ddb9235bead Started:0xc001f847fe}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:45:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:28.582564  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master"
Jan 20 12:45:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:28.582640  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master"
Jan 20 12:45:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:28.583549  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/kube-controller-manager-zcy-z390-aorus-master"
Jan 20 12:45:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:28.583708  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce isTerminal=false
Jan 20 12:45:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:28.583763  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce updateType=0
Jan 20 12:45:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:28.608237  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="ca-certs" volumeSpecName="ca-certs"
Jan 20 12:45:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:28.608341  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Jan 20 12:45:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:28.608419  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="etc-pki" volumeSpecName="etc-pki"
Jan 20 12:45:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:28.608486  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="flexvolume-dir" volumeSpecName="flexvolume-dir"
Jan 20 12:45:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:28.608549  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Jan 20 12:45:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:28.608613  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Jan 20 12:45:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:28.608679  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Jan 20 12:45:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:28.608778  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Jan 20 12:45:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:28.878975  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:45:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:28.879036  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:28.880211  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:28 GMT] X-Content-Type-Options:[nosniff]] 0xc0007068c0 2 [] true false map[] 0xc000fccf00 <nil>}
Jan 20 12:45:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:28.880324  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.483129  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.483202  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.490932  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[13fd6a42-eeec-4d8b-aea0-d8feb41a3ae7] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:29 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001486aa0 2 [] false false map[] 0xc000fcd100 0xc0015fa840}
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.490960  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.581902  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=2 pods=[kube-system/coredns-6d4b75cb6d-zdl2m kube-system/kube-proxy-prhfv]
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.581992  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.582056  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e updateType=0
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.582081  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/kube-proxy-prhfv" podUID=1c057a0e-3ee3-44f3-b294-434acc8f9491 updateType=0
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.582142  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.582171  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/kube-proxy-prhfv" podUID=1c057a0e-3ee3-44f3-b294-434acc8f9491
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.582176  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.582203  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/kube-proxy-prhfv"
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.582280  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/coredns-6d4b75cb6d-zdl2m" oldPhase=Running phase=Running
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.582286  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/kube-proxy-prhfv" oldPhase=Running phase=Running
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.582579  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/coredns-6d4b75cb6d-zdl2m" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:44 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:44 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.3 PodIPs:[{IP:10.244.0.3}] StartTime:2023-01-20 12:31:42 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:coredns State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:44 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:k8s.gcr.io/coredns/coredns:v1.8.6 ImageID:k8s.gcr.io/coredns/coredns@sha256:5b6ec0d6de9baaf3e92d0f66cd96a25b9edbce8716f5f15dcd1a616b3abd590e ContainerID:containerd://e489181a7f95431b6d92f037dbe3f788b46a5e024df7aaf29e0115dab1168ab1 Started:0xc00114bd69}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.582914  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/kube-proxy-prhfv" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:43 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:43 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.239.159.53 PodIPs:[{IP:10.239.159.53}] StartTime:2023-01-20 12:31:42 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:kube-proxy State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:43 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:k8s.gcr.io/kube-proxy:v1.24.0 ImageID:k8s.gcr.io/kube-proxy@sha256:c957d602267fa61082ab8847914b2118955d0739d592cc7b01e278513478d6a8 ContainerID:containerd://f78ed441ff85d669a403a76c0987f2d86913431bd96d454b1a3605bdcf0474f2 Started:0xc00101f9ee}] QOSClass:BestEffort EphemeralContainerStatuses:[]}
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.582945  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.583014  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.583255  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-proxy-prhfv"
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.583325  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/kube-proxy-prhfv"
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.583509  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.583684  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e isTerminal=false
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.583725  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/kube-proxy-prhfv"
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.583742  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e updateType=0
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.583881  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/kube-proxy-prhfv" podUID=1c057a0e-3ee3-44f3-b294-434acc8f9491 isTerminal=false
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.583935  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/kube-proxy-prhfv" podUID=1c057a0e-3ee3-44f3-b294-434acc8f9491 updateType=0
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.587747  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.587760  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.588312  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.588638  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.588683  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.588696  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.588705  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.616921  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-proxy-prhfv" volumeName="kube-proxy" volumeSpecName="kube-proxy"
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.616954  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-proxy-prhfv" volumeName="xtables-lock" volumeSpecName="xtables-lock"
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.616975  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-proxy-prhfv" volumeName="lib-modules" volumeSpecName="lib-modules"
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.617014  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-proxy-prhfv" volumeName="kube-api-access-2sbqp" volumeSpecName="kube-api-access-2sbqp"
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.617049  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/coredns-6d4b75cb6d-zdl2m" volumeName="config-volume" volumeSpecName="config-volume"
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.617080  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/coredns-6d4b75cb6d-zdl2m" volumeName="kube-api-access-tqzsm" volumeSpecName="kube-api-access-tqzsm"
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.633316  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-2sbqp\" (UniqueName: \"kubernetes.io/projected/1c057a0e-3ee3-44f3-b294-434acc8f9491-kube-api-access-2sbqp\") pod \"kube-proxy-prhfv\" (UID: \"1c057a0e-3ee3-44f3-b294-434acc8f9491\") Volume is already mounted to pod, but remount was requested." pod="kube-system/kube-proxy-prhfv"
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.633440  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/1c057a0e-3ee3-44f3-b294-434acc8f9491-kube-proxy\") pod \"kube-proxy-prhfv\" (UID: \"1c057a0e-3ee3-44f3-b294-434acc8f9491\") Volume is already mounted to pod, but remount was requested." pod="kube-system/kube-proxy-prhfv"
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.633519  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/cf25b9ea-6823-4eff-b30d-36a09f9d897e-config-volume\") pod \"coredns-6d4b75cb6d-zdl2m\" (UID: \"cf25b9ea-6823-4eff-b30d-36a09f9d897e\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.633590  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-tqzsm\" (UniqueName: \"kubernetes.io/projected/cf25b9ea-6823-4eff-b30d-36a09f9d897e-kube-api-access-tqzsm\") pod \"coredns-6d4b75cb6d-zdl2m\" (UID: \"cf25b9ea-6823-4eff-b30d-36a09f9d897e\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.633629  199956 projected.go:183] Setting up volume kube-api-access-2sbqp for pod 1c057a0e-3ee3-44f3-b294-434acc8f9491 at /var/lib/kubelet/pods/1c057a0e-3ee3-44f3-b294-434acc8f9491/volumes/kubernetes.io~projected/kube-api-access-2sbqp
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.633692  199956 configmap.go:181] Setting up volume kube-proxy for pod 1c057a0e-3ee3-44f3-b294-434acc8f9491 at /var/lib/kubelet/pods/1c057a0e-3ee3-44f3-b294-434acc8f9491/volumes/kubernetes.io~configmap/kube-proxy
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.633716  199956 projected.go:183] Setting up volume kube-api-access-tqzsm for pod cf25b9ea-6823-4eff-b30d-36a09f9d897e at /var/lib/kubelet/pods/cf25b9ea-6823-4eff-b30d-36a09f9d897e/volumes/kubernetes.io~projected/kube-api-access-tqzsm
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.633773  199956 configmap.go:205] Received configMap kube-system/kube-proxy containing (2) pieces of data, 1458 total bytes
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.633788  199956 configmap.go:181] Setting up volume config-volume for pod cf25b9ea-6823-4eff-b30d-36a09f9d897e at /var/lib/kubelet/pods/cf25b9ea-6823-4eff-b30d-36a09f9d897e/volumes/kubernetes.io~configmap/config-volume
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.633849  199956 quota_linux.go:271] SupportsQuotas called, but quotas disabled
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.633869  199956 configmap.go:205] Received configMap kube-system/coredns containing (1) pieces of data, 339 total bytes
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.633943  199956 quota_linux.go:271] SupportsQuotas called, but quotas disabled
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.634116  199956 atomic_writer.go:161] pod kube-system/kube-proxy-prhfv volume kube-proxy: no update required for target directory /var/lib/kubelet/pods/1c057a0e-3ee3-44f3-b294-434acc8f9491/volumes/kubernetes.io~configmap/kube-proxy
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.634126  199956 atomic_writer.go:161] pod kube-system/kube-proxy-prhfv volume kube-api-access-2sbqp: no update required for target directory /var/lib/kubelet/pods/1c057a0e-3ee3-44f3-b294-434acc8f9491/volumes/kubernetes.io~projected/kube-api-access-2sbqp
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.634134  199956 atomic_writer.go:161] pod kube-system/coredns-6d4b75cb6d-zdl2m volume kube-api-access-tqzsm: no update required for target directory /var/lib/kubelet/pods/cf25b9ea-6823-4eff-b30d-36a09f9d897e/volumes/kubernetes.io~projected/kube-api-access-tqzsm
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.634177  199956 atomic_writer.go:161] pod kube-system/coredns-6d4b75cb6d-zdl2m volume config-volume: no update required for target directory /var/lib/kubelet/pods/cf25b9ea-6823-4eff-b30d-36a09f9d897e/volumes/kubernetes.io~configmap/config-volume
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.634192  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/1c057a0e-3ee3-44f3-b294-434acc8f9491-kube-proxy\") pod \"kube-proxy-prhfv\" (UID: \"1c057a0e-3ee3-44f3-b294-434acc8f9491\") " pod="kube-system/kube-proxy-prhfv"
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.634207  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-tqzsm\" (UniqueName: \"kubernetes.io/projected/cf25b9ea-6823-4eff-b30d-36a09f9d897e-kube-api-access-tqzsm\") pod \"coredns-6d4b75cb6d-zdl2m\" (UID: \"cf25b9ea-6823-4eff-b30d-36a09f9d897e\") " pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.634200  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-2sbqp\" (UniqueName: \"kubernetes.io/projected/1c057a0e-3ee3-44f3-b294-434acc8f9491-kube-api-access-2sbqp\") pod \"kube-proxy-prhfv\" (UID: \"1c057a0e-3ee3-44f3-b294-434acc8f9491\") " pod="kube-system/kube-proxy-prhfv"
Jan 20 12:45:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:29.634254  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/cf25b9ea-6823-4eff-b30d-36a09f9d897e-config-volume\") pod \"coredns-6d4b75cb6d-zdl2m\" (UID: \"cf25b9ea-6823-4eff-b30d-36a09f9d897e\") " pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:45:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:30.329628  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:45:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:30.337780  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:45:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:30.347678  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59533776Ki" capacity="65586124Ki" time="2023-01-20 12:45:30.331635 -0500 EST m=+842.887409892"
Jan 20 12:45:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:30.347691  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64401960Ki" capacity="65061836Ki" time="2023-01-20 12:45:30.347617632 -0500 EST m=+842.903392457"
Jan 20 12:45:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:30.347698  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902002260Ki" capacity="981310056Ki" time="2023-01-20 12:45:30.331635 -0500 EST m=+842.887409892"
Jan 20 12:45:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:30.347705  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:45:30.331635 -0500 EST m=+842.887409892"
Jan 20 12:45:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:30.347711  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902002260Ki" capacity="981310056Ki" time="2023-01-20 12:45:21.262596678 -0500 EST"
Jan 20 12:45:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:30.347717  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:45:21.262596678 -0500 EST"
Jan 20 12:45:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:30.347723  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510771" capacity="511757" time="2023-01-20 12:45:30.347352306 -0500 EST m=+842.903127131"
Jan 20 12:45:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:30.347749  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:45:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:30.483964  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:30.484032  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:30.492104  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[e12239b6-2d45-4e20-86f3-721b785ce16c] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:30 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000707aa0 2 [] false false map[] 0xc000cf7b00 0xc0012708f0}
Jan 20 12:45:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:30.492134  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:30.581962  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[default/unsigned-unencrypted-cc-1]
Jan 20 12:45:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:30.582042  199956 pod_workers.go:888] "Processing pod event" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:45:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:30.582099  199956 kubelet.go:1501] "syncPod enter" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:45:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:30.582129  199956 kubelet_pods.go:1435] "Generating pod status" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:45:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:30.582228  199956 kubelet_pods.go:1447] "Got phase for pod" pod="default/unsigned-unencrypted-cc-1" oldPhase=Pending phase=Pending
Jan 20 12:45:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:30.582501  199956 status_manager.go:535] "Ignoring same status for pod" pod="default/unsigned-unencrypted-cc-1" status={Phase:Pending Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.15 PodIPs:[{IP:10.244.0.15}] StartTime:2023-01-20 12:41:08 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:ci-example256m State:{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "ngcn-registry.sh.intel.com:443/ci-example256m:latest",} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest ImageID: ContainerID: Started:0xc001746ae9}] QOSClass:Guaranteed EphemeralContainerStatuses:[]}
Jan 20 12:45:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:30.582802  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:45:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:30.582864  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:45:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:30.582911  199956 kuberuntime_manager.go:638] "Container of pod is not in the desired state and shall be started" containerName="ci-example256m" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:45:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:30.582951  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af Attempt:3 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:45:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:30.583224  199956 kuberuntime_manager.go:889] "Creating container in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:45:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:30.584341  199956 kuberuntime_manager.go:903] "Container start failed in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1" containerMessage="Back-off pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\"" err="ImagePullBackOff"
Jan 20 12:45:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:30.584344  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Normal" reason="BackOff" message="Back-off pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\""
Jan 20 12:45:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:30.584415  199956 kubelet.go:1503] "syncPod exit" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d isTerminal=false
Jan 20 12:45:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:30.584424  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Warning" reason="Failed" message="Error: ImagePullBackOff"
Jan 20 12:45:30 zcy-Z390-AORUS-MASTER kubelet[199956]: E0120 12:45:30.584462  199956 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ci-example256m\" with ImagePullBackOff: \"Back-off pulling image \\\"ngcn-registry.sh.intel.com:443/ci-example256m:latest\\\"\"" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:45:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:30.584511  199956 pod_workers.go:988] "Processing pod event done" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:45:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:30.624893  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="default/unsigned-unencrypted-cc-1" volumeName="kube-api-access-qcb8g" volumeSpecName="kube-api-access-qcb8g"
Jan 20 12:45:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:30.641038  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") Volume is already mounted to pod, but remount was requested." pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:45:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:30.641215  199956 projected.go:183] Setting up volume kube-api-access-qcb8g for pod 4f97314d-815b-4787-b174-5c158cd28c9d at /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:45:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:30.641622  199956 atomic_writer.go:161] pod default/unsigned-unencrypted-cc-1 volume kube-api-access-qcb8g: no update required for target directory /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:45:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:30.641693  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") " pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:45:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:31.166529  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:45:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:31.166601  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:31.174718  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[dbd9a3ba-6b7d-4de9-94e7-306dc9db4846] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:31 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001820000 2 [] false false map[] 0xc001a4aa00 0xc001affc30}
Jan 20 12:45:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:31.174747  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:31.483935  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:31.484003  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:31.491782  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[8beaed5d-203e-4b03-b9be-3b9a6e0241f9] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:31 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000707b80 2 [] false false map[] 0xc001a4ae00 0xc001bf5b80}
Jan 20 12:45:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:31.491813  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:31.581732  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:45:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:31.587660  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:45:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:31.587672  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:45:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:31.587679  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:45:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:31.588197  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:45:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:31.588245  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:45:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:31.588252  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:45:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:31.588260  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.483163  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.483227  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.491359  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[74dd3ade-7fb1-42b0-a0d9-1462a1f3e414] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:32 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001702220 2 [] false false map[] 0xc001a4b300 0xc001d2bad0}
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.491390  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.582243  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[confidential-containers-system/cc-operator-pre-install-daemon-qjplj]
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.582325  199956 pod_workers.go:888] "Processing pod event" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" podUID=b0713fbc-efc5-4044-9d08-2326a0752f87 updateType=0
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.582385  199956 kubelet.go:1501] "syncPod enter" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" podUID=b0713fbc-efc5-4044-9d08-2326a0752f87
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.582418  199956 kubelet_pods.go:1435] "Generating pod status" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj"
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.582504  199956 kubelet_pods.go:1447] "Got phase for pod" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" oldPhase=Running phase=Running
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.582788  199956 status_manager.go:535] "Ignoring same status for pod" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:01 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:05 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:05 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:01 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.5 PodIPs:[{IP:10.244.0.5}] StartTime:2023-01-20 12:32:01 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:cc-runtime-pre-install-pod State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:32:03 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:quay.io/confidential-containers/container-engine-for-cc-payload:1034f9fcf947b22eea080a6f77d8e164e2369849 ImageID:quay.io/confidential-containers/container-engine-for-cc-payload@sha256:f86f078b3a47026a066e65c7d836d9b9a43bf177555c276624d90f42e50279a1 ContainerID:containerd://e0c5b991f81d5128566686552e45a9bbc8c584e4f6c94909edb3a42720dacc90 Started:0xc000f3beae}] QOSClass:BestEffort EphemeralContainerStatuses:[]}
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.583109  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj"
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.583186  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj"
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.583567  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj"
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.583733  199956 kubelet.go:1503] "syncPod exit" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" podUID=b0713fbc-efc5-4044-9d08-2326a0752f87 isTerminal=false
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.583789  199956 pod_workers.go:988] "Processing pod event done" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" podUID=b0713fbc-efc5-4044-9d08-2326a0752f87 updateType=0
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.638730  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" volumeName="confidential-containers-artifacts" volumeSpecName="confidential-containers-artifacts"
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.638838  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" volumeName="etc-systemd-system" volumeSpecName="etc-systemd-system"
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.638916  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" volumeName="dbus" volumeSpecName="dbus"
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.638981  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" volumeName="systemd" volumeSpecName="systemd"
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.639106  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" volumeName="kube-api-access-gcgm6" volumeSpecName="kube-api-access-gcgm6"
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.658286  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-gcgm6\" (UniqueName: \"kubernetes.io/projected/b0713fbc-efc5-4044-9d08-2326a0752f87-kube-api-access-gcgm6\") pod \"cc-operator-pre-install-daemon-qjplj\" (UID: \"b0713fbc-efc5-4044-9d08-2326a0752f87\") Volume is already mounted to pod, but remount was requested." pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj"
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.658470  199956 projected.go:183] Setting up volume kube-api-access-gcgm6 for pod b0713fbc-efc5-4044-9d08-2326a0752f87 at /var/lib/kubelet/pods/b0713fbc-efc5-4044-9d08-2326a0752f87/volumes/kubernetes.io~projected/kube-api-access-gcgm6
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.658872  199956 atomic_writer.go:161] pod confidential-containers-system/cc-operator-pre-install-daemon-qjplj volume kube-api-access-gcgm6: no update required for target directory /var/lib/kubelet/pods/b0713fbc-efc5-4044-9d08-2326a0752f87/volumes/kubernetes.io~projected/kube-api-access-gcgm6
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.658946  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-gcgm6\" (UniqueName: \"kubernetes.io/projected/b0713fbc-efc5-4044-9d08-2326a0752f87-kube-api-access-gcgm6\") pod \"cc-operator-pre-install-daemon-qjplj\" (UID: \"b0713fbc-efc5-4044-9d08-2326a0752f87\") " pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj"
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.689491  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.689553  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.690635  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:32 GMT]] 0xc0013003c0 2 [] true false map[] 0xc001c0ba00 <nil>}
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.690745  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.695958  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.696018  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.697029  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:32 GMT]] 0xc0004c1f40 2 [] true false map[] 0xc001c0bc00 <nil>}
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.697127  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.710332  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.710358  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.710396  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.710415  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.711456  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:32 GMT]] 0xc0017023c0 2 [] true false map[] 0xc000cf7f00 <nil>}
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.711513  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:32 GMT]] 0xc001300480 2 [] true false map[] 0xc001a4b600 <nil>}
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.711555  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.711615  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:45:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:32.869743  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:45:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:33.484042  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:33.484115  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:33.491793  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[91efd29e-1030-4b17-b51b-985f82ff2fe1] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:33 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00013c000 2 [] false false map[] 0xc001a4a100 0xc001982000}
Jan 20 12:45:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:33.491827  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:33.581715  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:45:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:33.587698  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:45:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:33.587710  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:45:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:33.588250  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:45:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:33.588642  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:45:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:33.588691  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:45:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:33.588698  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:45:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:33.588706  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:45:34 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:34.484071  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:34 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:34.484140  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:34 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:34.492197  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[0fc2a75f-4ddd-465e-a39c-109a558ed55f] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:34 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ce9980 2 [] false false map[] 0xc001c0a100 0xc0018ae6e0}
Jan 20 12:45:34 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:34.492229  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:35.479001  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:45:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:35.479009  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:45:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:35.479213  199956 interface.go:209] Interface eno2 is up
Jan 20 12:45:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:35.479239  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:45:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:35.479246  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:45:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:35.479252  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:45:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:35.479255  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:45:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:35.479259  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:45:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:35.479730  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:45:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:35.479739  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:45:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:35.479744  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:45:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:35.479747  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:45:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:35.484119  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:35.484187  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:35.496581  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[c6f3031b-922b-4b6f-aabb-811a47b0ad24] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:35 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00129c900 2 [] false false map[] 0xc001716200 0xc0018aed10}
Jan 20 12:45:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:35.496737  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:35.581447  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:45:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:35.587643  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:45:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:35.587655  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:45:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:35.587662  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:45:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:35.588188  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:45:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:35.588237  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:45:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:35.588243  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:45:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:35.588251  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:45:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:36.164532  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:45:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:36.164602  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:36.172440  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:36 GMT] X-Content-Type-Options:[nosniff]] 0xc000ffb5c0 2 [] false false map[] 0xc0011c5f00 0xc0012fda20}
Jan 20 12:45:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:36.172483  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:45:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:36.272349  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:45:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:36.272412  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:36.273646  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:36 GMT]] 0xc00129ce00 29 [] true false map[] 0xc001f36100 <nil>}
Jan 20 12:45:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:36.273744  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:45:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:36.483408  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:36.483469  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:36.491958  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[d1145a49-8daa-493a-8bc1-85f613655c15] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:36 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ffb840 2 [] false false map[] 0xc001f36500 0xc0015756b0}
Jan 20 12:45:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:36.491984  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:36.895362  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:45:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:36.895431  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:36.902470  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:36 GMT] X-Content-Type-Options:[nosniff]] 0xc000ffb8a0 2 [] false false map[] 0xc001846300 0xc0016b1970}
Jan 20 12:45:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:36.902537  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:45:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:37.484151  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:37.484222  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:37.491796  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[1d187024-cca8-4fc4-bb1d-119064fac752] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:37 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000cb2f60 2 [] false false map[] 0xc001846600 0xc0016b1b80}
Jan 20 12:45:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:37.491828  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:37.581813  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:45:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:37.587742  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:45:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:37.587755  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:45:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:37.588275  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:45:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:37.588660  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:45:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:37.588712  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:45:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:37.588719  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:45:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:37.588727  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:45:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:37.870932  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:45:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:38.182666  199956 handler.go:293] error while reading "/proc/199956/fd/34" link: readlink /proc/199956/fd/34: no such file or directory
Jan 20 12:45:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:38.483305  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:38.483375  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:38.491131  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[f549b84f-48c9-47d4-94c1-d01df94ea2e7] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:38 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0003209e0 2 [] false false map[] 0xc001f36e00 0xc001a61080}
Jan 20 12:45:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:38.491157  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:38.878656  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/healthz" timeout="1s"
Jan 20 12:45:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:38.878721  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:45:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:38.878742  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:38.878780  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:38.879954  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:38 GMT] X-Content-Type-Options:[nosniff]] 0xc0008dc020 2 [] true false map[] 0xc0014ec200 <nil>}
Jan 20 12:45:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:38.880001  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:38 GMT] X-Content-Type-Options:[nosniff]] 0xc000320ee0 2 [] true false map[] 0xc001f37100 <nil>}
Jan 20 12:45:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:38.880074  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:45:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:38.880115  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:45:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:39.483609  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:39.483682  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:39.491724  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[85cc72a3-643c-4d8a-9712-f4e93279d04b] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:39 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000320200 2 [] false false map[] 0xc001f36400 0xc0013b1d90}
Jan 20 12:45:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:39.491757  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:39.581188  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:45:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:39.586776  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:45:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:39.586789  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:45:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:39.586796  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:45:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:39.587324  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:45:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:39.587372  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:45:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:39.587379  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:45:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:39.587388  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:45:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:40.348537  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:45:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:40.356731  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:45:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:40.367055  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64401712Ki" capacity="65061836Ki" time="2023-01-20 12:45:40.36700156 -0500 EST m=+852.922776385"
Jan 20 12:45:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:40.367069  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902002212Ki" capacity="981310056Ki" time="2023-01-20 12:45:40.350610151 -0500 EST m=+852.906385042"
Jan 20 12:45:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:40.367076  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:45:40.350610151 -0500 EST m=+852.906385042"
Jan 20 12:45:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:40.367081  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902002212Ki" capacity="981310056Ki" time="2023-01-20 12:45:31.26196538 -0500 EST"
Jan 20 12:45:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:40.367087  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:45:31.26196538 -0500 EST"
Jan 20 12:45:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:40.367092  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510771" capacity="511757" time="2023-01-20 12:45:40.36672699 -0500 EST m=+852.922501815"
Jan 20 12:45:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:40.367098  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59532180Ki" capacity="65586124Ki" time="2023-01-20 12:45:40.350610151 -0500 EST m=+852.906385042"
Jan 20 12:45:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:40.367124  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:45:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:40.484137  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:40.484202  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:40.492188  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[4c908ac4-a292-479d-a5b2-d4c8c411036d] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:40 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000320a20 2 [] false false map[] 0xc000cf6000 0xc000fd8a50}
Jan 20 12:45:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:40.492219  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.166378  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.166466  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.174731  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[23ae4e16-5b38-4de2-887e-764969fd1501] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:41 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e9ac0 2 [] false false map[] 0xc001716200 0xc000afefd0}
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.174815  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.483759  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.483793  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.486363  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[8ff65887-a598-4839-8e36-45bea84e7b88] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:41 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0009a18c0 2 [] false false map[] 0xc001716400 0xc00103ab00}
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.486400  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.581182  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[kube-flannel/kube-flannel-ds-hprn4]
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.581272  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.581406  199956 pod_workers.go:888] "Processing pod event" pod="kube-flannel/kube-flannel-ds-hprn4" podUID=04e472cb-b6f9-4065-b509-d7e1579fc48d updateType=0
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.581488  199956 kubelet.go:1501] "syncPod enter" pod="kube-flannel/kube-flannel-ds-hprn4" podUID=04e472cb-b6f9-4065-b509-d7e1579fc48d
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.581524  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.581648  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-flannel/kube-flannel-ds-hprn4" oldPhase=Running phase=Running
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.582042  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-flannel/kube-flannel-ds-hprn4" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:44 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:45 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:45 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.239.159.53 PodIPs:[{IP:10.239.159.53}] StartTime:2023-01-20 12:31:42 -0500 EST InitContainerStatuses:[{Name:install-cni-plugin State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2023-01-20 12:31:43 -0500 EST,FinishedAt:2023-01-20 12:31:43 -0500 EST,ContainerID:containerd://349eee1bf5a2d95206979822c956ea6f555dbb786434c7b9fe46524872f1a18d,}} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:docker.io/rancher/mirrored-flannelcni-flannel-cni-plugin:v1.1.0 ImageID:docker.io/rancher/mirrored-flannelcni-flannel-cni-plugin@sha256:28d3a6be9f450282bf42e4dad143d41da23e3d91f66f19c01ee7fd21fd17cb2b ContainerID:containerd://349eee1bf5a2d95206979822c956ea6f555dbb786434c7b9fe46524872f1a18d Started:<nil>} {Name:install-cni State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2023-01-20 12:31:43 -0500 EST,FinishedAt:2023-01-20 12:31:43 -0500 EST,ContainerID:containerd://85a4ede91a47b8d7df192dc39004a2a7fa45e095191a7335eaed8ec826cd9f36,}} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:docker.io/rancher/mirrored-flannelcni-flannel:v0.19.1 ImageID:docker.io/rancher/mirrored-flannelcni-flannel@sha256:e3b0f06121b0f7a11a684e910bba89b3ab49cd6e73aeb6c719f83b2456946366 ContainerID:containerd://85a4ede91a47b8d7df192dc39004a2a7fa45e095191a7335eaed8ec826cd9f36 Started:<nil>}] ContainerStatuses:[{Name:kube-flannel State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:44 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:docker.io/rancher/mirrored-flannelcni-flannel:v0.19.1 ImageID:docker.io/rancher/mirrored-flannelcni-flannel@sha256:e3b0f06121b0f7a11a684e910bba89b3ab49cd6e73aeb6c719f83b2456946366 ContainerID:containerd://0f9ae677fe935afcf43e145281deae0d663ba95fda6759ff24f0dd3e1cee17a9 Started:0xc001dafa9e}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.582422  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.582500  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.582938  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.583106  199956 kubelet.go:1503] "syncPod exit" pod="kube-flannel/kube-flannel-ds-hprn4" podUID=04e472cb-b6f9-4065-b509-d7e1579fc48d isTerminal=false
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.583161  199956 pod_workers.go:988] "Processing pod event done" pod="kube-flannel/kube-flannel-ds-hprn4" podUID=04e472cb-b6f9-4065-b509-d7e1579fc48d updateType=0
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.588633  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.588712  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.590943  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.592995  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.593218  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.593251  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.593295  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.603422  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="run" volumeSpecName="run"
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.603522  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="cni-plugin" volumeSpecName="cni-plugin"
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.603594  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="cni" volumeSpecName="cni"
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.603675  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="flannel-cfg" volumeSpecName="flannel-cfg"
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.603742  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="xtables-lock" volumeSpecName="xtables-lock"
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.603859  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="kube-api-access-hqj8d" volumeSpecName="kube-api-access-hqj8d"
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.621632  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"flannel-cfg\" (UniqueName: \"kubernetes.io/configmap/04e472cb-b6f9-4065-b509-d7e1579fc48d-flannel-cfg\") pod \"kube-flannel-ds-hprn4\" (UID: \"04e472cb-b6f9-4065-b509-d7e1579fc48d\") Volume is already mounted to pod, but remount was requested." pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.621766  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-hqj8d\" (UniqueName: \"kubernetes.io/projected/04e472cb-b6f9-4065-b509-d7e1579fc48d-kube-api-access-hqj8d\") pod \"kube-flannel-ds-hprn4\" (UID: \"04e472cb-b6f9-4065-b509-d7e1579fc48d\") Volume is already mounted to pod, but remount was requested." pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.621909  199956 projected.go:183] Setting up volume kube-api-access-hqj8d for pod 04e472cb-b6f9-4065-b509-d7e1579fc48d at /var/lib/kubelet/pods/04e472cb-b6f9-4065-b509-d7e1579fc48d/volumes/kubernetes.io~projected/kube-api-access-hqj8d
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.621943  199956 configmap.go:181] Setting up volume flannel-cfg for pod 04e472cb-b6f9-4065-b509-d7e1579fc48d at /var/lib/kubelet/pods/04e472cb-b6f9-4065-b509-d7e1579fc48d/volumes/kubernetes.io~configmap/flannel-cfg
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.622021  199956 configmap.go:205] Received configMap kube-flannel/kube-flannel-cfg containing (2) pieces of data, 365 total bytes
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.622094  199956 quota_linux.go:271] SupportsQuotas called, but quotas disabled
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.622342  199956 atomic_writer.go:161] pod kube-flannel/kube-flannel-ds-hprn4 volume kube-api-access-hqj8d: no update required for target directory /var/lib/kubelet/pods/04e472cb-b6f9-4065-b509-d7e1579fc48d/volumes/kubernetes.io~projected/kube-api-access-hqj8d
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.622367  199956 atomic_writer.go:161] pod kube-flannel/kube-flannel-ds-hprn4 volume flannel-cfg: no update required for target directory /var/lib/kubelet/pods/04e472cb-b6f9-4065-b509-d7e1579fc48d/volumes/kubernetes.io~configmap/flannel-cfg
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.622409  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-hqj8d\" (UniqueName: \"kubernetes.io/projected/04e472cb-b6f9-4065-b509-d7e1579fc48d-kube-api-access-hqj8d\") pod \"kube-flannel-ds-hprn4\" (UID: \"04e472cb-b6f9-4065-b509-d7e1579fc48d\") " pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:45:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:41.622440  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"flannel-cfg\" (UniqueName: \"kubernetes.io/configmap/04e472cb-b6f9-4065-b509-d7e1579fc48d-flannel-cfg\") pod \"kube-flannel-ds-hprn4\" (UID: \"04e472cb-b6f9-4065-b509-d7e1579fc48d\") " pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.483871  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.483941  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.491867  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[680d2f4a-c91f-4ba7-a3a0-259ce435bf91] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:42 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00013cca0 2 [] false false map[] 0xc001f37100 0xc0014e29a0}
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.491898  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.581483  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[default/unsigned-unencrypted-cc-1]
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.581564  199956 pod_workers.go:888] "Processing pod event" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.581622  199956 kubelet.go:1501] "syncPod enter" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.581652  199956 kubelet_pods.go:1435] "Generating pod status" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.581747  199956 kubelet_pods.go:1447] "Got phase for pod" pod="default/unsigned-unencrypted-cc-1" oldPhase=Pending phase=Pending
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.582021  199956 status_manager.go:535] "Ignoring same status for pod" pod="default/unsigned-unencrypted-cc-1" status={Phase:Pending Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.15 PodIPs:[{IP:10.244.0.15}] StartTime:2023-01-20 12:41:08 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:ci-example256m State:{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "ngcn-registry.sh.intel.com:443/ci-example256m:latest",} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest ImageID: ContainerID: Started:0xc0017bf46e}] QOSClass:Guaranteed EphemeralContainerStatuses:[]}
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.582324  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.582401  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.582447  199956 kuberuntime_manager.go:638] "Container of pod is not in the desired state and shall be started" containerName="ci-example256m" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.582489  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af Attempt:3 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.582760  199956 kuberuntime_manager.go:889] "Creating container in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.583820  199956 kuberuntime_image.go:47] "Pulling image without credentials" image="ngcn-registry.sh.intel.com:443/ci-example256m:latest"
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.583878  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Normal" reason="Pulling" message="Pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\""
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:45:42.584365308-05:00" level=info msg="PullImage \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\""
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:45:42.584491222-05:00" level=info msg="TaskManager get ImageService succeed." id=3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.610829  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="default/unsigned-unencrypted-cc-1" volumeName="kube-api-access-qcb8g" volumeSpecName="kube-api-access-qcb8g"
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.628938  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") Volume is already mounted to pod, but remount was requested." pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.629143  199956 projected.go:183] Setting up volume kube-api-access-qcb8g for pod 4f97314d-815b-4787-b174-5c158cd28c9d at /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.629550  199956 atomic_writer.go:161] pod default/unsigned-unencrypted-cc-1 volume kube-api-access-qcb8g: no update required for target directory /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.629621  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") " pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.689014  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.689080  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.690249  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:42 GMT]] 0xc00013d780 2 [] true false map[] 0xc0014c7000 <nil>}
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.690357  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.695586  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.695645  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.696733  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:42 GMT]] 0xc00013d8c0 2 [] true false map[] 0xc0014c7200 <nil>}
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.696841  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.710034  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.710094  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.710109  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.710167  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.711204  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:42 GMT]] 0xc001702040 2 [] true false map[] 0xc000cf7900 <nil>}
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.711245  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:42 GMT]] 0xc00013d940 2 [] true false map[] 0xc0014c7400 <nil>}
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.711308  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.711359  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:45:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:42.872440  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:45:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:43.483963  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:43.484033  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:43.491708  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[ab67bbbc-aedf-4e03-8f38-14ef60586333] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:43 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001718bc0 2 [] false false map[] 0xc001c0a100 0xc001765d90}
Jan 20 12:45:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:43.491740  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:43.581851  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:45:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:43.587699  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:45:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:43.587713  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:45:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:43.587720  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:45:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:43.588276  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:45:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:43.588321  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:45:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:43.588328  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:45:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:43.588336  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:45:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:44.484200  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:44.484270  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:44.492286  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[c2cecc8b-8dc6-4525-a5c1-c8f63d810c4f] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:44 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ffa9e0 2 [] false false map[] 0xc000cf7b00 0xc001b28840}
Jan 20 12:45:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:44.492319  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:45.484201  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:45.484265  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:45.491935  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[a480c518-4138-4e40-a689-0a970785435e] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:45 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001301ca0 2 [] false false map[] 0xc0021a2000 0xc001c91550}
Jan 20 12:45:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:45.491966  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:45.581879  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:45:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:45.589216  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:45:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:45.589281  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:45:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:45.591469  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:45:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:45.593537  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:45:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:45.593760  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:45:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:45.593794  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:45:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:45.593837  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:45:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:45.784502  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:45:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:45.784511  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:45:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:45.784669  199956 interface.go:209] Interface eno2 is up
Jan 20 12:45:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:45.784705  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:45:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:45.784712  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:45:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:45.784717  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:45:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:45.784720  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:45:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:45.784724  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:45:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:45.784942  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:45:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:45.784950  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:45:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:45.784954  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:45:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:45.784957  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:45:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:46.165138  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:45:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:46.165210  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:46.172433  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:46 GMT] X-Content-Type-Options:[nosniff]] 0xc00129daa0 2 [] false false map[] 0xc0002c3700 0xc001e1f340}
Jan 20 12:45:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:46.172459  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:45:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:46.272256  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:45:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:46.272320  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:46.273566  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:46 GMT]] 0xc000ffb540 29 [] true false map[] 0xc0000dd800 <nil>}
Jan 20 12:45:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:46.273678  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:45:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:46.483175  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:46.483235  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:46.496898  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[f74c0a97-e286-4ae0-b60e-ac7d37244bc5] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:46 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00129db60 2 [] false false map[] 0xc0000dda00 0xc00212d3f0}
Jan 20 12:45:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:46.497028  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:46.895048  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:45:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:46.895131  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:46.903464  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:46 GMT] X-Content-Type-Options:[nosniff]] 0xc00123e040 2 [] false false map[] 0xc001775b00 0xc00214b4a0}
Jan 20 12:45:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:46.903498  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:45:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:47.484039  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:47.484107  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:47.491951  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[6501c818-6ad6-4c2e-975f-c96af275566e] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:47 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00129c440 2 [] false false map[] 0xc001f36600 0xc0006a2c60}
Jan 20 12:45:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:47.491982  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:47.526351  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/etcd.yaml"
Jan 20 12:45:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:47.528497  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver.yaml"
Jan 20 12:45:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:47.531372  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Jan 20 12:45:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:47.531891  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-scheduler.yaml"
Jan 20 12:45:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:47.532178  199956 config.go:279] "Setting pods for source" source="file"
Jan 20 12:45:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:47.581476  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:45:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:47.587641  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:45:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:47.587653  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:45:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:47.587660  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:45:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:47.588198  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:45:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:47.588244  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:45:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:47.588250  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:45:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:47.588259  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:45:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:47.874386  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:45:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:48.483483  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:48.483555  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:48.492102  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[368814a8-af18-4367-bef4-5d0c967c8ee9] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:48 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008dc260 2 [] false false map[] 0xc0000ddf00 0xc000f8c6e0}
Jan 20 12:45:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:48.492131  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:48.879313  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:45:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:48.879383  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:48.880557  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:48 GMT] X-Content-Type-Options:[nosniff]] 0xc000ffabe0 2 [] true false map[] 0xc001846300 <nil>}
Jan 20 12:45:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:48.880678  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:45:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:48.916772  199956 handler.go:293] error while reading "/proc/199956/fd/34" link: readlink /proc/199956/fd/34: no such file or directory
Jan 20 12:45:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:49.483880  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:49.483947  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:49.491982  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[e5f007f7-d726-48a4-934c-133d0f17f1e1] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:49 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0020fd040 2 [] false false map[] 0xc0014ec100 0xc001261600}
Jan 20 12:45:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:49.492014  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:49.581649  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:45:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:49.588999  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:45:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:49.589062  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:45:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:49.591275  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:45:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:49.593235  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:45:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:49.593462  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:45:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:49.593497  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:45:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:49.593540  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:45:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:50.368136  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:45:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:50.375869  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:45:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:50.385749  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902002164Ki" capacity="981310056Ki" time="2023-01-20 12:45:41.261935759 -0500 EST"
Jan 20 12:45:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:50.385762  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:45:41.261935759 -0500 EST"
Jan 20 12:45:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:50.385768  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510774" capacity="511757" time="2023-01-20 12:45:50.385436534 -0500 EST m=+862.941211359"
Jan 20 12:45:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:50.385775  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59503380Ki" capacity="65586124Ki" time="2023-01-20 12:45:50.370280036 -0500 EST m=+862.926054927"
Jan 20 12:45:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:50.385781  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64401176Ki" capacity="65061836Ki" time="2023-01-20 12:45:50.385696928 -0500 EST m=+862.941471752"
Jan 20 12:45:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:50.385787  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902002164Ki" capacity="981310056Ki" time="2023-01-20 12:45:50.370280036 -0500 EST m=+862.926054927"
Jan 20 12:45:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:50.385792  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:45:50.370280036 -0500 EST m=+862.926054927"
Jan 20 12:45:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:50.385819  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:45:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:50.484081  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:50.484144  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:50.492209  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[315a36c0-ca55-462f-b765-656a8f2cdfcb] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:50 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000978c40 2 [] false false map[] 0xc001c0a100 0xc001529ad0}
Jan 20 12:45:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:50.492240  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:51.166445  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:45:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:51.166516  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:51.179041  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[bdfad40b-883a-4aad-8d7a-4a72584df341] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:51 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e8c80 2 [] false false map[] 0xc001c0a400 0xc001809a20}
Jan 20 12:45:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:51.179178  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:51.483698  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:51.483770  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:51.491824  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[82c0b5fd-2dcc-4571-a2f0-62a102b87845] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:51 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e8ea0 2 [] false false map[] 0xc001c0a700 0xc001883ad0}
Jan 20 12:45:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:51.491857  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:51.581585  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:45:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:51.587682  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:45:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:51.587696  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:45:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:51.587703  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:45:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:51.588365  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:45:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:51.588413  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:45:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:51.588420  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:45:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:51.588428  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:45:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:52.483335  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:52.483413  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:52.497924  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[c15cfa6f-3f77-459c-b98e-7ad34eb813ed] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:52 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000cb23c0 2 [] false false map[] 0xc0014c6100 0xc001361810}
Jan 20 12:45:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:52.498065  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:52.689977  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:45:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:52.690040  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:52.691139  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:52 GMT]] 0xc00013c320 2 [] true false map[] 0xc0014c6900 <nil>}
Jan 20 12:45:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:52.691247  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:45:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:52.695365  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:45:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:52.695432  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:52.696449  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:52 GMT]] 0xc00013c380 2 [] true false map[] 0xc0014c6b00 <nil>}
Jan 20 12:45:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:52.696558  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:45:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:52.710087  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:45:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:52.710145  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:52.710156  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:45:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:52.710225  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:52.711194  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:52 GMT]] 0xc0012e8020 2 [] true false map[] 0xc0014c6f00 <nil>}
Jan 20 12:45:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:52.711267  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:52 GMT]] 0xc00013c3e0 2 [] true false map[] 0xc0011c4200 <nil>}
Jan 20 12:45:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:52.711302  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:45:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:52.711370  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:45:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:52.875986  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:45:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:53.484141  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:53.484216  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:53.492057  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[b8515888-cda6-4fcf-9629-fdbb50883848] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:53 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000cb2920 2 [] false false map[] 0xc0011c4400 0xc001764580}
Jan 20 12:45:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:53.492089  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:53.581713  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[kube-system/etcd-zcy-z390-aorus-master]
Jan 20 12:45:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:53.581805  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:45:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:53.581943  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d updateType=0
Jan 20 12:45:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:53.582034  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d
Jan 20 12:45:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:53.582068  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/etcd-zcy-z390-aorus-master"
Jan 20 12:45:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:53.582159  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/etcd-zcy-z390-aorus-master" oldPhase=Running phase=Running
Jan 20 12:45:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:53.582456  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/etcd-zcy-z390-aorus-master" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:28 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:37 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:37 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:28 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.239.159.53 PodIPs:[{IP:10.239.159.53}] StartTime:2023-01-20 12:31:28 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:etcd State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:22 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:6 Image:k8s.gcr.io/etcd:3.5.3-0 ImageID:k8s.gcr.io/etcd@sha256:13f53ed1d91e2e11aac476ee9a0269fdda6cc4874eba903efd40daf50c55eee5 ContainerID:containerd://9b11acac1b891eafc7ff2b244f1c42938f71a575ce81ff917e3b7ebf61d2e045 Started:0xc001115ab9}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:45:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:53.582812  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/etcd-zcy-z390-aorus-master"
Jan 20 12:45:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:53.582887  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/etcd-zcy-z390-aorus-master"
Jan 20 12:45:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:53.583551  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/etcd-zcy-z390-aorus-master"
Jan 20 12:45:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:53.583717  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d isTerminal=false
Jan 20 12:45:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:53.583773  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d updateType=0
Jan 20 12:45:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:53.587777  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:45:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:53.587789  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:45:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:53.587927  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/etcd-zcy-z390-aorus-master" volumeName="etcd-certs" volumeSpecName="etcd-certs"
Jan 20 12:45:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:53.587945  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/etcd-zcy-z390-aorus-master" volumeName="etcd-data" volumeSpecName="etcd-data"
Jan 20 12:45:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:53.588607  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:45:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:53.588942  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:45:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:53.588988  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:45:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:53.588995  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:45:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:53.589004  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:45:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:54.483223  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:54.483292  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:54.491290  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[7c241094-5598-4e15-9915-ef626c809d99] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:54 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123e0e0 2 [] false false map[] 0xc0014c7300 0xc000fab8c0}
Jan 20 12:45:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:54.491347  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:55.483972  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:55.484041  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:55.492095  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[8c6c5e7f-31b7-4ad8-b3ea-88719ee7c600] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:55 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123eb40 2 [] false false map[] 0xc001f36f00 0xc0012702c0}
Jan 20 12:45:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:55.492126  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:55.582012  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:45:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:55.584092  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:45:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:55.584112  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:45:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:55.584122  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:45:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:55.584846  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:45:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:55.584914  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:45:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:55.584924  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:45:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:55.584936  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:45:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:55.938632  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:45:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:55.938640  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:45:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:55.938922  199956 interface.go:209] Interface eno2 is up
Jan 20 12:45:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:55.938964  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:45:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:55.938972  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:45:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:55.938977  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:45:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:55.938980  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:45:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:55.938983  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:45:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:55.939296  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:45:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:55.939303  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:45:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:55.939308  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:45:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:55.939312  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:45:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:56.164155  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:45:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:56.164224  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:56.171379  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:56 GMT] X-Content-Type-Options:[nosniff]] 0xc00123f620 2 [] false false map[] 0xc001a4a100 0xc0013b04d0}
Jan 20 12:45:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:56.171408  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:45:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:56.272668  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:45:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:56.272744  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:56.273985  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:56 GMT]] 0xc00013c7a0 29 [] true false map[] 0xc001774d00 <nil>}
Jan 20 12:45:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:56.274094  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:45:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:56.484126  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:56.484227  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:56.492106  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[ab4169e6-b0e4-414d-a06d-d6e175cbb6ab] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:56 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008ddcc0 2 [] false false map[] 0xc001774f00 0xc0013b0630}
Jan 20 12:45:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:56.492137  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:56.896049  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:45:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:56.896116  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:56.903449  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:56 GMT] X-Content-Type-Options:[nosniff]] 0xc00013d600 2 [] false false map[] 0xc001a4a600 0xc001974630}
Jan 20 12:45:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:56.903498  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:45:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:57.484016  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:57.484082  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:57.492070  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[bb9ad821-bae7-4445-a2d5-c55ea1f6bc7e] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:57 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0009a07a0 2 [] false false map[] 0xc001775200 0xc0014654a0}
Jan 20 12:45:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:57.492102  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:57.582187  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:45:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:57.583991  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:45:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:57.584007  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:45:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:57.584643  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:45:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:57.585096  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:45:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:57.585153  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:45:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:57.585162  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:45:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:57.585172  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:45:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:57.877636  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:45:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:58.483720  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:58.483796  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:58.492109  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[02857ecb-1bd3-4c0d-9d06-6a03960a24e9] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:58 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001719aa0 2 [] false false map[] 0xc001775800 0xc001e82000}
Jan 20 12:45:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:58.492143  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:58.581268  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr]
Jan 20 12:45:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:58.581356  199956 pod_workers.go:888] "Processing pod event" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d updateType=0
Jan 20 12:45:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:58.581416  199956 kubelet.go:1501] "syncPod enter" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d
Jan 20 12:45:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:58.581450  199956 kubelet_pods.go:1435] "Generating pod status" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:45:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:58.581547  199956 kubelet_pods.go:1447] "Got phase for pod" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" oldPhase=Running phase=Running
Jan 20 12:45:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:58.581880  199956 status_manager.go:535] "Ignoring same status for pod" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:58 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:08 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:08 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:58 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.4 PodIPs:[{IP:10.244.0.4}] StartTime:2023-01-20 12:31:58 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:kube-rbac-proxy State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:59 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:gcr.io/kubebuilder/kube-rbac-proxy:v0.13.0 ImageID:gcr.io/kubebuilder/kube-rbac-proxy@sha256:d99a8d144816b951a67648c12c0b988936ccd25cf3754f3cd85ab8c01592248f ContainerID:containerd://1cb44bdbde0b41852e382d7dab7c184af8dd4e5b25d5cfe6a10a9c8ab601659d Started:0xc001200c6f} {Name:manager State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:59 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:quay.io/confidential-containers/operator:v0.2.0 ImageID:quay.io/confidential-containers/operator@sha256:c965b55253a9abe4c2f7596c42467fa59f2cc741bfafeed1d25629ed6f8df12d ContainerID:containerd://186424b47c7b9022ecfe8996dc73cd9101f7539259cd65914279c84c9a02a288 Started:0xc001200c80}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:45:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:58.582213  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:45:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:58.582281  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:45:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:58.582926  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:45:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:58.583100  199956 kubelet.go:1503] "syncPod exit" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d isTerminal=false
Jan 20 12:45:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:58.583158  199956 pod_workers.go:988] "Processing pod event done" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d updateType=0
Jan 20 12:45:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:58.626782  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" volumeName="kube-api-access-4pnfq" volumeSpecName="kube-api-access-4pnfq"
Jan 20 12:45:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:58.648992  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-4pnfq\" (UniqueName: \"kubernetes.io/projected/d2688d45-2487-46e7-aecb-e3479626909d-kube-api-access-4pnfq\") pod \"cc-operator-controller-manager-79797456f6-m6znr\" (UID: \"d2688d45-2487-46e7-aecb-e3479626909d\") Volume is already mounted to pod, but remount was requested." pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:45:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:58.649182  199956 projected.go:183] Setting up volume kube-api-access-4pnfq for pod d2688d45-2487-46e7-aecb-e3479626909d at /var/lib/kubelet/pods/d2688d45-2487-46e7-aecb-e3479626909d/volumes/kubernetes.io~projected/kube-api-access-4pnfq
Jan 20 12:45:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:58.649599  199956 atomic_writer.go:161] pod confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr volume kube-api-access-4pnfq: no update required for target directory /var/lib/kubelet/pods/d2688d45-2487-46e7-aecb-e3479626909d/volumes/kubernetes.io~projected/kube-api-access-4pnfq
Jan 20 12:45:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:58.649675  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-4pnfq\" (UniqueName: \"kubernetes.io/projected/d2688d45-2487-46e7-aecb-e3479626909d-kube-api-access-4pnfq\") pod \"cc-operator-controller-manager-79797456f6-m6znr\" (UID: \"d2688d45-2487-46e7-aecb-e3479626909d\") " pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:45:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:58.879428  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/healthz" timeout="1s"
Jan 20 12:45:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:58.879481  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:45:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:58.879501  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:58.879540  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:58.880716  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:58 GMT] X-Content-Type-Options:[nosniff]] 0xc0020fc220 2 [] true false map[] 0xc001846000 <nil>}
Jan 20 12:45:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:58.880758  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:58 GMT] X-Content-Type-Options:[nosniff]] 0xc001300740 2 [] true false map[] 0xc001775a00 <nil>}
Jan 20 12:45:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:58.880832  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:45:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:58.880873  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:45:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:59.484115  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:45:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:59.484182  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:45:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:59.492039  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[f57fe44f-2a19-40ea-811b-33df92f6f199] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:45:59 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001486f60 2 [] false false map[] 0xc0014c7b00 0xc001ebbef0}
Jan 20 12:45:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:59.492072  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:45:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:59.581602  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:45:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:59.586164  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:45:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:59.586181  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:45:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:59.586736  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:45:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:59.587118  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:45:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:59.587169  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:45:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:59.587177  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:45:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:45:59.587186  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:46:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:00.386190  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:46:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:00.393857  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:46:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:00.403792  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64400828Ki" capacity="65061836Ki" time="2023-01-20 12:46:00.403740724 -0500 EST m=+872.959515549"
Jan 20 12:46:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:00.403805  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902002124Ki" capacity="981310056Ki" time="2023-01-20 12:46:00.388364321 -0500 EST m=+872.944139212"
Jan 20 12:46:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:00.403812  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:46:00.388364321 -0500 EST m=+872.944139212"
Jan 20 12:46:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:00.403817  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902002124Ki" capacity="981310056Ki" time="2023-01-20 12:45:51.261628349 -0500 EST"
Jan 20 12:46:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:00.403823  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:45:51.261628349 -0500 EST"
Jan 20 12:46:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:00.403829  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510788" capacity="511757" time="2023-01-20 12:46:00.403477782 -0500 EST m=+872.959252607"
Jan 20 12:46:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:00.403834  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59494000Ki" capacity="65586124Ki" time="2023-01-20 12:46:00.388364321 -0500 EST m=+872.944139212"
Jan 20 12:46:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:00.403860  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:46:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:00.483815  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:00.483882  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:00.498354  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[09513f14-93a0-446a-8252-d888dfa6492f] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:00 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ce8f40 2 [] false false map[] 0xc0014c7b00 0xc0019833f0}
Jan 20 12:46:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:00.498487  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:01.166777  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:46:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:01.166810  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:01.169300  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[06d9491f-64ba-46d6-a6fb-52c548df0a7d] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:01 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001486700 2 [] false false map[] 0xc001846200 0xc001764580}
Jan 20 12:46:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:01.169333  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:01.483165  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:01.483237  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:01.490940  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[dc521d77-3eff-4e25-a081-d6522cf9981c] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:01 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0020fd580 2 [] false false map[] 0xc000cf6200 0xc000de6a50}
Jan 20 12:46:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:01.490971  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:01.581824  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:46:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:01.587847  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:46:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:01.587860  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:46:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:01.588411  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:46:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:01.588763  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:46:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:01.588809  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:46:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:01.588816  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:46:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:01.588824  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:46:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:02.483624  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:02.483699  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:02.491901  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[8eda49f8-1de8-4831-8183-a8e079e7e2d3] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:02 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001486d20 2 [] false false map[] 0xc0011c4400 0xc001222790}
Jan 20 12:46:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:02.491934  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:02.689402  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:46:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:02.689467  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:02.690629  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:02 GMT]] 0xc001486da0 2 [] true false map[] 0xc001c0a100 <nil>}
Jan 20 12:46:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:02.690738  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:46:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:02.695860  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:46:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:02.695919  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:02.696925  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:02 GMT]] 0xc0020fda60 2 [] true false map[] 0xc001c0a300 <nil>}
Jan 20 12:46:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:02.697032  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:46:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:02.710210  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:46:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:02.710270  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:02.710282  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:46:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:02.710340  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:02.711388  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:02 GMT]] 0xc000978020 2 [] true false map[] 0xc0011c4700 <nil>}
Jan 20 12:46:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:02.711451  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:02 GMT]] 0xc001301100 2 [] true false map[] 0xc001846500 <nil>}
Jan 20 12:46:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:02.711492  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:46:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:02.711558  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:46:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:02.879127  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:46:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:03.483633  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:03.483701  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:03.491965  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[53f995b4-20e3-4aca-aa40-0937f25af69f] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:03 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0014877a0 2 [] false false map[] 0xc001846700 0xc0011e28f0}
Jan 20 12:46:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:03.491996  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:03.581616  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:46:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:03.587711  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:46:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:03.587723  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:46:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:03.587730  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:46:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:03.588386  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:46:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:03.588430  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:46:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:03.588436  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:46:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:03.588444  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:46:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:04.483887  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:04.483956  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:04.492301  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[d36042e1-227e-4870-995c-8870c3538ebb] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:04 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000978760 2 [] false false map[] 0xc001f37e00 0xc00166e2c0}
Jan 20 12:46:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:04.492331  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:05.483703  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:05.483769  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:05.491950  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[455ab98e-53e3-451b-aa06-212f9f63e65d] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:05 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001487c40 2 [] false false map[] 0xc0000dd500 0xc001763760}
Jan 20 12:46:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:05.491982  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:05.581731  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[kube-system/coredns-6d4b75cb6d-48zl2]
Jan 20 12:46:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:05.581824  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:46:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:05.581889  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 updateType=0
Jan 20 12:46:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:05.581968  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1
Jan 20 12:46:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:05.582004  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:46:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:05.582101  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/coredns-6d4b75cb6d-48zl2" oldPhase=Running phase=Running
Jan 20 12:46:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:05.582381  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/coredns-6d4b75cb6d-48zl2" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:45 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:45 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.2 PodIPs:[{IP:10.244.0.2}] StartTime:2023-01-20 12:31:42 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:coredns State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:43 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:k8s.gcr.io/coredns/coredns:v1.8.6 ImageID:k8s.gcr.io/coredns/coredns@sha256:5b6ec0d6de9baaf3e92d0f66cd96a25b9edbce8716f5f15dcd1a616b3abd590e ContainerID:containerd://b3af3b285c950aba4fcd0176473261f7f4700aeaafe9c73ae15b15a7d6304092 Started:0xc00164af39}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:46:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:05.582743  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:46:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:05.582818  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:46:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:05.583335  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:46:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:05.583501  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 isTerminal=false
Jan 20 12:46:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:05.583557  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 updateType=0
Jan 20 12:46:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:05.587781  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:46:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:05.587793  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:46:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:05.588283  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:46:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:05.588625  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:46:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:05.588669  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:46:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:05.588676  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:46:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:05.588684  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:46:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:05.677259  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/coredns-6d4b75cb6d-48zl2" volumeName="config-volume" volumeSpecName="config-volume"
Jan 20 12:46:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:05.677414  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/coredns-6d4b75cb6d-48zl2" volumeName="kube-api-access-9qh7j" volumeSpecName="kube-api-access-9qh7j"
Jan 20 12:46:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:05.704449  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/d94e1927-d22d-4831-a764-0170eae346a1-config-volume\") pod \"coredns-6d4b75cb6d-48zl2\" (UID: \"d94e1927-d22d-4831-a764-0170eae346a1\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:46:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:05.704593  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-9qh7j\" (UniqueName: \"kubernetes.io/projected/d94e1927-d22d-4831-a764-0170eae346a1-kube-api-access-9qh7j\") pod \"coredns-6d4b75cb6d-48zl2\" (UID: \"d94e1927-d22d-4831-a764-0170eae346a1\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:46:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:05.704768  199956 projected.go:183] Setting up volume kube-api-access-9qh7j for pod d94e1927-d22d-4831-a764-0170eae346a1 at /var/lib/kubelet/pods/d94e1927-d22d-4831-a764-0170eae346a1/volumes/kubernetes.io~projected/kube-api-access-9qh7j
Jan 20 12:46:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:05.704790  199956 configmap.go:181] Setting up volume config-volume for pod d94e1927-d22d-4831-a764-0170eae346a1 at /var/lib/kubelet/pods/d94e1927-d22d-4831-a764-0170eae346a1/volumes/kubernetes.io~configmap/config-volume
Jan 20 12:46:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:05.704872  199956 configmap.go:205] Received configMap kube-system/coredns containing (1) pieces of data, 339 total bytes
Jan 20 12:46:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:05.704953  199956 quota_linux.go:271] SupportsQuotas called, but quotas disabled
Jan 20 12:46:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:05.705190  199956 atomic_writer.go:161] pod kube-system/coredns-6d4b75cb6d-48zl2 volume config-volume: no update required for target directory /var/lib/kubelet/pods/d94e1927-d22d-4831-a764-0170eae346a1/volumes/kubernetes.io~configmap/config-volume
Jan 20 12:46:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:05.705193  199956 atomic_writer.go:161] pod kube-system/coredns-6d4b75cb6d-48zl2 volume kube-api-access-9qh7j: no update required for target directory /var/lib/kubelet/pods/d94e1927-d22d-4831-a764-0170eae346a1/volumes/kubernetes.io~projected/kube-api-access-9qh7j
Jan 20 12:46:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:05.705264  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/d94e1927-d22d-4831-a764-0170eae346a1-config-volume\") pod \"coredns-6d4b75cb6d-48zl2\" (UID: \"d94e1927-d22d-4831-a764-0170eae346a1\") " pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:46:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:05.705268  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-9qh7j\" (UniqueName: \"kubernetes.io/projected/d94e1927-d22d-4831-a764-0170eae346a1-kube-api-access-9qh7j\") pod \"coredns-6d4b75cb6d-48zl2\" (UID: \"d94e1927-d22d-4831-a764-0170eae346a1\") " pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:46:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:06.025584  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:46:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:06.025623  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:46:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:06.026190  199956 interface.go:209] Interface eno2 is up
Jan 20 12:46:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:06.026317  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:46:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:06.026351  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:46:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:06.026376  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:46:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:06.026391  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:46:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:06.026411  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:46:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:06.027372  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:46:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:06.027414  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:46:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:06.027433  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:46:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:06.027454  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:46:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:06.164134  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:46:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:06.164205  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:06.175192  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:06 GMT] X-Content-Type-Options:[nosniff]] 0xc000978000 2 [] false false map[] 0xc0011c4200 0xc0015f2160}
Jan 20 12:46:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:06.175330  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:46:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:06.272027  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:46:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:06.272091  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:06.273301  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:06 GMT]] 0xc0013006e0 29 [] true false map[] 0xc0011c4500 <nil>}
Jan 20 12:46:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:06.273405  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:46:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:06.484274  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:06.484351  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:06.492149  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[93723ab4-b0fd-4afd-a347-6e1977c7919c] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:06 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008dc4e0 2 [] false false map[] 0xc001716100 0xc000629970}
Jan 20 12:46:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:06.492178  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:06.895267  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:46:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:06.895330  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:06.902574  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:06 GMT] X-Content-Type-Options:[nosniff]] 0xc001300ae0 2 [] false false map[] 0xc0000dcf00 0xc000fd9a20}
Jan 20 12:46:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:06.902622  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:46:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:07.454253  199956 handler.go:293] error while reading "/proc/199956/fd/34" link: readlink /proc/199956/fd/34: no such file or directory
Jan 20 12:46:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:07.483910  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:07.483971  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:07.491997  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[589aecbf-4cd8-4204-8183-fbb78857b5af] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:07 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0009788a0 2 [] false false map[] 0xc001716400 0xc000fd9b80}
Jan 20 12:46:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:07.492029  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:07.525646  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/etcd.yaml"
Jan 20 12:46:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:07.527803  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver.yaml"
Jan 20 12:46:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:07.530743  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Jan 20 12:46:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:07.531776  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-scheduler.yaml"
Jan 20 12:46:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:07.532059  199956 config.go:279] "Setting pods for source" source="file"
Jan 20 12:46:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:07.581993  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:46:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:07.587790  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:46:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:07.587820  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:46:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:07.588519  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:46:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:07.588939  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:46:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:07.588984  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:46:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:07.588990  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:46:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:07.588999  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:46:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:07.880678  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:46:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:08.483198  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:08.483264  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:08.491431  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[faf1cd16-cf8c-47f7-afd7-0413fea04c78] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:08 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ffa9c0 2 [] false false map[] 0xc0011c4700 0xc000b55a20}
Jan 20 12:46:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:08.491481  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:08.879056  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:46:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:08.879121  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:08.880466  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:08 GMT] X-Content-Type-Options:[nosniff]] 0xc0008dd940 2 [] true false map[] 0xc0011c4900 <nil>}
Jan 20 12:46:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:08.880587  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:46:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:09.483115  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:09.483187  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:09.495972  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[a27862c0-04a6-4abe-8453-fb38de22dfb2] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:09 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0020fc140 2 [] false false map[] 0xc001a4b400 0xc001351550}
Jan 20 12:46:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:09.496112  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:09.581930  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:46:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:09.587736  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:46:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:09.587748  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:46:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:09.587755  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:46:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:09.588453  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:46:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:09.588499  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:46:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:09.588506  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:46:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:09.588515  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:46:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:10.404170  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:46:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:10.411884  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:46:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:10.421651  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59495204Ki" capacity="65586124Ki" time="2023-01-20 12:46:10.406278672 -0500 EST m=+882.962053561"
Jan 20 12:46:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:10.421663  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64401952Ki" capacity="65061836Ki" time="2023-01-20 12:46:10.421599536 -0500 EST m=+882.977374360"
Jan 20 12:46:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:10.421670  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902002084Ki" capacity="981310056Ki" time="2023-01-20 12:46:10.406278672 -0500 EST m=+882.962053561"
Jan 20 12:46:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:10.421675  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:46:10.406278672 -0500 EST m=+882.962053561"
Jan 20 12:46:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:10.421681  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902002084Ki" capacity="981310056Ki" time="2023-01-20 12:46:01.261941568 -0500 EST"
Jan 20 12:46:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:10.421686  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:46:01.261941568 -0500 EST"
Jan 20 12:46:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:10.421691  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510788" capacity="511757" time="2023-01-20 12:46:10.42135968 -0500 EST m=+882.977134504"
Jan 20 12:46:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:10.421724  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:46:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:10.483457  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:10.483519  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:10.491202  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[e16cde3d-3288-4559-a585-a658f222c514] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:10 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0020fcd40 2 [] false false map[] 0xc001846000 0xc0014b7080}
Jan 20 12:46:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:10.491230  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:11.166851  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:46:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:11.166922  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:11.175112  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[cdb2f0bb-b75e-4846-b233-72c2612826d2] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:11 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001487480 2 [] false false map[] 0xc001846800 0xc001c3ee70}
Jan 20 12:46:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:11.175166  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:11.483396  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:11.483469  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:11.496295  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[4cd17b1d-622d-4933-a685-199865d6f200] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:11 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000320da0 2 [] false false map[] 0xc0011c5d00 0xc001d58d10}
Jan 20 12:46:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:11.496436  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:11.581140  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:46:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:11.588509  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:46:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:11.588572  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:46:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:11.590751  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:46:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:11.592717  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:46:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:11.592944  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:46:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:11.592975  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:46:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:11.593019  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:46:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:12.483818  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:12.483884  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:12.492162  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[81d992ed-d536-4fe1-bc90-92f525107a6c] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:12 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001702700 2 [] false false map[] 0xc0002c3700 0xc001ff1a20}
Jan 20 12:46:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:12.492193  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:12.688911  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:46:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:12.688982  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:12.690059  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:12 GMT]] 0xc001703f40 2 [] true false map[] 0xc001846d00 <nil>}
Jan 20 12:46:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:12.690168  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:46:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:12.696379  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:46:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:12.696442  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:12.697533  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:12 GMT]] 0xc000321520 2 [] true false map[] 0xc001846f00 <nil>}
Jan 20 12:46:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:12.697632  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:46:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:12.709893  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:46:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:12.709952  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:12.709966  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:46:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:12.710024  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:12.711024  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:12 GMT]] 0xc0020fd980 2 [] true false map[] 0xc0021a2000 <nil>}
Jan 20 12:46:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:12.711068  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:12 GMT]] 0xc000ce92c0 2 [] true false map[] 0xc0002c3a00 <nil>}
Jan 20 12:46:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:12.711120  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:46:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:12.711170  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:46:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:12.882472  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:46:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:13.483831  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:13.483902  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:13.496561  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[02dd4509-97f2-41a6-abf5-0e20487b39a8] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:13 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0020fdb20 2 [] false false map[] 0xc001847100 0xc0021d4b00}
Jan 20 12:46:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:13.496757  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:13.581523  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:46:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:13.587236  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:46:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:13.587363  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:46:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:13.587438  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:46:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:13.588322  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:46:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:13.588371  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:46:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:13.588377  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:46:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:13.588386  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:46:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:14.483222  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:14.483296  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:14.491518  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[38f551b9-008c-45b1-9ecf-4b88c5f67ca3] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:14 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ce9460 2 [] false false map[] 0xc0014ec100 0xc0013602c0}
Jan 20 12:46:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:14.491552  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:15.483915  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:15.483985  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:15.491974  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[a4e5354a-9eb3-4fa4-8368-32ff4123c946] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:15 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123e4a0 2 [] false false map[] 0xc0014ec400 0xc001982630}
Jan 20 12:46:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:15.492005  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:15.581827  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:46:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:15.587785  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:46:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:15.587797  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:46:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:15.588617  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:46:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:15.589014  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:46:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:15.589064  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:46:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:15.589071  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:46:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:15.589080  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:46:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:16.164505  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:46:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:16.164571  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:16.172295  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:16 GMT] X-Content-Type-Options:[nosniff]] 0xc0020fdea0 2 [] false false map[] 0xc0021a2500 0xc001982790}
Jan 20 12:46:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:16.172322  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:46:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:16.272180  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:46:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:16.272241  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:16.273480  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:16 GMT]] 0xc00123e7e0 29 [] true false map[] 0xc0014ec800 <nil>}
Jan 20 12:46:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:16.273579  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:46:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:16.376753  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:46:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:16.376760  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:46:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:16.376915  199956 interface.go:209] Interface eno2 is up
Jan 20 12:46:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:16.376941  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:46:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:16.376948  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:46:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:16.376952  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:46:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:16.376956  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:46:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:16.376959  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:46:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:16.377293  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:46:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:16.377301  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:46:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:16.377331  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:46:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:16.377335  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:46:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:16.483951  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:16.484014  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:16.496656  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[2bae4327-b5a0-469d-b593-251a886a5e0f] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:16 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123e800 2 [] false false map[] 0xc0000dd800 0xc001336420}
Jan 20 12:46:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:16.496841  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:16.895931  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:46:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:16.896000  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:16.903447  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:16 GMT] X-Content-Type-Options:[nosniff]] 0xc001300d80 2 [] false false map[] 0xc0021a2700 0xc0012356b0}
Jan 20 12:46:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:16.903504  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:46:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:17.483994  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:17.484067  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:17.491965  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[6c833b03-a7f9-40aa-a221-711d1cb2f481] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:17 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001301700 2 [] false false map[] 0xc0021a2a00 0xc0015b51e0}
Jan 20 12:46:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:17.491996  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:17.582117  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[kube-system/kube-scheduler-zcy-z390-aorus-master]
Jan 20 12:46:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:17.582219  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:46:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:17.582281  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 updateType=0
Jan 20 12:46:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:17.582385  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3
Jan 20 12:46:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:17.582419  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/kube-scheduler-zcy-z390-aorus-master"
Jan 20 12:46:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:17.582505  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" oldPhase=Running phase=Running
Jan 20 12:46:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:17.583157  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:27 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:41 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:41 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:27 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.239.159.53 PodIPs:[{IP:10.239.159.53}] StartTime:2023-01-20 12:31:27 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:kube-scheduler State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:22 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:6 Image:k8s.gcr.io/kube-scheduler:v1.24.0 ImageID:k8s.gcr.io/kube-scheduler@sha256:db842a7c431fd51db7e1911f6d1df27a7b6b6963ceda24852b654d2cd535b776 ContainerID:containerd://13149330f3752d0ff65bcac86c7b522b2040811e815fbc87e56b40b612875d5a Started:0xc00114a0c2}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:46:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:17.583537  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-scheduler-zcy-z390-aorus-master"
Jan 20 12:46:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:17.583608  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/kube-scheduler-zcy-z390-aorus-master"
Jan 20 12:46:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:17.584147  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/kube-scheduler-zcy-z390-aorus-master"
Jan 20 12:46:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:17.584332  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 isTerminal=false
Jan 20 12:46:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:17.584390  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 updateType=0
Jan 20 12:46:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:17.589629  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:46:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:17.589689  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:46:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:17.591802  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:46:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:17.593802  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:46:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:17.594022  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:46:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:17.594054  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:46:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:17.594096  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:46:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:17.666318  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Jan 20 12:46:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:17.884521  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:46:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:18.090454  199956 handler.go:293] error while reading "/proc/199956/fd/34" link: readlink /proc/199956/fd/34: no such file or directory
Jan 20 12:46:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:18.483130  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:18.483204  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:18.491282  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[c34b8633-8b96-4e12-969f-6e46b25f166d] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:18 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123f360 2 [] false false map[] 0xc0021a3100 0xc0016acfd0}
Jan 20 12:46:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:18.491313  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:18.879155  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/healthz" timeout="1s"
Jan 20 12:46:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:18.879218  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:46:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:18.879290  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:18.879221  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:18.880497  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:18 GMT] X-Content-Type-Options:[nosniff]] 0xc001718f60 2 [] true false map[] 0xc0014ecd00 <nil>}
Jan 20 12:46:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:18.880619  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:46:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:18.880572  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:18 GMT] X-Content-Type-Options:[nosniff]] 0xc000cb2580 2 [] true false map[] 0xc000cf7900 <nil>}
Jan 20 12:46:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:18.880737  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:46:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:19.483367  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:19.483385  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:19.486200  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[38e4e69a-ec2b-4459-834e-8a18d465362f] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:19 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001719140 2 [] false false map[] 0xc001a4a100 0xc002350000}
Jan 20 12:46:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:19.486232  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:19.582010  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:46:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:19.589345  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:46:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:19.589408  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:46:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:19.589439  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:46:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:19.591688  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:46:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:19.591909  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:46:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:19.591942  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:46:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:19.591985  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:46:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:20.422640  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:46:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:20.440505  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:46:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:20.474496  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59486092Ki" capacity="65586124Ki" time="2023-01-20 12:46:20.424651854 -0500 EST m=+892.980426740"
Jan 20 12:46:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:20.474520  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64401692Ki" capacity="65061836Ki" time="2023-01-20 12:46:20.474426603 -0500 EST m=+893.030201425"
Jan 20 12:46:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:20.474526  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902002052Ki" capacity="981310056Ki" time="2023-01-20 12:46:20.424651854 -0500 EST m=+892.980426740"
Jan 20 12:46:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:20.474532  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:46:20.424651854 -0500 EST m=+892.980426740"
Jan 20 12:46:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:20.474536  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902002052Ki" capacity="981310056Ki" time="2023-01-20 12:46:11.26251169 -0500 EST"
Jan 20 12:46:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:20.474541  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:46:11.26251169 -0500 EST"
Jan 20 12:46:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:20.474546  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510787" capacity="511757" time="2023-01-20 12:46:20.474169095 -0500 EST m=+893.029943917"
Jan 20 12:46:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:20.474570  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:46:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:20.483836  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:20.483898  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:20.497170  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[00371525-4ef6-4f2c-bd46-4ea4b1dcb650] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:20 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00013c1a0 2 [] false false map[] 0xc001f36400 0xc001b03970}
Jan 20 12:46:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:20.497331  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:21.166580  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:46:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:21.166653  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:21.174946  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[da1aaed9-96d2-481d-b477-8d2515211adf] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:21 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ffb6a0 2 [] false false map[] 0xc001f36800 0xc001ebe6e0}
Jan 20 12:46:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:21.175008  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:21.483480  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:21.483552  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:21.491765  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[f7787728-a1c0-4f54-9306-739b8cd91837] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:21 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0014878a0 2 [] false false map[] 0xc001f37100 0xc001fba630}
Jan 20 12:46:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:21.491795  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:21.581413  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:46:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:21.588797  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:46:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:21.588857  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:46:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:21.591141  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:46:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:21.593274  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:46:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:21.593495  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:46:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:21.593528  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:46:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:21.593571  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:46:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:22.483391  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:22.483466  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:22.497671  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[1f1cbaae-f57a-4852-bdeb-54b320e2e2e6] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:22 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00013c600 2 [] false false map[] 0xc0014c6100 0xc000d428f0}
Jan 20 12:46:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:22.497811  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:22.688917  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:46:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:22.688979  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:22.690097  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:22 GMT]] 0xc00013c840 2 [] true false map[] 0xc0014c6900 <nil>}
Jan 20 12:46:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:22.690211  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:46:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:22.696436  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:46:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:22.696495  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:22.697468  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:22 GMT]] 0xc00123e060 2 [] true false map[] 0xc0021a2100 <nil>}
Jan 20 12:46:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:22.697570  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:46:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:22.709811  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:46:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:22.709868  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:22.709882  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:46:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:22.709940  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:22.710863  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:22 GMT]] 0xc00123e0c0 2 [] true false map[] 0xc0014c6b00 <nil>}
Jan 20 12:46:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:22.710875  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:22 GMT]] 0xc00013c8e0 2 [] true false map[] 0xc0021a2300 <nil>}
Jan 20 12:46:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:22.710977  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:46:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:22.711002  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:46:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:22.885970  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:46:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:23.483993  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:23.484060  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:23.496789  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[5f3eb2e6-6239-4de6-b80e-f9921461ace0] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:23 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001820620 2 [] false false map[] 0xc0021a2500 0xc000e220b0}
Jan 20 12:46:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:23.496937  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:23.581532  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:46:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:23.586926  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:46:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:23.586938  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:46:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:23.586945  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:46:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:23.587592  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:46:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:23.587652  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:46:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:23.587658  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:46:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:23.587666  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:46:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:24.483738  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:24.483806  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:24.492272  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[695c1bfb-00a8-4026-b8af-e166e017ad52] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:24 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ce92e0 2 [] false false map[] 0xc0021a2700 0xc000fc8210}
Jan 20 12:46:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:24.492319  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:25.483315  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:25.483331  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:25.486508  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[5f8f26bf-5923-4a93-a723-782584d66d49] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:25 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001820dc0 2 [] false false map[] 0xc001c0a300 0xc00102a370}
Jan 20 12:46:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:25.486541  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:25.581427  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:46:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:25.586892  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:46:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:25.586904  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:46:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:25.587457  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:46:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:25.587871  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:46:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:25.587917  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:46:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:25.587924  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:46:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:25.587933  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:46:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:26.165122  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:46:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:26.165193  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:26.175933  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:26 GMT] X-Content-Type-Options:[nosniff]] 0xc000321a40 2 [] false false map[] 0xc001846f00 0xc0014d1760}
Jan 20 12:46:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:26.176059  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:46:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:26.271891  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:46:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:26.271949  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:26.273226  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:26 GMT]] 0xc000978a00 29 [] true false map[] 0xc001c0a600 <nil>}
Jan 20 12:46:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:26.273329  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:46:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:26.483602  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:26.483672  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:26.487341  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:46:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:26.487360  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:46:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:26.487572  199956 interface.go:209] Interface eno2 is up
Jan 20 12:46:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:26.487613  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:46:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:26.487620  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:46:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:26.487625  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:46:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:26.487628  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:46:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:26.487632  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:46:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:26.488172  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:46:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:26.488181  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:46:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:26.488186  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:46:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:26.488190  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:46:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:26.488819  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[cf2f1410-6f28-4cbb-b9d1-be9ea72a5798] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:26 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000979520 2 [] false false map[] 0xc0021a3b00 0xc00156f8c0}
Jan 20 12:46:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:26.488845  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:26.895637  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:46:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:26.895709  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:26.903536  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:26 GMT] X-Content-Type-Options:[nosniff]] 0xc000979840 2 [] false false map[] 0xc0014c6f00 0xc0005ffce0}
Jan 20 12:46:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:26.903597  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.483218  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.483287  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.495913  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[ddfb5c15-012a-416a-ac66-f186625b839f] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:27 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00013ce00 2 [] false false map[] 0xc0014c7400 0xc0005ffef0}
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.496052  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.526329  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/etcd.yaml"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.528480  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver.yaml"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.531470  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.533378  199956 kubelet.go:1308] "Image garbage collection succeeded"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.534317  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-scheduler.yaml"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.535930  199956 config.go:279] "Setting pods for source" source="file"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.540842  199956 kubelet_getters.go:176] "Pod status updated" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" status=Running
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.540924  199956 kubelet_getters.go:176] "Pod status updated" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" status=Running
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.540945  199956 kubelet_getters.go:176] "Pod status updated" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" status=Running
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.540966  199956 kubelet_getters.go:176] "Pod status updated" pod="kube-system/etcd-zcy-z390-aorus-master" status=Running
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.582027  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.587818  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.587830  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.588550  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.588852  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.588896  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.588902  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.588910  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.629151  199956 oom_linux.go:66] attempting to set "/proc/199956/oom_score_adj" to "-999"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630657  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/kmod-static-nodes.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630666  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/kmod-static-nodes.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630671  199956 factory.go:255] Factory "raw" can handle container "/system.slice/kmod-static-nodes.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630677  199956 manager.go:925] ignoring container "/system.slice/kmod-static-nodes.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630681  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630683  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630687  199956 factory.go:255] Factory "raw" can handle container "/user.slice", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630691  199956 manager.go:925] ignoring container "/user.slice"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630693  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-d2688d45\\x2d2487\\x2d46e7\\x2daecb\\x2de3479626909d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d4pnfq.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630699  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-d2688d45\\x2d2487\\x2d46e7\\x2daecb\\x2de3479626909d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d4pnfq.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630705  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-d2688d45\\x2d2487\\x2d46e7\\x2daecb\\x2de3479626909d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d4pnfq.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630709  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2d719b045b\\x2df019\\x2d025c\\x2d185d\\x2da976163f375a.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630713  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2d719b045b\\x2df019\\x2d025c\\x2d185d\\x2da976163f375a.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630717  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2d719b045b\\x2df019\\x2d025c\\x2d185d\\x2da976163f375a.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630721  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f8443d2868215ec42d3fa335663976e33df166c5e98369aa3f9d7ddb9235bead-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630725  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f8443d2868215ec42d3fa335663976e33df166c5e98369aa3f9d7ddb9235bead-rootfs.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630729  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f8443d2868215ec42d3fa335663976e33df166c5e98369aa3f9d7ddb9235bead-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630733  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/lvm2-lvmpolld.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630736  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/lvm2-lvmpolld.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630739  199956 factory.go:255] Factory "raw" can handle container "/system.slice/lvm2-lvmpolld.socket", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630743  199956 manager.go:925] ignoring container "/system.slice/lvm2-lvmpolld.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630746  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/user-runtime-dir@0.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630749  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/user-runtime-dir@0.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630752  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/user-runtime-dir@0.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630756  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/user-runtime-dir@0.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630759  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journald.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630762  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journald.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630765  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journald.socket", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630769  199956 manager.go:925] ignoring container "/system.slice/systemd-journald.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630772  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/cups.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630775  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/cups.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630778  199956 factory.go:255] Factory "raw" can handle container "/system.slice/cups.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630782  199956 manager.go:925] ignoring container "/system.slice/cups.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630785  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-shm.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630789  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-shm.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630794  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-shm.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630797  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/keyboard-setup.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630800  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/keyboard-setup.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630803  199956 factory.go:255] Factory "raw" can handle container "/system.slice/keyboard-setup.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630807  199956 manager.go:925] ignoring container "/system.slice/keyboard-setup.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630810  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snapd.apparmor.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630814  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/snapd.apparmor.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630817  199956 factory.go:255] Factory "raw" can handle container "/system.slice/snapd.apparmor.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630821  199956 manager.go:925] ignoring container "/system.slice/snapd.apparmor.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630824  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-fs-fuse-connections.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630827  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-fs-fuse-connections.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630831  199956 manager.go:925] ignoring container "/system.slice/sys-fs-fuse-connections.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630835  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-shm.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630839  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-shm.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630843  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-shm.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630847  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-initctl.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630850  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-initctl.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630853  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-initctl.socket", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630857  199956 manager.go:925] ignoring container "/system.slice/systemd-initctl.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630862  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/apport.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630865  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/apport.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630868  199956 factory.go:255] Factory "raw" can handle container "/system.slice/apport.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630872  199956 manager.go:925] ignoring container "/system.slice/apport.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630875  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/avahi-daemon.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630878  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/avahi-daemon.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630881  199956 factory.go:255] Factory "raw" can handle container "/system.slice/avahi-daemon.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630885  199956 manager.go:925] ignoring container "/system.slice/avahi-daemon.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630888  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/user@0.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630891  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/user@0.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630894  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/user@0.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630898  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/user@0.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630901  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630904  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630907  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630911  199956 manager.go:925] ignoring container "/user.slice/user-0.slice"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630913  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-9b11acac1b891eafc7ff2b244f1c42938f71a575ce81ff917e3b7ebf61d2e045-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630918  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-9b11acac1b891eafc7ff2b244f1c42938f71a575ce81ff917e3b7ebf61d2e045-rootfs.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630923  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-9b11acac1b891eafc7ff2b244f1c42938f71a575ce81ff917e3b7ebf61d2e045-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630926  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-shm.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630930  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-shm.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630935  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-shm.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630939  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/docker.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630941  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/docker.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630944  199956 factory.go:255] Factory "raw" can handle container "/system.slice/docker.socket", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630949  199956 manager.go:925] ignoring container "/system.slice/docker.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630951  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/alsa-restore.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630954  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/alsa-restore.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630957  199956 factory.go:255] Factory "raw" can handle container "/system.slice/alsa-restore.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630961  199956 manager.go:925] ignoring container "/system.slice/alsa-restore.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630964  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/kerneloops.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630966  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/kerneloops.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630970  199956 factory.go:255] Factory "raw" can handle container "/system.slice/kerneloops.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630974  199956 manager.go:925] ignoring container "/system.slice/kerneloops.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630976  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-logind.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630979  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-logind.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630982  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-logind.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630986  199956 manager.go:925] ignoring container "/system.slice/systemd-logind.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630990  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-1000.slice/user@1000.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630993  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-1000.slice/user@1000.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.630996  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-1000.slice/user@1000.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631001  199956 manager.go:925] ignoring container "/user.slice/user-1000.slice/user@1000.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631003  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/-.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631006  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/-.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631010  199956 manager.go:925] ignoring container "/system.slice/-.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631013  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-rfkill.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631016  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-rfkill.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631019  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-rfkill.socket", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631023  199956 manager.go:925] ignoring container "/system.slice/systemd-rfkill.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631026  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journald-audit.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631029  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journald-audit.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631032  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journald-audit.socket", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631036  199956 manager.go:925] ignoring container "/system.slice/systemd-journald-audit.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631039  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-systemd\\x2dfsck.slice/systemd-fsck@dev-disk-by\\x2duuid-4B5A\\x2dDC89.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631042  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-systemd\\x2dfsck.slice/systemd-fsck@dev-disk-by\\x2duuid-4B5A\\x2dDC89.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631046  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-systemd\\x2dfsck.slice/systemd-fsck@dev-disk-by\\x2duuid-4B5A\\x2dDC89.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631051  199956 manager.go:925] ignoring container "/system.slice/system-systemd\\x2dfsck.slice/systemd-fsck@dev-disk-by\\x2duuid-4B5A\\x2dDC89.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631054  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-1cb44bdbde0b41852e382d7dab7c184af8dd4e5b25d5cfe6a10a9c8ab601659d-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631058  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-1cb44bdbde0b41852e382d7dab7c184af8dd4e5b25d5cfe6a10a9c8ab601659d-rootfs.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631063  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-1cb44bdbde0b41852e382d7dab7c184af8dd4e5b25d5cfe6a10a9c8ab601659d-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631067  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2d562f9e70\\x2db2b7\\x2d3ac4\\x2d2311\\x2db91510a5a9ac.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631071  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2d562f9e70\\x2db2b7\\x2d3ac4\\x2d2311\\x2db91510a5a9ac.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631075  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2d562f9e70\\x2db2b7\\x2d3ac4\\x2d2311\\x2db91510a5a9ac.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631079  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/irqbalance.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631082  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/irqbalance.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631085  199956 factory.go:255] Factory "raw" can handle container "/system.slice/irqbalance.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631089  199956 manager.go:925] ignoring container "/system.slice/irqbalance.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631092  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-update-utmp.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631094  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-update-utmp.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631097  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-update-utmp.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631101  199956 manager.go:925] ignoring container "/system.slice/systemd-update-utmp.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631104  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631108  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-rootfs.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631113  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631117  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/rtkit-daemon.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631120  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/rtkit-daemon.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631123  199956 factory.go:255] Factory "raw" can handle container "/system.slice/rtkit-daemon.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631127  199956 manager.go:925] ignoring container "/system.slice/rtkit-daemon.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631130  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-snapd-16292.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631133  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-snapd-16292.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631138  199956 manager.go:925] ignoring container "/system.slice/snap-snapd-16292.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631141  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dev-hugepages.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631144  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/dev-hugepages.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631148  199956 manager.go:925] ignoring container "/system.slice/dev-hugepages.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631151  199956 factory.go:262] Factory "containerd" was unable to handle container "/init.scope"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631154  199956 factory.go:262] Factory "systemd" was unable to handle container "/init.scope"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631157  199956 factory.go:255] Factory "raw" can handle container "/init.scope", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631161  199956 manager.go:925] ignoring container "/init.scope"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631163  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-snapd-ns-snap\\x2dstore.mnt.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631167  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-snapd-ns-snap\\x2dstore.mnt.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631171  199956 manager.go:925] ignoring container "/system.slice/run-snapd-ns-snap\\x2dstore.mnt.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631174  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-d94e1927\\x2dd22d\\x2d4831\\x2da764\\x2d0170eae346a1-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d9qh7j.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631179  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-d94e1927\\x2dd22d\\x2d4831\\x2da764\\x2d0170eae346a1-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d9qh7j.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631184  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-d94e1927\\x2dd22d\\x2d4831\\x2da764\\x2d0170eae346a1-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d9qh7j.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631189  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-shm.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631193  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-shm.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631198  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-shm.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631202  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-getty.slice"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631205  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-getty.slice"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631208  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-getty.slice", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631212  199956 manager.go:925] ignoring container "/system.slice/system-getty.slice"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631215  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631217  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631220  199956 factory.go:255] Factory "raw" can handle container "/system.slice", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631224  199956 manager.go:925] ignoring container "/system.slice"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631227  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-udevd-kernel.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631229  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-udevd-kernel.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631233  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-udevd-kernel.socket", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631237  199956 manager.go:925] ignoring container "/system.slice/systemd-udevd-kernel.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631240  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-modules-load.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631242  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-modules-load.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631245  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-modules-load.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631250  199956 manager.go:925] ignoring container "/system.slice/systemd-modules-load.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631253  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/avahi-daemon.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631255  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/avahi-daemon.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631258  199956 factory.go:255] Factory "raw" can handle container "/system.slice/avahi-daemon.socket", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631262  199956 manager.go:925] ignoring container "/system.slice/avahi-daemon.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631265  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/acpid.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631269  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/acpid.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631272  199956 factory.go:255] Factory "raw" can handle container "/system.slice/acpid.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631276  199956 manager.go:925] ignoring container "/system.slice/acpid.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631279  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e0c5b991f81d5128566686552e45a9bbc8c584e4f6c94909edb3a42720dacc90-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631284  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e0c5b991f81d5128566686552e45a9bbc8c584e4f6c94909edb3a42720dacc90-rootfs.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631289  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e0c5b991f81d5128566686552e45a9bbc8c584e4f6c94909edb3a42720dacc90-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631292  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-0f9ae677fe935afcf43e145281deae0d663ba95fda6759ff24f0dd3e1cee17a9-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631296  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-0f9ae677fe935afcf43e145281deae0d663ba95fda6759ff24f0dd3e1cee17a9-rootfs.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631301  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-0f9ae677fe935afcf43e145281deae0d663ba95fda6759ff24f0dd3e1cee17a9-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631305  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/whoopsie.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631307  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/whoopsie.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631311  199956 factory.go:255] Factory "raw" can handle container "/system.slice/whoopsie.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631315  199956 manager.go:925] ignoring container "/system.slice/whoopsie.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631317  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/NetworkManager.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631320  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/NetworkManager.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631323  199956 factory.go:255] Factory "raw" can handle container "/system.slice/NetworkManager.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631327  199956 manager.go:925] ignoring container "/system.slice/NetworkManager.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631330  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journald.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631332  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journald.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631336  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journald.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631340  199956 manager.go:925] ignoring container "/system.slice/systemd-journald.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631343  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631347  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-rootfs.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631352  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631355  199956 factory.go:262] Factory "containerd" was unable to handle container "/kata_overhead"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631358  199956 factory.go:262] Factory "systemd" was unable to handle container "/kata_overhead"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631361  199956 factory.go:255] Factory "raw" can handle container "/kata_overhead", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631365  199956 manager.go:925] ignoring container "/kata_overhead"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631367  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-shm.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631371  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-shm.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631376  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-shm.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631380  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snapd.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631383  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/snapd.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631386  199956 factory.go:255] Factory "raw" can handle container "/system.slice/snapd.socket", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631390  199956 manager.go:925] ignoring container "/system.slice/snapd.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631393  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f78ed441ff85d669a403a76c0987f2d86913431bd96d454b1a3605bdcf0474f2-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631397  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f78ed441ff85d669a403a76c0987f2d86913431bd96d454b1a3605bdcf0474f2-rootfs.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631402  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f78ed441ff85d669a403a76c0987f2d86913431bd96d454b1a3605bdcf0474f2-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631406  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-shm.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631410  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-shm.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631415  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-shm.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631418  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-user-0.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631421  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-user-0.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631425  199956 manager.go:925] ignoring container "/system.slice/run-user-0.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631428  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2d58469e7a\\x2dc331\\x2d785c\\x2dc760\\x2d93c7232334bd.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631432  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2d58469e7a\\x2dc331\\x2d785c\\x2dc760\\x2d93c7232334bd.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631436  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2d58469e7a\\x2dc331\\x2d785c\\x2dc760\\x2d93c7232334bd.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631439  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/acpid.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631442  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/acpid.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631445  199956 factory.go:255] Factory "raw" can handle container "/system.slice/acpid.socket", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631449  199956 manager.go:925] ignoring container "/system.slice/acpid.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631453  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-04e472cb\\x2db6f9\\x2d4065\\x2db509\\x2dd7e1579fc48d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dhqj8d.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631458  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-04e472cb\\x2db6f9\\x2d4065\\x2db509\\x2dd7e1579fc48d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dhqj8d.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631463  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-04e472cb\\x2db6f9\\x2d4065\\x2db509\\x2dd7e1579fc48d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dhqj8d.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631467  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-resolved.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631470  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-resolved.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631473  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-resolved.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631477  199956 manager.go:925] ignoring container "/system.slice/systemd-resolved.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631480  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/udisks2.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631482  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/udisks2.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631486  199956 factory.go:255] Factory "raw" can handle container "/system.slice/udisks2.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631489  199956 manager.go:925] ignoring container "/system.slice/udisks2.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631492  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-shm.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631496  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-shm.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631501  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-shm.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631505  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/bluetooth.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631508  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/bluetooth.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631511  199956 factory.go:255] Factory "raw" can handle container "/system.slice/bluetooth.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631515  199956 manager.go:925] ignoring container "/system.slice/bluetooth.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631518  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/cups-browsed.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631520  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/cups-browsed.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631524  199956 factory.go:255] Factory "raw" can handle container "/system.slice/cups-browsed.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631528  199956 manager.go:925] ignoring container "/system.slice/cups-browsed.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631530  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-kernel-debug-tracing.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631534  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-kernel-debug-tracing.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631538  199956 manager.go:925] ignoring container "/system.slice/sys-kernel-debug-tracing.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631541  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/session-40.scope"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631544  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/session-40.scope"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631547  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/session-40.scope", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631551  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/session-40.scope"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631554  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-systemd\\x2dfsck.slice"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631557  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-systemd\\x2dfsck.slice"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631560  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-systemd\\x2dfsck.slice", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631564  199956 manager.go:925] ignoring container "/system.slice/system-systemd\\x2dfsck.slice"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631567  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-udev-trigger.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631570  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-udev-trigger.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631573  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-udev-trigger.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631577  199956 manager.go:925] ignoring container "/system.slice/systemd-udev-trigger.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631580  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-13149330f3752d0ff65bcac86c7b522b2040811e815fbc87e56b40b612875d5a-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631584  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-13149330f3752d0ff65bcac86c7b522b2040811e815fbc87e56b40b612875d5a-rootfs.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631589  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-13149330f3752d0ff65bcac86c7b522b2040811e815fbc87e56b40b612875d5a-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631592  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/lvm2-monitor.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631596  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/lvm2-monitor.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631599  199956 factory.go:255] Factory "raw" can handle container "/system.slice/lvm2-monitor.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631603  199956 manager.go:925] ignoring container "/system.slice/lvm2-monitor.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631606  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/ufw.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631609  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/ufw.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631612  199956 factory.go:255] Factory "raw" can handle container "/system.slice/ufw.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631616  199956 manager.go:925] ignoring container "/system.slice/ufw.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631619  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/NetworkManager-wait-online.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631622  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/NetworkManager-wait-online.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631625  199956 factory.go:255] Factory "raw" can handle container "/system.slice/NetworkManager-wait-online.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631629  199956 manager.go:925] ignoring container "/system.slice/NetworkManager-wait-online.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631632  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631636  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-rootfs.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631641  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631645  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/syslog.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631648  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/syslog.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631651  199956 factory.go:255] Factory "raw" can handle container "/system.slice/syslog.socket", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631655  199956 manager.go:925] ignoring container "/system.slice/syslog.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631658  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/blk-availability.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631660  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/blk-availability.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631664  199956 factory.go:255] Factory "raw" can handle container "/system.slice/blk-availability.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631668  199956 manager.go:925] ignoring container "/system.slice/blk-availability.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631670  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-core20-1611.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631674  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-core20-1611.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631678  199956 manager.go:925] ignoring container "/system.slice/snap-core20-1611.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631680  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-kernel-config.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631684  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-kernel-config.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631688  199956 manager.go:925] ignoring container "/system.slice/sys-kernel-config.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631690  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-gnome\\x2d3\\x2d38\\x2d2004-115.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631694  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-gnome\\x2d3\\x2d38\\x2d2004-115.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631698  199956 manager.go:925] ignoring container "/system.slice/snap-gnome\\x2d3\\x2d38\\x2d2004-115.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631701  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-kernel-debug.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631704  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-kernel-debug.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631708  199956 manager.go:925] ignoring container "/system.slice/sys-kernel-debug.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631711  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-shm.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631715  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-shm.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631720  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-shm.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631723  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2dd678d9bc\\x2d8fcb\\x2d9fbe\\x2d1142\\x2ddede54862bab.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631728  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2dd678d9bc\\x2d8fcb\\x2d9fbe\\x2d1142\\x2ddede54862bab.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631733  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2dd678d9bc\\x2d8fcb\\x2d9fbe\\x2d1142\\x2ddede54862bab.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631737  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snapd.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631740  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/snapd.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631743  199956 factory.go:255] Factory "raw" can handle container "/system.slice/snapd.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631747  199956 manager.go:925] ignoring container "/system.slice/snapd.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631749  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/session-42.scope"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631752  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/session-42.scope"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631755  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/session-42.scope", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631759  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/session-42.scope"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631762  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e489181a7f95431b6d92f037dbe3f788b46a5e024df7aaf29e0115dab1168ab1-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631767  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e489181a7f95431b6d92f037dbe3f788b46a5e024df7aaf29e0115dab1168ab1-rootfs.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631772  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e489181a7f95431b6d92f037dbe3f788b46a5e024df7aaf29e0115dab1168ab1-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631775  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-tmpfiles-setup.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631778  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-tmpfiles-setup.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631781  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-tmpfiles-setup.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631785  199956 manager.go:925] ignoring container "/system.slice/systemd-tmpfiles-setup.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631788  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-udevd-control.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631791  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-udevd-control.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631794  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-udevd-control.socket", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631798  199956 manager.go:925] ignoring container "/system.slice/systemd-udevd-control.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631801  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631805  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-rootfs.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631810  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631813  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-timesyncd.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631816  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-timesyncd.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631820  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-timesyncd.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631824  199956 manager.go:925] ignoring container "/system.slice/systemd-timesyncd.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631826  199956 factory.go:262] Factory "containerd" was unable to handle container "/docker"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631829  199956 factory.go:262] Factory "systemd" was unable to handle container "/docker"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631832  199956 factory.go:255] Factory "raw" can handle container "/docker", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631836  199956 manager.go:925] ignoring container "/docker"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631838  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/docker.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631841  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/docker.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631844  199956 factory.go:255] Factory "raw" can handle container "/system.slice/docker.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631848  199956 manager.go:925] ignoring container "/system.slice/docker.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631851  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/gdm.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631854  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/gdm.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631857  199956 factory.go:255] Factory "raw" can handle container "/system.slice/gdm.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631862  199956 manager.go:925] ignoring container "/system.slice/gdm.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631865  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-remount-fs.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631868  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-remount-fs.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631871  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-remount-fs.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631875  199956 manager.go:925] ignoring container "/system.slice/systemd-remount-fs.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631878  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/user@0.service/dbus.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631880  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/user@0.service/dbus.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631884  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/user@0.service/dbus.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631888  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/user@0.service/dbus.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631891  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-1c057a0e\\x2d3ee3\\x2d44f3\\x2db294\\x2d434acc8f9491-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d2sbqp.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631895  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-1c057a0e\\x2d3ee3\\x2d44f3\\x2db294\\x2d434acc8f9491-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d2sbqp.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631901  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-1c057a0e\\x2d3ee3\\x2d44f3\\x2db294\\x2d434acc8f9491-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d2sbqp.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631911  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631915  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-rootfs.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631920  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631924  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-shm.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631928  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-shm.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631933  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-shm.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631937  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-lvm2\\x2dpvscan.slice/lvm2-pvscan@8:10.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631940  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-lvm2\\x2dpvscan.slice/lvm2-pvscan@8:10.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631943  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-lvm2\\x2dpvscan.slice/lvm2-pvscan@8:10.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631947  199956 manager.go:925] ignoring container "/system.slice/system-lvm2\\x2dpvscan.slice/lvm2-pvscan@8:10.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631950  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631954  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-rootfs.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631959  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631963  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/networkd-dispatcher.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631966  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/networkd-dispatcher.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631969  199956 factory.go:255] Factory "raw" can handle container "/system.slice/networkd-dispatcher.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631973  199956 manager.go:925] ignoring container "/system.slice/networkd-dispatcher.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631976  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/wpa_supplicant.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631979  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/wpa_supplicant.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631982  199956 factory.go:255] Factory "raw" can handle container "/system.slice/wpa_supplicant.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631986  199956 manager.go:925] ignoring container "/system.slice/wpa_supplicant.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631988  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/colord.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631991  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/colord.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631994  199956 factory.go:255] Factory "raw" can handle container "/system.slice/colord.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.631998  199956 manager.go:925] ignoring container "/system.slice/colord.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632001  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/user@0.service/dbus.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632004  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/user@0.service/dbus.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632007  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/user@0.service/dbus.socket", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632011  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/user@0.service/dbus.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632014  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/uuidd.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632017  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/uuidd.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632020  199956 factory.go:255] Factory "raw" can handle container "/system.slice/uuidd.socket", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632025  199956 manager.go:925] ignoring container "/system.slice/uuidd.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632029  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b3af3b285c950aba4fcd0176473261f7f4700aeaafe9c73ae15b15a7d6304092-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632033  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b3af3b285c950aba4fcd0176473261f7f4700aeaafe9c73ae15b15a7d6304092-rootfs.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632038  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b3af3b285c950aba4fcd0176473261f7f4700aeaafe9c73ae15b15a7d6304092-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632044  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/proc-sys-fs-binfmt_misc.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632047  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/proc-sys-fs-binfmt_misc.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632051  199956 manager.go:925] ignoring container "/system.slice/proc-sys-fs-binfmt_misc.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632054  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/boot-efi.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632057  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/boot-efi.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632061  199956 manager.go:925] ignoring container "/system.slice/boot-efi.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632064  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632068  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-rootfs.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632073  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632077  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e10efaa459a32161059fda75d4bcaf3bbdb896781f772b508e43fe00bc7c9003-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632081  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e10efaa459a32161059fda75d4bcaf3bbdb896781f772b508e43fe00bc7c9003-rootfs.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632086  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e10efaa459a32161059fda75d4bcaf3bbdb896781f772b508e43fe00bc7c9003-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632089  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-7af065b7\\x2d9095\\x2d4d91\\x2d9b9e\\x2d2644e7b1f4bf-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dx6vjr.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632093  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-7af065b7\\x2d9095\\x2d4d91\\x2d9b9e\\x2d2644e7b1f4bf-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dx6vjr.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632099  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-7af065b7\\x2d9095\\x2d4d91\\x2d9b9e\\x2d2644e7b1f4bf-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dx6vjr.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632103  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-udevd.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632105  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-udevd.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632109  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-udevd.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632113  199956 manager.go:925] ignoring container "/system.slice/systemd-udevd.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632115  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/thermald.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632118  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/thermald.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632121  199956 factory.go:255] Factory "raw" can handle container "/system.slice/thermald.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632125  199956 manager.go:925] ignoring container "/system.slice/thermald.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632128  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-5900ac5c1383a321a6ae837b57d6d29d7a660a102c0806210a9ea57290673062-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632132  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-5900ac5c1383a321a6ae837b57d6d29d7a660a102c0806210a9ea57290673062-rootfs.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632137  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-5900ac5c1383a321a6ae837b57d6d29d7a660a102c0806210a9ea57290673062-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632141  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-user-1000.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632144  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-user-1000.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632148  199956 manager.go:925] ignoring container "/system.slice/run-user-1000.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632151  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journald-dev-log.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632154  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journald-dev-log.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632157  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journald-dev-log.socket", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632161  199956 manager.go:925] ignoring container "/system.slice/systemd-journald-dev-log.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632164  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-snapd-ns.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632167  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-snapd-ns.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632171  199956 manager.go:925] ignoring container "/system.slice/run-snapd-ns.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632174  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/rsyslog.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632177  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/rsyslog.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632180  199956 factory.go:255] Factory "raw" can handle container "/system.slice/rsyslog.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632184  199956 manager.go:925] ignoring container "/system.slice/rsyslog.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632186  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/openvpn.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632189  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/openvpn.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632192  199956 factory.go:255] Factory "raw" can handle container "/system.slice/openvpn.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632197  199956 manager.go:925] ignoring container "/system.slice/openvpn.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632200  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-bare-5.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632203  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-bare-5.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632207  199956 manager.go:925] ignoring container "/system.slice/snap-bare-5.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632210  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-shm.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632214  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-shm.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632219  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-shm.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632223  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-tmpfiles-setup-dev.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632226  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-tmpfiles-setup-dev.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632229  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-tmpfiles-setup-dev.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632233  199956 manager.go:925] ignoring container "/system.slice/systemd-tmpfiles-setup-dev.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632236  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dm-event.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632239  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/dm-event.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632242  199956 factory.go:255] Factory "raw" can handle container "/system.slice/dm-event.socket", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632246  199956 manager.go:925] ignoring container "/system.slice/dm-event.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632249  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/ssh.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632251  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/ssh.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632255  199956 factory.go:255] Factory "raw" can handle container "/system.slice/ssh.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632259  199956 manager.go:925] ignoring container "/system.slice/ssh.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632261  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-user-1000-gvfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632264  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-user-1000-gvfs.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632269  199956 manager.go:925] ignoring container "/system.slice/run-user-1000-gvfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632272  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-lvm2\\x2dpvscan.slice"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632275  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-lvm2\\x2dpvscan.slice"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632278  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-lvm2\\x2dpvscan.slice", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632282  199956 manager.go:925] ignoring container "/system.slice/system-lvm2\\x2dpvscan.slice"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632285  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-sysusers.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632288  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-sysusers.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632291  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-sysusers.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632295  199956 manager.go:925] ignoring container "/system.slice/systemd-sysusers.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632298  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snapd.seeded.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632300  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/snapd.seeded.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632304  199956 factory.go:255] Factory "raw" can handle container "/system.slice/snapd.seeded.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632308  199956 manager.go:925] ignoring container "/system.slice/snapd.seeded.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632310  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-cf25b9ea\\x2d6823\\x2d4eff\\x2db30d\\x2d36a09f9d897e-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dtqzsm.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632315  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-cf25b9ea\\x2d6823\\x2d4eff\\x2db30d\\x2d36a09f9d897e-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dtqzsm.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632320  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-cf25b9ea\\x2d6823\\x2d4eff\\x2db30d\\x2d36a09f9d897e-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dtqzsm.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632324  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/uuidd.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632328  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/uuidd.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632332  199956 factory.go:255] Factory "raw" can handle container "/system.slice/uuidd.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632336  199956 manager.go:925] ignoring container "/system.slice/uuidd.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632338  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-fsckd.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632341  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-fsckd.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632344  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-fsckd.socket", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632348  199956 manager.go:925] ignoring container "/system.slice/systemd-fsckd.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632351  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/apparmor.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632354  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/apparmor.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632357  199956 factory.go:255] Factory "raw" can handle container "/system.slice/apparmor.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632361  199956 manager.go:925] ignoring container "/system.slice/apparmor.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632364  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-sysctl.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632367  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-sysctl.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632370  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-sysctl.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632374  199956 manager.go:925] ignoring container "/system.slice/systemd-sysctl.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632377  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/cron.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632379  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/cron.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632382  199956 factory.go:255] Factory "raw" can handle container "/system.slice/cron.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632386  199956 manager.go:925] ignoring container "/system.slice/cron.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632389  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-1000.slice/session-1.scope"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632392  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-1000.slice/session-1.scope"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632396  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-1000.slice/session-1.scope", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632400  199956 manager.go:925] ignoring container "/user.slice/user-1000.slice/session-1.scope"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632402  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632407  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-rootfs.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632412  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632415  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-modprobe.slice"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632418  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-modprobe.slice"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632421  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-modprobe.slice", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632425  199956 manager.go:925] ignoring container "/system.slice/system-modprobe.slice"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632428  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/ModemManager.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632430  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/ModemManager.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632434  199956 factory.go:255] Factory "raw" can handle container "/system.slice/ModemManager.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632438  199956 manager.go:925] ignoring container "/system.slice/ModemManager.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632441  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-random-seed.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632444  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-random-seed.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632448  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-random-seed.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632452  199956 manager.go:925] ignoring container "/system.slice/systemd-random-seed.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632455  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/setvtrgb.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632458  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/setvtrgb.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632461  199956 factory.go:255] Factory "raw" can handle container "/system.slice/setvtrgb.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632465  199956 manager.go:925] ignoring container "/system.slice/setvtrgb.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632468  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/containerd.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632471  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/containerd.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632474  199956 factory.go:255] Factory "raw" can handle container "/system.slice/containerd.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632478  199956 manager.go:925] ignoring container "/system.slice/containerd.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632480  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/polkit.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632483  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/polkit.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632486  199956 factory.go:255] Factory "raw" can handle container "/system.slice/polkit.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632490  199956 manager.go:925] ignoring container "/system.slice/polkit.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632493  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/upower.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632495  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/upower.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632498  199956 factory.go:255] Factory "raw" can handle container "/system.slice/upower.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632502  199956 manager.go:925] ignoring container "/system.slice/upower.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632505  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-1000.slice/user-runtime-dir@1000.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632508  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-1000.slice/user-runtime-dir@1000.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632512  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-1000.slice/user-runtime-dir@1000.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632516  199956 manager.go:925] ignoring container "/user.slice/user-1000.slice/user-runtime-dir@1000.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632519  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dbus.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632522  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/dbus.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632525  199956 factory.go:255] Factory "raw" can handle container "/system.slice/dbus.socket", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632528  199956 manager.go:925] ignoring container "/system.slice/dbus.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632531  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-kernel-tracing.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632534  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-kernel-tracing.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632538  199956 manager.go:925] ignoring container "/system.slice/sys-kernel-tracing.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632541  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632545  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-rootfs.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632550  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632554  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-snap\\x2dstore-558.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632557  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-snap\\x2dstore-558.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632562  199956 manager.go:925] ignoring container "/system.slice/snap-snap\\x2dstore-558.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632566  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-shm.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632570  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-shm.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632575  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-shm.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632578  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-user-sessions.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632581  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-user-sessions.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632584  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-user-sessions.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632588  199956 manager.go:925] ignoring container "/system.slice/systemd-user-sessions.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632591  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-machine-id-commit.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632594  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-machine-id-commit.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632597  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-machine-id-commit.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632601  199956 manager.go:925] ignoring container "/system.slice/systemd-machine-id-commit.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632604  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/cups.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632607  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/cups.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632610  199956 factory.go:255] Factory "raw" can handle container "/system.slice/cups.socket", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632614  199956 manager.go:925] ignoring container "/system.slice/cups.socket"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632616  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632620  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-rootfs.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632626  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632629  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dbus.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632632  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/dbus.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632635  199956 factory.go:255] Factory "raw" can handle container "/system.slice/dbus.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632639  199956 manager.go:925] ignoring container "/system.slice/dbus.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632642  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/session-44.scope"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632644  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/session-44.scope"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632648  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/session-44.scope", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632652  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/session-44.scope"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632654  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-gtk\\x2dcommon\\x2dthemes-1535.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632658  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-gtk\\x2dcommon\\x2dthemes-1535.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632662  199956 manager.go:925] ignoring container "/system.slice/snap-gtk\\x2dcommon\\x2dthemes-1535.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632665  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/accounts-daemon.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632667  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/accounts-daemon.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632671  199956 factory.go:255] Factory "raw" can handle container "/system.slice/accounts-daemon.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632675  199956 manager.go:925] ignoring container "/system.slice/accounts-daemon.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632678  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/unattended-upgrades.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632681  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/unattended-upgrades.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632684  199956 factory.go:255] Factory "raw" can handle container "/system.slice/unattended-upgrades.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632692  199956 manager.go:925] ignoring container "/system.slice/unattended-upgrades.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632696  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journal-flush.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632699  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journal-flush.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632702  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journal-flush.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632706  199956 manager.go:925] ignoring container "/system.slice/systemd-journal-flush.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632709  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632713  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-rootfs.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632718  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632721  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dev-mqueue.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632724  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/dev-mqueue.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632728  199956 manager.go:925] ignoring container "/system.slice/dev-mqueue.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632731  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/switcheroo-control.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632734  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/switcheroo-control.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632737  199956 factory.go:255] Factory "raw" can handle container "/system.slice/switcheroo-control.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632741  199956 manager.go:925] ignoring container "/system.slice/switcheroo-control.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632744  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-b0713fbc\\x2defc5\\x2d4044\\x2d9d08\\x2d2326a0752f87-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dgcgm6.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632748  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-b0713fbc\\x2defc5\\x2d4044\\x2d9d08\\x2d2326a0752f87-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dgcgm6.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632753  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-b0713fbc\\x2defc5\\x2d4044\\x2d9d08\\x2d2326a0752f87-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dgcgm6.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632757  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2d8b7da23a\\x2d3511\\x2dfe06\\x2d72d7\\x2d26a549eb607d.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632761  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2d8b7da23a\\x2d3511\\x2dfe06\\x2d72d7\\x2d26a549eb607d.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632766  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2d8b7da23a\\x2d3511\\x2dfe06\\x2d72d7\\x2d26a549eb607d.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632769  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-1000.slice"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632772  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-1000.slice"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632775  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-1000.slice", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632779  199956 manager.go:925] ignoring container "/user.slice/user-1000.slice"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632782  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-186424b47c7b9022ecfe8996dc73cd9101f7539259cd65914279c84c9a02a288-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632786  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-186424b47c7b9022ecfe8996dc73cd9101f7539259cd65914279c84c9a02a288-rootfs.mount", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632791  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-186424b47c7b9022ecfe8996dc73cd9101f7539259cd65914279c84c9a02a288-rootfs.mount"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632795  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/console-setup.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632798  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/console-setup.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632801  199956 factory.go:255] Factory "raw" can handle container "/system.slice/console-setup.service", but ignoring.
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.632805  199956 manager.go:925] ignoring container "/system.slice/console-setup.service"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.637038  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.647730  199956 qos_container_manager_linux.go:379] "Updated QoS cgroup configuration"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.717121  199956 kubelet.go:1280] "Container garbage collection succeeded"
Jan 20 12:46:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:27.887335  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:46:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:28.483597  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:28.483664  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:28.492019  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[d91cfa8b-3887-4151-905d-c2bc65a26ce1] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:28 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008dcf20 2 [] false false map[] 0xc0022c8400 0xc0013b0fd0}
Jan 20 12:46:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:28.492051  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:28.878935  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:46:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:28.879000  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:28.880103  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:28 GMT] X-Content-Type-Options:[nosniff]] 0xc000321540 2 [] true false map[] 0xc0022c8700 <nil>}
Jan 20 12:46:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:28.880217  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:46:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:29.483762  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:29.483780  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:29.486620  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[e7c05a00-bc7a-4d7b-880a-228ce5b53f98] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:29 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001300700 2 [] false false map[] 0xc000bb0300 0xc0013babb0}
Jan 20 12:46:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:29.486648  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:29.581556  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:46:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:29.588931  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:46:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:29.588992  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:46:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:29.591245  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:46:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:29.593331  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:46:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:29.593545  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:46:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:29.593577  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:46:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:29.593620  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:46:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:30.474727  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:46:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:30.482936  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:46:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:30.483159  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:30.483188  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:30.486120  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[d5294a94-fc67-46f6-bf90-263ec6c62491] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:30 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001301e00 2 [] false false map[] 0xc001485200 0xc0018820b0}
Jan 20 12:46:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:30.486153  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:30.492712  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59476680Ki" capacity="65586124Ki" time="2023-01-20 12:46:30.476853446 -0500 EST m=+903.032628339"
Jan 20 12:46:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:30.492725  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64401392Ki" capacity="65061836Ki" time="2023-01-20 12:46:30.492644568 -0500 EST m=+903.048419393"
Jan 20 12:46:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:30.492732  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902001888Ki" capacity="981310056Ki" time="2023-01-20 12:46:30.476853446 -0500 EST m=+903.032628339"
Jan 20 12:46:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:30.492739  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:46:30.476853446 -0500 EST m=+903.032628339"
Jan 20 12:46:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:30.492744  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902001888Ki" capacity="981310056Ki" time="2023-01-20 12:46:21.263203154 -0500 EST"
Jan 20 12:46:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:30.492750  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:46:21.263203154 -0500 EST"
Jan 20 12:46:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:30.492756  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510788" capacity="511757" time="2023-01-20 12:46:30.492371318 -0500 EST m=+903.048146142"
Jan 20 12:46:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:30.492782  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:46:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:31.166099  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:46:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:31.166162  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:31.174341  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[753a1539-1c46-47ff-bcba-ad3031e3fb8f] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:31 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0009a0620 2 [] false false map[] 0xc0017cf200 0xc001882210}
Jan 20 12:46:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:31.174372  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:31.484118  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:31.484190  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:31.491849  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[305b958e-39a8-4458-b666-7f1d58f4c1e2] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:31 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0009a0860 2 [] false false map[] 0xc001485400 0xc001e84370}
Jan 20 12:46:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:31.491878  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:31.581984  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:46:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:31.587832  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:46:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:31.587844  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:46:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:31.587851  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:46:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:31.588361  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:46:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:31.588410  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:46:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:31.588416  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:46:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:31.588424  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:46:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:32.483421  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:32.483491  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:32.491320  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[066f222e-421a-4738-a9e8-465a21e8c0d4] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:32 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0009a19c0 2 [] false false map[] 0xc001485700 0xc001e76a50}
Jan 20 12:46:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:32.491366  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:32.688938  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:46:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:32.689004  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:32.690100  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:32 GMT]] 0xc001487300 2 [] true false map[] 0xc001485900 <nil>}
Jan 20 12:46:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:32.690214  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:46:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:32.696388  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:46:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:32.696451  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:32.697531  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:32 GMT]] 0xc001487340 2 [] true false map[] 0xc001a66500 <nil>}
Jan 20 12:46:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:32.697632  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:46:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:32.710135  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:46:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:32.710195  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:32.710200  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:46:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:32.710263  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:32.711227  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:32 GMT]] 0xc000321b40 2 [] true false map[] 0xc0005c1300 <nil>}
Jan 20 12:46:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:32.711280  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:32 GMT]] 0xc0009a1aa0 2 [] true false map[] 0xc001a66700 <nil>}
Jan 20 12:46:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:32.711327  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:46:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:32.711381  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:46:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:32.888952  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.483707  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.483779  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.491968  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[3db95583-4444-4b9e-9502-5ba971dadef9] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:33 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0009a1b20 2 [] false false map[] 0xc001f36100 0xc00112e9a0}
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.491999  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.582161  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[kube-system/kube-proxy-prhfv]
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.582251  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.582381  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/kube-proxy-prhfv" podUID=1c057a0e-3ee3-44f3-b294-434acc8f9491 updateType=0
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.582466  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/kube-proxy-prhfv" podUID=1c057a0e-3ee3-44f3-b294-434acc8f9491
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.582497  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/kube-proxy-prhfv"
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.582578  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/kube-proxy-prhfv" oldPhase=Running phase=Running
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.582865  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/kube-proxy-prhfv" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:43 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:43 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.239.159.53 PodIPs:[{IP:10.239.159.53}] StartTime:2023-01-20 12:31:42 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:kube-proxy State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:43 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:k8s.gcr.io/kube-proxy:v1.24.0 ImageID:k8s.gcr.io/kube-proxy@sha256:c957d602267fa61082ab8847914b2118955d0739d592cc7b01e278513478d6a8 ContainerID:containerd://f78ed441ff85d669a403a76c0987f2d86913431bd96d454b1a3605bdcf0474f2 Started:0xc0015d327e}] QOSClass:BestEffort EphemeralContainerStatuses:[]}
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.583212  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-proxy-prhfv"
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.583286  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/kube-proxy-prhfv"
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.583718  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/kube-proxy-prhfv"
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.583872  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/kube-proxy-prhfv" podUID=1c057a0e-3ee3-44f3-b294-434acc8f9491 isTerminal=false
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.583931  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/kube-proxy-prhfv" podUID=1c057a0e-3ee3-44f3-b294-434acc8f9491 updateType=0
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.587880  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.587891  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.588435  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.588782  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.588829  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.588836  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.588844  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.679584  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-proxy-prhfv" volumeName="kube-proxy" volumeSpecName="kube-proxy"
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.679693  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-proxy-prhfv" volumeName="xtables-lock" volumeSpecName="xtables-lock"
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.679768  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-proxy-prhfv" volumeName="lib-modules" volumeSpecName="lib-modules"
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.679893  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-proxy-prhfv" volumeName="kube-api-access-2sbqp" volumeSpecName="kube-api-access-2sbqp"
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.712808  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-2sbqp\" (UniqueName: \"kubernetes.io/projected/1c057a0e-3ee3-44f3-b294-434acc8f9491-kube-api-access-2sbqp\") pod \"kube-proxy-prhfv\" (UID: \"1c057a0e-3ee3-44f3-b294-434acc8f9491\") Volume is already mounted to pod, but remount was requested." pod="kube-system/kube-proxy-prhfv"
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.712934  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/1c057a0e-3ee3-44f3-b294-434acc8f9491-kube-proxy\") pod \"kube-proxy-prhfv\" (UID: \"1c057a0e-3ee3-44f3-b294-434acc8f9491\") Volume is already mounted to pod, but remount was requested." pod="kube-system/kube-proxy-prhfv"
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.713072  199956 configmap.go:181] Setting up volume kube-proxy for pod 1c057a0e-3ee3-44f3-b294-434acc8f9491 at /var/lib/kubelet/pods/1c057a0e-3ee3-44f3-b294-434acc8f9491/volumes/kubernetes.io~configmap/kube-proxy
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.713130  199956 configmap.go:205] Received configMap kube-system/kube-proxy containing (2) pieces of data, 1458 total bytes
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.713126  199956 projected.go:183] Setting up volume kube-api-access-2sbqp for pod 1c057a0e-3ee3-44f3-b294-434acc8f9491 at /var/lib/kubelet/pods/1c057a0e-3ee3-44f3-b294-434acc8f9491/volumes/kubernetes.io~projected/kube-api-access-2sbqp
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.713204  199956 quota_linux.go:271] SupportsQuotas called, but quotas disabled
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.713472  199956 atomic_writer.go:161] pod kube-system/kube-proxy-prhfv volume kube-proxy: no update required for target directory /var/lib/kubelet/pods/1c057a0e-3ee3-44f3-b294-434acc8f9491/volumes/kubernetes.io~configmap/kube-proxy
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.713533  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/1c057a0e-3ee3-44f3-b294-434acc8f9491-kube-proxy\") pod \"kube-proxy-prhfv\" (UID: \"1c057a0e-3ee3-44f3-b294-434acc8f9491\") " pod="kube-system/kube-proxy-prhfv"
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.713582  199956 atomic_writer.go:161] pod kube-system/kube-proxy-prhfv volume kube-api-access-2sbqp: no update required for target directory /var/lib/kubelet/pods/1c057a0e-3ee3-44f3-b294-434acc8f9491/volumes/kubernetes.io~projected/kube-api-access-2sbqp
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.713655  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-2sbqp\" (UniqueName: \"kubernetes.io/projected/1c057a0e-3ee3-44f3-b294-434acc8f9491-kube-api-access-2sbqp\") pod \"kube-proxy-prhfv\" (UID: \"1c057a0e-3ee3-44f3-b294-434acc8f9491\") " pod="kube-system/kube-proxy-prhfv"
Jan 20 12:46:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:33.735821  199956 handler.go:293] error while reading "/proc/199956/fd/34" link: readlink /proc/199956/fd/34: no such file or directory
Jan 20 12:46:34 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:34.483799  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:34 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:34.483868  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:34 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:34.492175  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[889f59e5-a616-4a4a-876f-36b435cda8bf] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:34 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000321b40 2 [] false false map[] 0xc0014c7300 0xc000d43ef0}
Jan 20 12:46:34 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:34.492208  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:35.484030  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:35.484052  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:35.487043  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[2568d968-2848-489f-ace3-1769c1c025cb] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:35 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0014878c0 2 [] false false map[] 0xc0014c7600 0xc0010184d0}
Jan 20 12:46:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:35.487079  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:35.582103  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:46:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:35.587866  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:46:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:35.587879  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:46:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:35.587887  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:46:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:35.588435  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:46:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:35.588483  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:46:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:35.588490  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:46:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:35.588499  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:46:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:36.164551  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:46:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:36.164637  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:36.175447  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:36 GMT] X-Content-Type-Options:[nosniff]] 0xc0008e21e0 2 [] false false map[] 0xc00105b200 0xc00138c160}
Jan 20 12:46:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:36.175573  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:46:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:36.272362  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:46:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:36.272438  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:36.273673  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:36 GMT]] 0xc0012e84c0 29 [] true false map[] 0xc0014ed400 <nil>}
Jan 20 12:46:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:36.273775  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:46:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:36.483290  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:36.483315  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:36.489369  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[e92b5726-7d1e-447b-b195-32c195e91a42] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:36 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008e2a00 2 [] false false map[] 0xc0014ed600 0xc00138c2c0}
Jan 20 12:46:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:36.489438  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:36.601560  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:46:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:36.601567  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:46:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:36.601726  199956 interface.go:209] Interface eno2 is up
Jan 20 12:46:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:36.601755  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:46:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:36.601763  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:46:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:36.601768  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:46:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:36.601771  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:46:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:36.601775  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:46:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:36.602059  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:46:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:36.602067  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:46:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:36.602072  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:46:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:36.602076  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:46:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:36.895434  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:46:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:36.895474  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:36.902199  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:36 GMT] X-Content-Type-Options:[nosniff]] 0xc0008e3a40 2 [] false false map[] 0xc0002c3d00 0xc001600d10}
Jan 20 12:46:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:36.902226  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:46:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:37.483314  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:37.483387  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:37.490904  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[461872ba-6546-4a27-8889-4eadabddd03a] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:37 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e8aa0 2 [] false false map[] 0xc0021a2800 0xc001711ce0}
Jan 20 12:46:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:37.490935  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:37.581900  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:46:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:37.587847  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:46:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:37.587862  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:46:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:37.588373  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:46:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:37.588692  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:46:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:37.588738  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:46:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:37.588745  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:46:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:37.588754  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:46:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:37.889950  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:46:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:38.483765  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:38.483834  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:38.492125  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[6ee6777b-5263-4b46-9b23-0911d78c1501] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:38 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e9560 2 [] false false map[] 0xc00105ba00 0xc001a6cc60}
Jan 20 12:46:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:38.492156  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:38.879657  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:46:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:38.879732  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:38.879746  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/healthz" timeout="1s"
Jan 20 12:46:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:38.879809  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:38.880905  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:38 GMT] X-Content-Type-Options:[nosniff]] 0xc000ce9c20 2 [] true false map[] 0xc00105bd00 <nil>}
Jan 20 12:46:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:38.880945  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:38 GMT] X-Content-Type-Options:[nosniff]] 0xc0012e9c00 2 [] true false map[] 0xc001774400 <nil>}
Jan 20 12:46:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:38.881031  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:46:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:38.881079  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:46:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:39.483967  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:39.484044  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:39.496462  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[944d0822-daef-474f-a582-fe9d87ef0d27] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:39 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ce8280 2 [] false false map[] 0xc0021a2100 0xc001e76000}
Jan 20 12:46:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:39.496602  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:39.582013  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:46:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:39.583483  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:46:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:39.583496  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:46:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:39.583503  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:46:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:39.584061  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:46:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:39.584109  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:46:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:39.584115  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:46:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:39.584124  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:46:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:40.483827  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:40.483898  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:40.492124  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[7204fcbb-60e5-4bf7-a895-ec73b6124ecd] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:40 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000978980 2 [] false false map[] 0xc0021a2700 0xc0013b09a0}
Jan 20 12:46:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:40.492160  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:40.493159  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:46:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:40.497043  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:46:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:40.507846  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64401896Ki" capacity="65061836Ki" time="2023-01-20 12:46:40.507783178 -0500 EST m=+913.063558005"
Jan 20 12:46:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:40.507864  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902001848Ki" capacity="981310056Ki" time="2023-01-20 12:46:40.493642065 -0500 EST m=+913.049416889"
Jan 20 12:46:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:40.507872  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:46:40.493642065 -0500 EST m=+913.049416889"
Jan 20 12:46:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:40.507878  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902001848Ki" capacity="981310056Ki" time="2023-01-20 12:46:31.262468575 -0500 EST"
Jan 20 12:46:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:40.507884  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:46:31.262468575 -0500 EST"
Jan 20 12:46:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:40.507891  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510788" capacity="511757" time="2023-01-20 12:46:40.507484629 -0500 EST m=+913.063259458"
Jan 20 12:46:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:40.507897  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59467284Ki" capacity="65586124Ki" time="2023-01-20 12:46:40.493642065 -0500 EST m=+913.049416889"
Jan 20 12:46:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:40.507926  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:46:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:40.581564  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[kube-system/kube-controller-manager-zcy-z390-aorus-master]
Jan 20 12:46:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:40.581645  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce updateType=0
Jan 20 12:46:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:40.581705  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce
Jan 20 12:46:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:40.581737  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master"
Jan 20 12:46:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:40.581818  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" oldPhase=Running phase=Running
Jan 20 12:46:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:40.582099  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:28 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:37 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:37 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:28 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.239.159.53 PodIPs:[{IP:10.239.159.53}] StartTime:2023-01-20 12:31:28 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:kube-controller-manager State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:22 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:6 Image:k8s.gcr.io/kube-controller-manager:v1.24.0 ImageID:k8s.gcr.io/kube-controller-manager@sha256:df044a154e79a18f749d3cd9d958c3edde2b6a00c815176472002b7bbf956637 ContainerID:containerd://f8443d2868215ec42d3fa335663976e33df166c5e98369aa3f9d7ddb9235bead Started:0xc0016590ce}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:46:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:40.582407  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master"
Jan 20 12:46:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:40.582479  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master"
Jan 20 12:46:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:40.583428  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/kube-controller-manager-zcy-z390-aorus-master"
Jan 20 12:46:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:40.583585  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce isTerminal=false
Jan 20 12:46:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:40.583638  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce updateType=0
Jan 20 12:46:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:40.630067  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="ca-certs" volumeSpecName="ca-certs"
Jan 20 12:46:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:40.630180  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Jan 20 12:46:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:40.630251  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="etc-pki" volumeSpecName="etc-pki"
Jan 20 12:46:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:40.630317  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="flexvolume-dir" volumeSpecName="flexvolume-dir"
Jan 20 12:46:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:40.630383  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Jan 20 12:46:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:40.630450  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Jan 20 12:46:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:40.630514  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Jan 20 12:46:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:40.630579  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Jan 20 12:46:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:41.166866  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:46:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:41.166931  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:41.180837  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[15e4629c-8afe-4789-9352-f4cc788cb020] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:41 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008dd740 2 [] false false map[] 0xc0016abb00 0xc00112f340}
Jan 20 12:46:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:41.180962  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:41.483835  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:41.483927  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:41.492047  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[93ec70cd-a1bc-45d4-9ce2-2aac1590a4ab] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:41 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e8580 2 [] false false map[] 0xc001775700 0xc0012fa4d0}
Jan 20 12:46:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:41.492077  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:41.581987  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:46:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:41.589313  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:46:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:41.589376  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:46:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:41.591625  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:46:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:41.593624  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:46:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:41.593848  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:46:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:41.593882  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:46:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:41.593925  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:46:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:42.483887  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:42.483960  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:42.491993  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[b35f4dcd-764e-467d-8d2d-55e59e636159] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:42 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e93e0 2 [] false false map[] 0xc0016abe00 0xc001498fd0}
Jan 20 12:46:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:42.492025  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:42 zcy-Z390-AORUS-MASTER kata[207759]: time="2023-01-20T12:46:42.588631142-05:00" level=error msg="agent pull image err. context deadline exceeded" name=containerd-shim-v2 pid=207759 sandbox=3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af source=virtcontainers subsystem=kata_agent
Jan 20 12:46:42 zcy-Z390-AORUS-MASTER kata[207759]: time="2023-01-20T12:46:42.588814615-05:00" level=error msg="kata runtime PullImage err. context deadline exceeded" name=containerd-shim-v2 pid=207759 sandbox=3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af source=containerd-kata-shim-v2
Jan 20 12:46:42 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:46:42.588631142-05:00" level=error msg="agent pull image err. context deadline exceeded" name=containerd-shim-v2 pid=207759 sandbox=3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af source=virtcontainers subsystem=kata_agent
Jan 20 12:46:42 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:46:42.588814615-05:00" level=error msg="kata runtime PullImage err. context deadline exceeded" name=containerd-shim-v2 pid=207759 sandbox=3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af source=containerd-kata-shim-v2
Jan 20 12:46:42 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:46:42.589366157-05:00" level=error msg="PullImage \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\" failed" error="rpc error: code = DeadlineExceeded desc = context deadline exceeded"
Jan 20 12:46:42 zcy-Z390-AORUS-MASTER kubelet[199956]: E0120 12:46:42.589793  199956 remote_image.go:238] "PullImage from image service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" image="ngcn-registry.sh.intel.com:443/ci-example256m:latest"
Jan 20 12:46:42 zcy-Z390-AORUS-MASTER kubelet[199956]: E0120 12:46:42.589865  199956 kuberuntime_image.go:51] "Failed to pull image" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" image="ngcn-registry.sh.intel.com:443/ci-example256m:latest"
Jan 20 12:46:42 zcy-Z390-AORUS-MASTER kubelet[199956]: E0120 12:46:42.590028  199956 kuberuntime_manager.go:905] container &Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod unsigned-unencrypted-cc-1_default(4f97314d-815b-4787-b174-5c158cd28c9d): ErrImagePull: rpc error: code = DeadlineExceeded desc = context deadline exceeded
Jan 20 12:46:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:42.590088  199956 kubelet.go:1503] "syncPod exit" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d isTerminal=false
Jan 20 12:46:42 zcy-Z390-AORUS-MASTER kubelet[199956]: E0120 12:46:42.590132  199956 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ci-example256m\" with ErrImagePull: \"rpc error: code = DeadlineExceeded desc = context deadline exceeded\"" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:46:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:42.590150  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Warning" reason="Failed" message="Failed to pull image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\": rpc error: code = DeadlineExceeded desc = context deadline exceeded"
Jan 20 12:46:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:42.590181  199956 pod_workers.go:988] "Processing pod event done" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:46:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:42.590211  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Warning" reason="Failed" message="Error: ErrImagePull"
Jan 20 12:46:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:42.689951  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:46:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:42.690017  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:42.691051  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:42 GMT]] 0xc0008ddbe0 2 [] true false map[] 0xc001f37000 <nil>}
Jan 20 12:46:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:42.691153  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:46:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:42.696348  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:46:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:42.696411  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:42.697436  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:42 GMT]] 0xc0012e9720 2 [] true false map[] 0xc001775c00 <nil>}
Jan 20 12:46:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:42.697538  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:46:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:42.709649  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:46:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:42.709689  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:46:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:42.709722  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:42.709751  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:42.710857  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:42 GMT]] 0xc0012e9760 2 [] true false map[] 0xc001f37200 <nil>}
Jan 20 12:46:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:42.710892  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:42 GMT]] 0xc001820260 2 [] true false map[] 0xc0014c7900 <nil>}
Jan 20 12:46:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:42.710962  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:46:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:42.710987  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:46:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:42.891210  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:43.483143  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:43.483206  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:43.495828  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[fbe120d6-b194-4b10-bd36-787ccf8905bb] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:43 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000320d60 2 [] false false map[] 0xc001775e00 0xc0016d7b80}
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:43.495967  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:43.581243  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:43.586887  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:43.586900  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:43.586908  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:43.587434  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:43.587481  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:43.587488  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:43.587496  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER kata[207759]: time="2023-01-20T12:46:43.93644361-05:00" level=warning msg="notify on errors" error="failed to ping agent: Failed to Check if grpc server is working: context deadline exceeded" sandbox=3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af source=virtcontainers subsystem=virtcontainers/monitor
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER kata[207759]: time="2023-01-20T12:46:43.936636929-05:00" level=warning msg="write error to watcher" error="failed to ping agent: Failed to Check if grpc server is working: context deadline exceeded" sandbox=3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af source=virtcontainers subsystem=virtcontainers/monitor
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:46:43.936443610-05:00" level=warning msg="notify on errors" error="failed to ping agent: Failed to Check if grpc server is working: context deadline exceeded" sandbox=3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af source=virtcontainers subsystem=virtcontainers/monitor
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:46:43.936636929-05:00" level=warning msg="write error to watcher" error="failed to ping agent: Failed to Check if grpc server is working: context deadline exceeded" sandbox=3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af source=virtcontainers subsystem=virtcontainers/monitor
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:46:43.936775868-05:00" level=error msg="Wait for process failed" container=3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af error="ttrpc: closed" name=containerd-shim-v2 pid=3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af sandbox=3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af source=containerd-kata-shim-v2
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:46:43.93689928-05:00" level=warning msg="sandbox stopped unexpectedly" error="failed to ping agent: Failed to Check if grpc server is working: context deadline exceeded" name=containerd-shim-v2 pid=207759 sandbox=3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af source=containerd-kata-shim-v2
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER kata[207759]: time="2023-01-20T12:46:43.936775868-05:00" level=error msg="Wait for process failed" container=3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af error="ttrpc: closed" name=containerd-shim-v2 pid=3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af sandbox=3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af source=containerd-kata-shim-v2
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:46:43.937674823-05:00" level=warning msg="notify on errors" error="failed to ping agent: Failed to Check if grpc server is working: Dead agent" sandbox=3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af source=virtcontainers subsystem=virtcontainers/monitor
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:46:43.937907907-05:00" level=warning msg="write error to watcher" error="failed to ping agent: Failed to Check if grpc server is working: Dead agent" sandbox=3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af source=virtcontainers subsystem=virtcontainers/monitor
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER kata[207759]: time="2023-01-20T12:46:43.93689928-05:00" level=warning msg="sandbox stopped unexpectedly" error="failed to ping agent: Failed to Check if grpc server is working: context deadline exceeded" name=containerd-shim-v2 pid=207759 sandbox=3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af source=containerd-kata-shim-v2
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER kata[207759]: time="2023-01-20T12:46:43.937674823-05:00" level=warning msg="notify on errors" error="failed to ping agent: Failed to Check if grpc server is working: Dead agent" sandbox=3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af source=virtcontainers subsystem=virtcontainers/monitor
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:46:43.938879378-05:00" level=warning msg="Agent did not stop sandbox" error="Dead agent" name=containerd-shim-v2 pid=207759 sandbox=3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af sandboxid=3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af source=virtcontainers subsystem=sandbox
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER kata[207759]: time="2023-01-20T12:46:43.937907907-05:00" level=warning msg="write error to watcher" error="failed to ping agent: Failed to Check if grpc server is working: Dead agent" sandbox=3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af source=virtcontainers subsystem=virtcontainers/monitor
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER kata[207759]: time="2023-01-20T12:46:43.938879378-05:00" level=warning msg="Agent did not stop sandbox" error="Dead agent" name=containerd-shim-v2 pid=207759 sandbox=3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af sandboxid=3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af source=virtcontainers subsystem=sandbox
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER kata[207759]: time="2023-01-20T12:46:43.940133245-05:00" level=error msg="Failed to read guest console logs" console-protocol=unix console-url=/run/vc/vm/3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af/console.sock error="read unix @->/run/vc/vm/3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af/console.sock: use of closed network connection" name=containerd-shim-v2 pid=207759 sandbox=3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af source=virtcontainers subsystem=sandbox
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:46:43.940133245-05:00" level=error msg="Failed to read guest console logs" console-protocol=unix console-url=/run/vc/vm/3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af/console.sock error="read unix @->/run/vc/vm/3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af/console.sock: use of closed network connection" name=containerd-shim-v2 pid=207759 sandbox=3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af source=virtcontainers subsystem=sandbox
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER systemd[193112]: run-kata\x2dcontainers-shared-sandboxes-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af-shared-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af\x2d44e89a1158ae25de\x2dresolv.conf.mount: Succeeded.
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER systemd[193112]: run-kata\x2dcontainers-shared-sandboxes-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af-mounts-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af\x2d44e89a1158ae25de\x2dresolv.conf.mount: Succeeded.
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER systemd[193112]: run-kata\x2dcontainers-shared-sandboxes-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af-shared-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af-rootfs.mount: Succeeded.
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER systemd[193112]: run-kata\x2dcontainers-shared-sandboxes-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af-mounts-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af-rootfs.mount: Succeeded.
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER systemd[1]: run-kata\x2dcontainers-shared-sandboxes-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af-shared-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af\x2d44e89a1158ae25de\x2dresolv.conf.mount: Succeeded.
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER systemd[1]: run-kata\x2dcontainers-shared-sandboxes-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af-mounts-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af\x2d44e89a1158ae25de\x2dresolv.conf.mount: Succeeded.
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER systemd[1]: run-kata\x2dcontainers-shared-sandboxes-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af-shared-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af-rootfs.mount: Succeeded.
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER systemd[1]: run-kata\x2dcontainers-shared-sandboxes-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af-mounts-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af-rootfs.mount: Succeeded.
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER systemd[1066]: run-kata\x2dcontainers-shared-sandboxes-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af-shared-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af\x2d44e89a1158ae25de\x2dresolv.conf.mount: Succeeded.
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER systemd[1066]: run-kata\x2dcontainers-shared-sandboxes-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af-mounts-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af\x2d44e89a1158ae25de\x2dresolv.conf.mount: Succeeded.
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER systemd[1066]: run-kata\x2dcontainers-shared-sandboxes-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af-shared-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af-rootfs.mount: Succeeded.
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER systemd[1066]: run-kata\x2dcontainers-shared-sandboxes-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af-mounts-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af-rootfs.mount: Succeeded.
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:43.973985  199956 manager.go:1044] Destroyed container: "/kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af" (aliases: [3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af /kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af], namespace: "containerd")
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:43.974014  199956 handler.go:325] Added event &{/kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af 2023-01-20 12:46:43.974009568 -0500 EST m=+916.529784393 containerDeletion {<nil>}}
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:43.975621  199956 manager.go:1044] Destroyed container: "/kata_overhead/3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af" (aliases: [3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af /kata_overhead/3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af], namespace: "containerd")
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:43.975634  199956 handler.go:325] Added event &{/kata_overhead/3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af 2023-01-20 12:46:43.975631716 -0500 EST m=+916.531406541 containerDeletion {<nil>}}
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER systemd[193112]: run-kata\x2dcontainers-shared-sandboxes-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af-shared.mount: Succeeded.
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER systemd[193112]: run-containerd-io.containerd.runtime.v2.task-k8s.io-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af-rootfs.mount: Succeeded.
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER systemd[1066]: run-kata\x2dcontainers-shared-sandboxes-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af-shared.mount: Succeeded.
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER systemd[1066]: run-containerd-io.containerd.runtime.v2.task-k8s.io-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af-rootfs.mount: Succeeded.
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER systemd[1]: run-kata\x2dcontainers-shared-sandboxes-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af-shared.mount: Succeeded.
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER systemd[1]: run-containerd-io.containerd.runtime.v2.task-k8s.io-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af-rootfs.mount: Succeeded.
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER kata[207759]: time="2023-01-20T12:46:43.979738806-05:00" level=error msg="failed to cleanup the &{%!s(*cgroups.cgroup=&{0x56532db55980 [0xc000295ce0 0xc0002129b0 0xc0002129c0 0xc000212a50 0xc000212a60 0xc000212a70 0xc000212aa0 0xc000212ad0 0xc000212b40 0xc0000129a8 0xc000295d00 0xc000212b50 0xc000212b80 0xc0001c3d10] {0 0} <nil>}) /kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af %!s(*specs.LinuxCPU=&{<nil> <nil> <nil> <nil> <nil>  }) [{%!s(bool=false)  %!s(*int64=<nil>) %!s(*int64=<nil>) rwm} {%!s(bool=true) c %!s(*int64=0xc000045e60) %!s(*int64=0xc000045e68) rwm} {%!s(bool=true) c %!s(*int64=0xc000045e70) %!s(*int64=0xc000045e78) rwm} {%!s(bool=true) c %!s(*int64=0xc0001c3ad8) %!s(*int64=0xc0001c3ae0) rwm} {%!s(bool=true) c %!s(*int64=0xc0001c3b08) %!s(*int64=0xc0001c3b10) rwm} {%!s(bool=true) c %!s(*int64=0xc0001c3b38) %!s(*int64=0xc0001c3b40) rwm} {%!s(bool=true) c %!s(*int64=0xc0001c3b68) %!s(*int64=0xc0001c3b70) rwm} {%!s(bool=true) c %!s(*int64=0xc0001c3b98) %!s(*int64=0xc0001c3ba0) rwm} {%!s(bool=true) c %!s(*int64=0xc0001c3bc8) %!s(*int64=0xc0001c3bd0) rwm} {%!s(bool=true) c %!s(*int64=0xc0001c3bf8) %!s(*int64=0xc0001c3c00) rwm} {%!s(bool=true) c %!s(*int64=0xc0001c3c28) %!s(*int64=0xc0001c3c30) rwm} {%!s(bool=true) c %!s(*int64=0xc0001c3c58) %!s(*int64=0xc0001c3c60) rwm} {%!s(bool=true) c %!s(*int64=0xc0001c3c88) %!s(*int64=0xc0001c3c90) rwm} {%!s(bool=true) c %!s(*int64=0xc0001c3cb8) %!s(*int64=0xc0001c3cc0) rwm} {%!s(bool=true) c %!s(*int64=0xc000045f98) %!s(*int64=0xc000045fa0) m} {%!s(bool=true) b %!s(*int64=0xc000045f98) %!s(*int64=0xc000045fa0) m} {%!s(bool=true) c %!s(*int64=0xc000045fa8) %!s(*int64=0xc000045fa0) rwm} {%!s(bool=true) c %!s(*int64=0xc000045fb0) %!s(*int64=0xc000045fb8) rwm}] {%!s(int32=0) %!s(uint32=0)}} resource controllers" error="cgroups: cgroup deleted" name=containerd-shim-v2 pid=207759 sandbox=3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af source=virtcontainers subsystem=sandbox
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER kata[207759]: time="2023-01-20T12:46:43.980045926-05:00" level=warning msg="Calling Cleanup() on an already cleaned up filesystem" name=containerd-shim-v2 pid=207759 sandbox=3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af source=virtcontainers subsystem="filesystem share"
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:46:43.979738806-05:00" level=error msg="failed to cleanup the &{%!s(*cgroups.cgroup=&{0x56532db55980 [0xc000295ce0 0xc0002129b0 0xc0002129c0 0xc000212a50 0xc000212a60 0xc000212a70 0xc000212aa0 0xc000212ad0 0xc000212b40 0xc0000129a8 0xc000295d00 0xc000212b50 0xc000212b80 0xc0001c3d10] {0 0} <nil>}) /kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af %!s(*specs.LinuxCPU=&{<nil> <nil> <nil> <nil> <nil>  }) [{%!s(bool=false)  %!s(*int64=<nil>) %!s(*int64=<nil>) rwm} {%!s(bool=true) c %!s(*int64=0xc000045e60) %!s(*int64=0xc000045e68) rwm} {%!s(bool=true) c %!s(*int64=0xc000045e70) %!s(*int64=0xc000045e78) rwm} {%!s(bool=true) c %!s(*int64=0xc0001c3ad8) %!s(*int64=0xc0001c3ae0) rwm} {%!s(bool=true) c %!s(*int64=0xc0001c3b08) %!s(*int64=0xc0001c3b10) rwm} {%!s(bool=true) c %!s(*int64=0xc0001c3b38) %!s(*int64=0xc0001c3b40) rwm} {%!s(bool=true) c %!s(*int64=0xc0001c3b68) %!s(*int64=0xc0001c3b70) rwm} {%!s(bool=true) c %!s(*int64=0xc0001c3b98) %!s(*int64=0xc0001c3ba0) rwm} {%!s(bool=true) c %!s(*int64=0xc0001c3bc8) %!s(*int64=0xc0001c3bd0) rwm} {%!s(bool=true) c %!s(*int64=0xc0001c3bf8) %!s(*int64=0xc0001c3c00) rwm} {%!s(bool=true) c %!s(*int64=0xc0001c3c28) %!s(*int64=0xc0001c3c30) rwm} {%!s(bool=true) c %!s(*int64=0xc0001c3c58) %!s(*int64=0xc0001c3c60) rwm} {%!s(bool=true) c %!s(*int64=0xc0001c3c88) %!s(*int64=0xc0001c3c90) rwm} {%!s(bool=true) c %!s(*int64=0xc0001c3cb8) %!s(*int64=0xc0001c3cc0) rwm} {%!s(bool=true) c %!s(*int64=0xc000045f98) %!s(*int64=0xc000045fa0) m} {%!s(bool=true) b %!s(*int64=0xc000045f98) %!s(*int64=0xc000045fa0) m} {%!s(bool=true) c %!s(*int64=0xc000045fa8) %!s(*int64=0xc000045fa0) rwm} {%!s(bool=true) c %!s(*int64=0xc000045fb0) %!s(*int64=0xc000045fb8) rwm}] {%!s(int32=0) %!s(uint32=0)}} resource controllers" error="cgroups: cgroup deleted" name=containerd-shim-v2 pid=207759 sandbox=3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af source=virtcontainers subsystem=sandbox
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:46:43.980045926-05:00" level=warning msg="Calling Cleanup() on an already cleaned up filesystem" name=containerd-shim-v2 pid=207759 sandbox=3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af source=virtcontainers subsystem="filesystem share"
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:46:43.982430440-05:00" level=error msg="error receiving message" error="read unix /run/containerd/containerd.sock.ttrpc->@: read: connection reset by peer"
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:46:43.982459097-05:00" level=info msg="shim disconnected" id=3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:46:43.982482255-05:00" level=warning msg="cleaning up after shim disconnected" id=3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af namespace=k8s.io
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:46:43.982490226-05:00" level=info msg="cleaning up dead shim"
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:46:43.993335462-05:00" level=error msg="failed to delete" cmd="/usr/local/bin/containerd-shim-kata-qemu-v2 -namespace k8s.io -address /run/containerd/containerd.sock -publish-binary /opt/confidential-containers/bin/containerd -id 3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af -bundle /run/containerd/io.containerd.runtime.v2.task/k8s.io/3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af delete" error="exit status 1"
Jan 20 12:46:43 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:46:43.993369931-05:00" level=warning msg="failed to clean up after shim disconnected" error="time=\"2023-01-20T12:46:43-05:00\" level=warning msg=\"failed to cleanup container\" container=3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af error=\"open /run/vc/sbs/3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af: no such file or directory\" name=containerd-shim-v2 pid=207973 sandbox=3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af source=containerd-kata-shim-v2\nio.containerd.kata.v2: open /run/vc/sbs/3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af: no such file or directory: exit status 1" id=3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af namespace=k8s.io
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:44.167442  199956 generic.go:155] "GenericPLEG" podUID=4f97314d-815b-4787-b174-5c158cd28c9d containerID="3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af" oldState=running newState=exited
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:44.168249  199956 kuberuntime_manager.go:1037] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af] pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:44.169841  199956 generic.go:421] "PLEG: Write status" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:44.169924  199956 kubelet.go:2098] "SyncLoop (PLEG): event for pod" pod="default/unsigned-unencrypted-cc-1" event=&{ID:4f97314d-815b-4787-b174-5c158cd28c9d Type:ContainerDied Data:3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af}
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:44.169963  199956 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af"
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:44.170006  199956 pod_workers.go:888] "Processing pod event" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:44.170033  199956 kubelet.go:1501] "syncPod enter" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:44.170066  199956 kubelet_pods.go:1435] "Generating pod status" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:44.170135  199956 kubelet_pods.go:1447] "Got phase for pod" pod="default/unsigned-unencrypted-cc-1" oldPhase=Pending phase=Pending
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:44.170586  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:44.170657  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:44.170690  199956 kuberuntime_manager.go:488] "No ready sandbox for pod can be found. Need to start a new one" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:44.170732  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:true CreateSandbox:true SandboxID:3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af Attempt:4 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:44.170780  199956 kuberuntime_manager.go:730] "Stopping PodSandbox for pod, will start new one" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:44.170938  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="" kind="Pod" apiVersion="v1" type="Normal" reason="SandboxChanged" message="Pod sandbox changed, it will be killed and re-created."
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:46:44.171197181-05:00" level=info msg="StopPodSandbox for \"3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af\""
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER systemd[193112]: run-containerd-io.containerd.grpc.v1.cri-sandboxes-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af-shm.mount: Succeeded.
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER systemd[1]: run-containerd-io.containerd.grpc.v1.cri-sandboxes-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af-shm.mount: Succeeded.
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER systemd[1066]: run-containerd-io.containerd.grpc.v1.cri-sandboxes-3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af-shm.mount: Succeeded.
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER avahi-daemon[896]: Interface vethad7b1431.IPv6 no longer relevant for mDNS.
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER avahi-daemon[896]: Leaving mDNS multicast group on interface vethad7b1431.IPv6 with address fe80::881e:54ff:fecc:1fb9.
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:44.195955  199956 config.go:279] "Setting pods for source" source="api"
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:44.196554  199956 status_manager.go:685] "Patch status for pod" pod="default/unsigned-unencrypted-cc-1" patch="{\"metadata\":{\"uid\":\"4f97314d-815b-4787-b174-5c158cd28c9d\"},\"status\":{\"containerStatuses\":[{\"image\":\"ngcn-registry.sh.intel.com:443/ci-example256m:latest\",\"imageID\":\"\",\"lastState\":{},\"name\":\"ci-example256m\",\"ready\":false,\"restartCount\":0,\"started\":false,\"state\":{\"waiting\":{\"message\":\"rpc error: code = DeadlineExceeded desc = context deadline exceeded\",\"reason\":\"ErrImagePull\"}}}]}}"
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kernel: cni0: port 6(vethad7b1431) entered disabled state
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:44.196715  199956 status_manager.go:694] "Status for pod updated successfully" pod="default/unsigned-unencrypted-cc-1" statusVersion=8 status={Phase:Pending Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.15 PodIPs:[{IP:10.244.0.15}] StartTime:2023-01-20 12:41:08 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:ci-example256m State:{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = DeadlineExceeded desc = context deadline exceeded,} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest ImageID: ContainerID: Started:0xc00101fb3e}] QOSClass:Guaranteed EphemeralContainerStatuses:[]}
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:44.197477  199956 kubelet.go:2073] "SyncLoop RECONCILE" source="api" pods=[default/unsigned-unencrypted-cc-1]
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kernel: device vethad7b1431 left promiscuous mode
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kernel: cni0: port 6(vethad7b1431) entered disabled state
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:44.257407  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="default/unsigned-unencrypted-cc-1" volumeName="kube-api-access-qcb8g" volumeSpecName="kube-api-access-qcb8g"
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER avahi-daemon[896]: Withdrawing address record for fe80::881e:54ff:fecc:1fb9 on vethad7b1431.
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER NetworkManager[905]: <info>  [1674236804.2690] device (vethad7b1431): released from master device cni0
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER gnome-shell[1450]: Removing a network device that was not added
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:44.292653  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") Volume is already mounted to pod, but remount was requested." pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:44.292880  199956 projected.go:183] Setting up volume kube-api-access-qcb8g for pod 4f97314d-815b-4787-b174-5c158cd28c9d at /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:44.293304  199956 atomic_writer.go:161] pod default/unsigned-unencrypted-cc-1 volume kube-api-access-qcb8g: no update required for target directory /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:44.293382  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") " pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER systemd[1]: run-netns-cni\x2de72bc154\x2d1f55\x2d300e\x2d4d28\x2dd31fbfa0e95f.mount: Succeeded.
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER systemd[1066]: run-netns-cni\x2de72bc154\x2d1f55\x2d300e\x2d4d28\x2dd31fbfa0e95f.mount: Succeeded.
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER systemd[193112]: run-netns-cni\x2de72bc154\x2d1f55\x2d300e\x2d4d28\x2dd31fbfa0e95f.mount: Succeeded.
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:46:44.309287994-05:00" level=info msg="TearDown network for sandbox \"3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af\" successfully"
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:46:44.309373947-05:00" level=info msg="StopPodSandbox for \"3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af\" returns successfully"
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:44.309811  199956 kuberuntime_manager.go:785] "Creating PodSandbox for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:44.310067  199956 kuberuntime_sandbox.go:63] "Running pod with runtime handler" pod="default/unsigned-unencrypted-cc-1" runtimeHandler="kata-qemu"
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:46:44.310636985-05:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:unsigned-unencrypted-cc-1,Uid:4f97314d-815b-4787-b174-5c158cd28c9d,Namespace:default,Attempt:4,}"
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER systemd-udevd[208016]: ethtool: autonegotiation is unset or enabled, the speed and duplex are not writable.
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER systemd-udevd[208016]: Using default interface naming scheme 'v245'.
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER systemd-udevd[208016]: vetha66295d7: Could not generate persistent MAC: No data available
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kernel: cni0: port 6(vetha66295d7) entered blocking state
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kernel: cni0: port 6(vetha66295d7) entered disabled state
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER NetworkManager[905]: <info>  [1674236804.3288] manager: (vetha66295d7): new Veth device (/org/freedesktop/NetworkManager/Devices/126)
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kernel: device vetha66295d7 entered promiscuous mode
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kernel: cni0: port 6(vetha66295d7) entered blocking state
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kernel: cni0: port 6(vetha66295d7) entered forwarding state
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kernel: IPv6: ADDRCONF(NETDEV_CHANGE): vetha66295d7: link becomes ready
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER NetworkManager[905]: <info>  [1674236804.3374] device (vetha66295d7): carrier: link connected
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER containerd[201983]: map[string]interface {}{"cniVersion":"0.3.1", "hairpinMode":true, "ipMasq":false, "ipam":map[string]interface {}{"ranges":[][]map[string]interface {}{[]map[string]interface {}{map[string]interface {}{"subnet":"10.244.0.0/24"}}}, "routes":[]types.Route{types.Route{Dst:net.IPNet{IP:net.IP{0xa, 0xf4, 0x0, 0x0}, Mask:net.IPMask{0xff, 0xff, 0x0, 0x0}}, GW:net.IP(nil)}}, "type":"host-local"}, "isDefaultGateway":true, "isGateway":true, "mtu":(*uint)(0xc0000b48e8), "name":"cbr0", "type":"bridge"}
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:44.398411  199956 factory.go:258] Using factory "containerd" for container "/kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_08841bf63919f9c8acff77c51620bd8e684c550feb6b3e8784239bc3d9079457"
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kernel: eth0: Caught tx_queue_len zero misconfig
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:44.484131  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:44.484207  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:44.497286  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[7984a7b1-98bf-4e4a-9ed2-e707c4b0d979] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:44 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e9c60 2 [] false false map[] 0xc000338a00 0xc001c96b00}
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:44.497417  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER virtiofsd[208088]: zcy-Z390-AORUS-MASTER virtiofsd[208088]: Use of deprecated flag '-f': This flag has no effect, please remove it
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER virtiofsd[208088]: zcy-Z390-AORUS-MASTER virtiofsd[208088]: Use of deprecated option format '-o': Please specify options without it (e.g., '--cache auto' instead of '-o cache=auto')
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER virtiofsd[208090]: zcy-Z390-AORUS-MASTER virtiofsd[208088]: Waiting for vhost-user socket connection...
Jan 20 12:46:44 zcy-Z390-AORUS-MASTER virtiofsd[208090]: zcy-Z390-AORUS-MASTER virtiofsd[208088]: Client connected, servicing requests
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.090215  199956 manager.go:988] Added container: "/kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_08841bf63919f9c8acff77c51620bd8e684c550feb6b3e8784239bc3d9079457" (aliases: [08841bf63919f9c8acff77c51620bd8e684c550feb6b3e8784239bc3d9079457 /kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_08841bf63919f9c8acff77c51620bd8e684c550feb6b3e8784239bc3d9079457], namespace: "containerd")
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.090690  199956 handler.go:325] Added event &{/kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_08841bf63919f9c8acff77c51620bd8e684c550feb6b3e8784239bc3d9079457 2023-01-20 12:46:44.393881513 -0500 EST containerCreation {<nil>}}
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.090855  199956 container.go:530] Start housekeeping for container "/kubepods/pod4f97314d-815b-4787-b174-5c158cd28c9d/kata_08841bf63919f9c8acff77c51620bd8e684c550feb6b3e8784239bc3d9079457"
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.091734  199956 factory.go:258] Using factory "containerd" for container "/kata_overhead/08841bf63919f9c8acff77c51620bd8e684c550feb6b3e8784239bc3d9079457"
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER containerd[201983]: {"cniVersion":"0.3.1","hairpinMode":true,"ipMasq":false,"ipam":{"ranges":[[{"subnet":"10.244.0.0/24"}]],"routes":[{"dst":"10.244.0.0/16"}],"type":"host-local"},"isDefaultGateway":true,"isGateway":true,"mtu":1450,"name":"cbr0","type":"bridge"}time="2023-01-20T12:46:45.092533480-05:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:unsigned-unencrypted-cc-1,Uid:4f97314d-815b-4787-b174-5c158cd28c9d,Namespace:default,Attempt:4,} returns sandbox id \"08841bf63919f9c8acff77c51620bd8e684c550feb6b3e8784239bc3d9079457\""
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.093003  199956 kuberuntime_manager.go:823] "Created PodSandbox for pod" podSandboxID="08841bf63919f9c8acff77c51620bd8e684c550feb6b3e8784239bc3d9079457" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.093789  199956 kuberuntime_manager.go:846] "Determined the ip for pod after sandbox changed" IPs=[10.244.0.16] pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.094128  199956 kuberuntime_manager.go:889] "Creating container in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.094908  199956 manager.go:988] Added container: "/kata_overhead/08841bf63919f9c8acff77c51620bd8e684c550feb6b3e8784239bc3d9079457" (aliases: [08841bf63919f9c8acff77c51620bd8e684c550feb6b3e8784239bc3d9079457 /kata_overhead/08841bf63919f9c8acff77c51620bd8e684c550feb6b3e8784239bc3d9079457], namespace: "containerd")
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.095018  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Normal" reason="BackOff" message="Back-off pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\""
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.095025  199956 kuberuntime_manager.go:903] "Container start failed in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1" containerMessage="Back-off pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\"" err="ImagePullBackOff"
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.095063  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Warning" reason="Failed" message="Error: ImagePullBackOff"
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.095100  199956 kubelet.go:1503] "syncPod exit" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d isTerminal=false
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: E0120 12:46:45.095141  199956 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ci-example256m\" with ImagePullBackOff: \"Back-off pulling image \\\"ngcn-registry.sh.intel.com:443/ci-example256m:latest\\\"\"" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.095188  199956 pod_workers.go:988] "Processing pod event done" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.095376  199956 handler.go:325] Added event &{/kata_overhead/08841bf63919f9c8acff77c51620bd8e684c550feb6b3e8784239bc3d9079457 2023-01-20 12:46:44.429881514 -0500 EST containerCreation {<nil>}}
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.095452  199956 container.go:530] Start housekeeping for container "/kata_overhead/08841bf63919f9c8acff77c51620bd8e684c550feb6b3e8784239bc3d9079457"
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.172613  199956 generic.go:155] "GenericPLEG" podUID=4f97314d-815b-4787-b174-5c158cd28c9d containerID="08841bf63919f9c8acff77c51620bd8e684c550feb6b3e8784239bc3d9079457" oldState=non-existent newState=running
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.173493  199956 kuberuntime_manager.go:1037] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[08841bf63919f9c8acff77c51620bd8e684c550feb6b3e8784239bc3d9079457 3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af] pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.175535  199956 generic.go:421] "PLEG: Write status" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.175634  199956 kubelet.go:2098] "SyncLoop (PLEG): event for pod" pod="default/unsigned-unencrypted-cc-1" event=&{ID:4f97314d-815b-4787-b174-5c158cd28c9d Type:ContainerStarted Data:08841bf63919f9c8acff77c51620bd8e684c550feb6b3e8784239bc3d9079457}
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.175697  199956 pod_workers.go:888] "Processing pod event" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.175731  199956 kubelet.go:1501] "syncPod enter" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.175761  199956 kubelet_pods.go:1435] "Generating pod status" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.175831  199956 kubelet_pods.go:1447] "Got phase for pod" pod="default/unsigned-unencrypted-cc-1" oldPhase=Pending phase=Pending
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.176246  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.176314  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.176360  199956 kuberuntime_manager.go:638] "Container of pod is not in the desired state and shall be started" containerName="ci-example256m" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.176402  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:08841bf63919f9c8acff77c51620bd8e684c550feb6b3e8784239bc3d9079457 Attempt:4 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.176680  199956 kuberuntime_manager.go:889] "Creating container in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.177785  199956 kuberuntime_manager.go:903] "Container start failed in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1" containerMessage="Back-off pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\"" err="ImagePullBackOff"
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.177793  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Normal" reason="BackOff" message="Back-off pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\""
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.177862  199956 kubelet.go:1503] "syncPod exit" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d isTerminal=false
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.177871  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Warning" reason="Failed" message="Error: ImagePullBackOff"
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: E0120 12:46:45.177911  199956 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ci-example256m\" with ImagePullBackOff: \"Back-off pulling image \\\"ngcn-registry.sh.intel.com:443/ci-example256m:latest\\\"\"" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.177961  199956 pod_workers.go:988] "Processing pod event done" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.197029  199956 config.go:279] "Setting pods for source" source="api"
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.197051  199956 status_manager.go:685] "Patch status for pod" pod="default/unsigned-unencrypted-cc-1" patch="{\"metadata\":{\"uid\":\"4f97314d-815b-4787-b174-5c158cd28c9d\"},\"status\":{\"$setElementOrder/podIPs\":[{\"ip\":\"10.244.0.16\"}],\"containerStatuses\":[{\"image\":\"ngcn-registry.sh.intel.com:443/ci-example256m:latest\",\"imageID\":\"\",\"lastState\":{},\"name\":\"ci-example256m\",\"ready\":false,\"restartCount\":0,\"started\":false,\"state\":{\"waiting\":{\"message\":\"Back-off pulling image \\\"ngcn-registry.sh.intel.com:443/ci-example256m:latest\\\"\",\"reason\":\"ImagePullBackOff\"}}}],\"podIP\":\"10.244.0.16\",\"podIPs\":[{\"ip\":\"10.244.0.16\"},{\"$patch\":\"delete\",\"ip\":\"10.244.0.15\"}]}}"
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.197175  199956 status_manager.go:694] "Status for pod updated successfully" pod="default/unsigned-unencrypted-cc-1" statusVersion=9 status={Phase:Pending Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.16 PodIPs:[{IP:10.244.0.16}] StartTime:2023-01-20 12:41:08 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:ci-example256m State:{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "ngcn-registry.sh.intel.com:443/ci-example256m:latest",} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest ImageID: ContainerID: Started:0xc001752a2e}] QOSClass:Guaranteed EphemeralContainerStatuses:[]}
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.197991  199956 kubelet.go:2073] "SyncLoop RECONCILE" source="api" pods=[default/unsigned-unencrypted-cc-1]
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.264578  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="default/unsigned-unencrypted-cc-1" volumeName="kube-api-access-qcb8g" volumeSpecName="kube-api-access-qcb8g"
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.297678  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") Volume is already mounted to pod, but remount was requested." pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.297854  199956 projected.go:183] Setting up volume kube-api-access-qcb8g for pod 4f97314d-815b-4787-b174-5c158cd28c9d at /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.298283  199956 atomic_writer.go:161] pod default/unsigned-unencrypted-cc-1 volume kube-api-access-qcb8g: no update required for target directory /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.298354  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") " pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER avahi-daemon[896]: Joining mDNS multicast group on interface vetha66295d7.IPv6 with address fe80::8cb4:63ff:fe1d:9fbc.
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER avahi-daemon[896]: New relevant interface vetha66295d7.IPv6 for mDNS.
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER avahi-daemon[896]: Registering new address record for fe80::8cb4:63ff:fe1d:9fbc on vetha66295d7.*.
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.484107  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.484172  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.492018  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[df120437-4a91-49ac-be4c-d7abba3ec58e] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:45 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00013c1a0 2 [] false false map[] 0xc000339200 0xc001559d90}
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.492049  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.581921  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.587842  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.587855  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.588633  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.590022  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.590084  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.590091  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:46:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:45.590100  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.164807  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.164881  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.172444  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:46 GMT] X-Content-Type-Options:[nosniff]] 0xc0017189e0 2 [] false false map[] 0xc0000dd900 0xc0006a3c30}
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.172471  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.271551  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.271615  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.272867  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:46 GMT]] 0xc00129c120 29 [] true false map[] 0xc001a4a100 <nil>}
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.272967  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.483792  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.483857  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.492134  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[e1920ec3-6257-4121-818e-e373d514eda2] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:46 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00129c140 2 [] false false map[] 0xc001a4a500 0xc000b1e6e0}
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.492164  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.582056  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=2 pods=[kube-flannel/kube-flannel-ds-hprn4 kube-system/kube-apiserver-zcy-z390-aorus-master]
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.582136  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd updateType=0
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.582195  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.582217  199956 pod_workers.go:888] "Processing pod event" pod="kube-flannel/kube-flannel-ds-hprn4" podUID=04e472cb-b6f9-4065-b509-d7e1579fc48d updateType=0
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.582231  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/kube-apiserver-zcy-z390-aorus-master"
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.582301  199956 kubelet.go:1501] "syncPod enter" pod="kube-flannel/kube-flannel-ds-hprn4" podUID=04e472cb-b6f9-4065-b509-d7e1579fc48d
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.582334  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.582336  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" oldPhase=Running phase=Running
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.582447  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-flannel/kube-flannel-ds-hprn4" oldPhase=Running phase=Running
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.582624  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:28 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:41 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:41 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:28 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.239.159.53 PodIPs:[{IP:10.239.159.53}] StartTime:2023-01-20 12:31:28 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:kube-apiserver State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:22 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:6 Image:k8s.gcr.io/kube-apiserver:v1.24.0 ImageID:k8s.gcr.io/kube-apiserver@sha256:a04522b882e919de6141b47d72393fb01226c78e7388400f966198222558c955 ContainerID:containerd://5900ac5c1383a321a6ae837b57d6d29d7a660a102c0806210a9ea57290673062 Started:0xc001201e19}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.582942  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-apiserver-zcy-z390-aorus-master"
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.583011  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/kube-apiserver-zcy-z390-aorus-master"
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.583216  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-flannel/kube-flannel-ds-hprn4" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:44 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:45 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:45 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.239.159.53 PodIPs:[{IP:10.239.159.53}] StartTime:2023-01-20 12:31:42 -0500 EST InitContainerStatuses:[{Name:install-cni-plugin State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2023-01-20 12:31:43 -0500 EST,FinishedAt:2023-01-20 12:31:43 -0500 EST,ContainerID:containerd://349eee1bf5a2d95206979822c956ea6f555dbb786434c7b9fe46524872f1a18d,}} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:docker.io/rancher/mirrored-flannelcni-flannel-cni-plugin:v1.1.0 ImageID:docker.io/rancher/mirrored-flannelcni-flannel-cni-plugin@sha256:28d3a6be9f450282bf42e4dad143d41da23e3d91f66f19c01ee7fd21fd17cb2b ContainerID:containerd://349eee1bf5a2d95206979822c956ea6f555dbb786434c7b9fe46524872f1a18d Started:<nil>} {Name:install-cni State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2023-01-20 12:31:43 -0500 EST,FinishedAt:2023-01-20 12:31:43 -0500 EST,ContainerID:containerd://85a4ede91a47b8d7df192dc39004a2a7fa45e095191a7335eaed8ec826cd9f36,}} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:docker.io/rancher/mirrored-flannelcni-flannel:v0.19.1 ImageID:docker.io/rancher/mirrored-flannelcni-flannel@sha256:e3b0f06121b0f7a11a684e910bba89b3ab49cd6e73aeb6c719f83b2456946366 ContainerID:containerd://85a4ede91a47b8d7df192dc39004a2a7fa45e095191a7335eaed8ec826cd9f36 Started:<nil>}] ContainerStatuses:[{Name:kube-flannel State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:44 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:docker.io/rancher/mirrored-flannelcni-flannel:v0.19.1 ImageID:docker.io/rancher/mirrored-flannelcni-flannel@sha256:e3b0f06121b0f7a11a684e910bba89b3ab49cd6e73aeb6c719f83b2456946366 ContainerID:containerd://0f9ae677fe935afcf43e145281deae0d663ba95fda6759ff24f0dd3e1cee17a9 Started:0xc001a4040e}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.583553  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.583635  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.584057  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.584122  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/kube-apiserver-zcy-z390-aorus-master"
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.584231  199956 kubelet.go:1503] "syncPod exit" pod="kube-flannel/kube-flannel-ds-hprn4" podUID=04e472cb-b6f9-4065-b509-d7e1579fc48d isTerminal=false
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.584273  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd isTerminal=false
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.584286  199956 pod_workers.go:988] "Processing pod event done" pod="kube-flannel/kube-flannel-ds-hprn4" podUID=04e472cb-b6f9-4065-b509-d7e1579fc48d updateType=0
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.584326  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd updateType=0
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.674873  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="run" volumeSpecName="run"
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.674977  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="cni-plugin" volumeSpecName="cni-plugin"
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.675047  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="cni" volumeSpecName="cni"
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.675125  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="flannel-cfg" volumeSpecName="flannel-cfg"
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.675193  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="xtables-lock" volumeSpecName="xtables-lock"
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.675307  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="kube-api-access-hqj8d" volumeSpecName="kube-api-access-hqj8d"
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.675417  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" volumeName="ca-certs" volumeSpecName="ca-certs"
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.675481  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.675544  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" volumeName="etc-pki" volumeSpecName="etc-pki"
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.675607  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.675670  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.675734  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.708091  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"flannel-cfg\" (UniqueName: \"kubernetes.io/configmap/04e472cb-b6f9-4065-b509-d7e1579fc48d-flannel-cfg\") pod \"kube-flannel-ds-hprn4\" (UID: \"04e472cb-b6f9-4065-b509-d7e1579fc48d\") Volume is already mounted to pod, but remount was requested." pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.708218  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-hqj8d\" (UniqueName: \"kubernetes.io/projected/04e472cb-b6f9-4065-b509-d7e1579fc48d-kube-api-access-hqj8d\") pod \"kube-flannel-ds-hprn4\" (UID: \"04e472cb-b6f9-4065-b509-d7e1579fc48d\") Volume is already mounted to pod, but remount was requested." pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.708352  199956 projected.go:183] Setting up volume kube-api-access-hqj8d for pod 04e472cb-b6f9-4065-b509-d7e1579fc48d at /var/lib/kubelet/pods/04e472cb-b6f9-4065-b509-d7e1579fc48d/volumes/kubernetes.io~projected/kube-api-access-hqj8d
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.708407  199956 configmap.go:181] Setting up volume flannel-cfg for pod 04e472cb-b6f9-4065-b509-d7e1579fc48d at /var/lib/kubelet/pods/04e472cb-b6f9-4065-b509-d7e1579fc48d/volumes/kubernetes.io~configmap/flannel-cfg
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.708486  199956 configmap.go:205] Received configMap kube-flannel/kube-flannel-cfg containing (2) pieces of data, 365 total bytes
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.708558  199956 quota_linux.go:271] SupportsQuotas called, but quotas disabled
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.708798  199956 atomic_writer.go:161] pod kube-flannel/kube-flannel-ds-hprn4 volume kube-api-access-hqj8d: no update required for target directory /var/lib/kubelet/pods/04e472cb-b6f9-4065-b509-d7e1579fc48d/volumes/kubernetes.io~projected/kube-api-access-hqj8d
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.708864  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-hqj8d\" (UniqueName: \"kubernetes.io/projected/04e472cb-b6f9-4065-b509-d7e1579fc48d-kube-api-access-hqj8d\") pod \"kube-flannel-ds-hprn4\" (UID: \"04e472cb-b6f9-4065-b509-d7e1579fc48d\") " pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.708880  199956 atomic_writer.go:161] pod kube-flannel/kube-flannel-ds-hprn4 volume flannel-cfg: no update required for target directory /var/lib/kubelet/pods/04e472cb-b6f9-4065-b509-d7e1579fc48d/volumes/kubernetes.io~configmap/flannel-cfg
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.708951  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"flannel-cfg\" (UniqueName: \"kubernetes.io/configmap/04e472cb-b6f9-4065-b509-d7e1579fc48d-flannel-cfg\") pod \"kube-flannel-ds-hprn4\" (UID: \"04e472cb-b6f9-4065-b509-d7e1579fc48d\") " pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.895498  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.895562  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.902603  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:46 GMT] X-Content-Type-Options:[nosniff]] 0xc00129d060 2 [] false false map[] 0xc000fcc700 0xc0013b0210}
Jan 20 12:46:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:46.902653  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:46:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:47.004903  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:46:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:47.004910  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:46:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:47.005099  199956 interface.go:209] Interface eno2 is up
Jan 20 12:46:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:47.005140  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:46:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:47.005161  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:46:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:47.005166  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:46:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:47.005186  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:46:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:47.005189  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:46:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:47.005617  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:46:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:47.005625  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:46:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:47.005628  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:46:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:47.005633  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:46:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:47.483791  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:47.483859  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:47.492284  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[eb2d93d3-a0fd-4f27-b790-7ea71f7f6d15] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:47 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008e29a0 2 [] false false map[] 0xc00105a800 0xc001167ce0}
Jan 20 12:46:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:47.492316  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:47.525860  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/etcd.yaml"
Jan 20 12:46:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:47.527971  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver.yaml"
Jan 20 12:46:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:47.530954  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Jan 20 12:46:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:47.531953  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-scheduler.yaml"
Jan 20 12:46:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:47.532235  199956 config.go:279] "Setting pods for source" source="file"
Jan 20 12:46:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:47.581415  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:46:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:47.586921  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:46:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:47.586933  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:46:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:47.586940  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:46:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:47.587683  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:46:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:47.587729  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:46:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:47.587736  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:46:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:47.587744  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:46:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:47.892417  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:46:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:48.363226  199956 reflector.go:536] object-"kube-system"/"coredns": Watch close - *v1.ConfigMap total 0 items received
Jan 20 12:46:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:48.483496  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:48.483570  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:48.491285  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[f8d312b3-8fba-4545-8f64-ab1f96aa391e] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:48 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0020fde80 2 [] false false map[] 0xc0021a2c00 0xc0015a7b80}
Jan 20 12:46:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:48.491313  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:48.879125  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:46:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:48.879149  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:48.879576  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:48 GMT] X-Content-Type-Options:[nosniff]] 0xc001300520 2 [] true false map[] 0xc001c0ac00 <nil>}
Jan 20 12:46:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:48.879609  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:46:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:49.483810  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:49.483875  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:49.492320  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[244bb6a4-254a-4e18-a952-d5149fa32505] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:49 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0009790a0 2 [] false false map[] 0xc0021a2f00 0xc0015d5c30}
Jan 20 12:46:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:49.492350  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:49.581908  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:46:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:49.583466  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:46:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:49.583482  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:46:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:49.583994  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:46:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:49.584353  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:46:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:49.584401  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:46:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:49.584408  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:46:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:49.584417  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:46:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:50.484120  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:50.484188  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:50.492141  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[d3ac722b-ba0d-4df6-a091-702cbe6b26df] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:50 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008ddba0 2 [] false false map[] 0xc001c0b000 0xc001b6b600}
Jan 20 12:46:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:50.492169  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:50.508522  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:46:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:50.513331  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:46:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:50.524414  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945631" capacity="62382080" time="2023-01-20 12:46:50.509074982 -0500 EST m=+923.064849809"
Jan 20 12:46:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:50.524427  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902001684Ki" capacity="981310056Ki" time="2023-01-20 12:46:41.262517408 -0500 EST"
Jan 20 12:46:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:50.524433  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945631" capacity="62382080" time="2023-01-20 12:46:41.262517408 -0500 EST"
Jan 20 12:46:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:50.524439  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510777" capacity="511757" time="2023-01-20 12:46:50.524090381 -0500 EST m=+923.079865206"
Jan 20 12:46:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:50.524445  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59532052Ki" capacity="65586124Ki" time="2023-01-20 12:46:50.509074982 -0500 EST m=+923.064849809"
Jan 20 12:46:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:50.524451  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64402148Ki" capacity="65061836Ki" time="2023-01-20 12:46:50.52435892 -0500 EST m=+923.080133745"
Jan 20 12:46:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:50.524457  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902001684Ki" capacity="981310056Ki" time="2023-01-20 12:46:50.509074982 -0500 EST m=+923.064849809"
Jan 20 12:46:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:50.524485  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:46:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:50.581927  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[confidential-containers-system/cc-operator-daemon-install-t6mp7]
Jan 20 12:46:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:50.582006  199956 pod_workers.go:888] "Processing pod event" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" podUID=7af065b7-9095-4d91-9b9e-2644e7b1f4bf updateType=0
Jan 20 12:46:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:50.582070  199956 kubelet.go:1501] "syncPod enter" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" podUID=7af065b7-9095-4d91-9b9e-2644e7b1f4bf
Jan 20 12:46:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:50.582102  199956 kubelet_pods.go:1435] "Generating pod status" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7"
Jan 20 12:46:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:50.582184  199956 kubelet_pods.go:1447] "Got phase for pod" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" oldPhase=Running phase=Running
Jan 20 12:46:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:50.582468  199956 status_manager.go:535] "Ignoring same status for pod" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:04 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:20 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:20 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:04 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.6 PodIPs:[{IP:10.244.0.6}] StartTime:2023-01-20 12:32:04 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:cc-runtime-install-pod State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:32:20 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:quay.io/confidential-containers/runtime-payload-ci:kata-containers-amd64 ImageID:quay.io/confidential-containers/runtime-payload-ci@sha256:4736ba274765c889404fb98f01de0a997e68d2d7e5acca2440488f0e1337032b ContainerID:containerd://e10efaa459a32161059fda75d4bcaf3bbdb896781f772b508e43fe00bc7c9003 Started:0xc0011ff6ce}] QOSClass:BestEffort EphemeralContainerStatuses:[]}
Jan 20 12:46:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:50.582768  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7"
Jan 20 12:46:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:50.582844  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7"
Jan 20 12:46:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:50.583266  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="confidential-containers-system/cc-operator-daemon-install-t6mp7"
Jan 20 12:46:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:50.583432  199956 kubelet.go:1503] "syncPod exit" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" podUID=7af065b7-9095-4d91-9b9e-2644e7b1f4bf isTerminal=false
Jan 20 12:46:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:50.583486  199956 pod_workers.go:988] "Processing pod event done" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" podUID=7af065b7-9095-4d91-9b9e-2644e7b1f4bf updateType=0
Jan 20 12:46:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:50.604441  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" volumeName="containerd-conf" volumeSpecName="containerd-conf"
Jan 20 12:46:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:50.604549  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" volumeName="kata-artifacts" volumeSpecName="kata-artifacts"
Jan 20 12:46:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:50.604621  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" volumeName="dbus" volumeSpecName="dbus"
Jan 20 12:46:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:50.604705  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" volumeName="systemd" volumeSpecName="systemd"
Jan 20 12:46:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:50.604780  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" volumeName="local-bin" volumeSpecName="local-bin"
Jan 20 12:46:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:50.604898  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" volumeName="kube-api-access-x6vjr" volumeSpecName="kube-api-access-x6vjr"
Jan 20 12:46:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:50.636614  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-x6vjr\" (UniqueName: \"kubernetes.io/projected/7af065b7-9095-4d91-9b9e-2644e7b1f4bf-kube-api-access-x6vjr\") pod \"cc-operator-daemon-install-t6mp7\" (UID: \"7af065b7-9095-4d91-9b9e-2644e7b1f4bf\") Volume is already mounted to pod, but remount was requested." pod="confidential-containers-system/cc-operator-daemon-install-t6mp7"
Jan 20 12:46:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:50.636839  199956 projected.go:183] Setting up volume kube-api-access-x6vjr for pod 7af065b7-9095-4d91-9b9e-2644e7b1f4bf at /var/lib/kubelet/pods/7af065b7-9095-4d91-9b9e-2644e7b1f4bf/volumes/kubernetes.io~projected/kube-api-access-x6vjr
Jan 20 12:46:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:50.637232  199956 atomic_writer.go:161] pod confidential-containers-system/cc-operator-daemon-install-t6mp7 volume kube-api-access-x6vjr: no update required for target directory /var/lib/kubelet/pods/7af065b7-9095-4d91-9b9e-2644e7b1f4bf/volumes/kubernetes.io~projected/kube-api-access-x6vjr
Jan 20 12:46:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:50.637305  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-x6vjr\" (UniqueName: \"kubernetes.io/projected/7af065b7-9095-4d91-9b9e-2644e7b1f4bf-kube-api-access-x6vjr\") pod \"cc-operator-daemon-install-t6mp7\" (UID: \"7af065b7-9095-4d91-9b9e-2644e7b1f4bf\") " pod="confidential-containers-system/cc-operator-daemon-install-t6mp7"
Jan 20 12:46:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:51.165967  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:46:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:51.166039  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:51.174222  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[6dc1ae3b-2525-408d-b4ae-086ac7ad1147] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:51 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000979c20 2 [] false false map[] 0xc001f36100 0xc000afe840}
Jan 20 12:46:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:51.174261  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:51.484063  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:51.484127  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:51.492163  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[897b43e1-2d62-406a-a4c3-5a404d22342f] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:51 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001820ea0 2 [] false false map[] 0xc001846200 0xc000fd8370}
Jan 20 12:46:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:51.492194  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:51.582148  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:46:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:51.587915  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:46:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:51.587927  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:46:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:51.588756  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:46:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:51.589122  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:46:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:51.589169  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:46:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:51.589176  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:46:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:51.589185  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:46:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:52.484017  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:52.484087  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:52.492265  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[19a26882-963c-4e1c-a180-bd0f8688eda6] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:52 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0018218c0 2 [] false false map[] 0xc001846500 0xc001d0ee70}
Jan 20 12:46:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:52.492297  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:52.689985  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:46:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:52.690055  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:52.691028  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:52 GMT]] 0xc00129d2e0 2 [] true false map[] 0xc001846700 <nil>}
Jan 20 12:46:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:52.691150  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:46:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:52.696375  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:46:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:52.696433  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:52.697458  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:52 GMT]] 0xc00129d320 2 [] true false map[] 0xc001846900 <nil>}
Jan 20 12:46:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:52.697562  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:46:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:52.709801  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:46:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:52.709867  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:52.709880  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:46:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:52.709938  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:52.710893  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:52 GMT]] 0xc001821b00 2 [] true false map[] 0xc001846b00 <nil>}
Jan 20 12:46:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:52.710996  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:46:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:52.711003  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:52 GMT]] 0xc00013c400 2 [] true false map[] 0xc0014ecb00 <nil>}
Jan 20 12:46:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:52.711116  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:46:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:52.894232  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:46:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:53.394783  199956 handler.go:293] error while reading "/proc/199956/fd/34" link: readlink /proc/199956/fd/34: no such file or directory
Jan 20 12:46:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:53.483195  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:53.483260  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:53.491397  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[dea0948b-a01f-4bf5-ad8a-2a2817e9afa2] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:53 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00129dcc0 2 [] false false map[] 0xc001846d00 0xc0013fe420}
Jan 20 12:46:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:53.491425  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:53.582096  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[confidential-containers-system/cc-operator-pre-install-daemon-qjplj]
Jan 20 12:46:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:53.582193  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:46:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:53.582288  199956 pod_workers.go:888] "Processing pod event" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" podUID=b0713fbc-efc5-4044-9d08-2326a0752f87 updateType=0
Jan 20 12:46:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:53.582372  199956 kubelet.go:1501] "syncPod enter" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" podUID=b0713fbc-efc5-4044-9d08-2326a0752f87
Jan 20 12:46:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:53.582407  199956 kubelet_pods.go:1435] "Generating pod status" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj"
Jan 20 12:46:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:53.582492  199956 kubelet_pods.go:1447] "Got phase for pod" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" oldPhase=Running phase=Running
Jan 20 12:46:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:53.582793  199956 status_manager.go:535] "Ignoring same status for pod" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:01 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:05 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:05 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:01 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.5 PodIPs:[{IP:10.244.0.5}] StartTime:2023-01-20 12:32:01 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:cc-runtime-pre-install-pod State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:32:03 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:quay.io/confidential-containers/container-engine-for-cc-payload:1034f9fcf947b22eea080a6f77d8e164e2369849 ImageID:quay.io/confidential-containers/container-engine-for-cc-payload@sha256:f86f078b3a47026a066e65c7d836d9b9a43bf177555c276624d90f42e50279a1 ContainerID:containerd://e0c5b991f81d5128566686552e45a9bbc8c584e4f6c94909edb3a42720dacc90 Started:0xc001a405c9}] QOSClass:BestEffort EphemeralContainerStatuses:[]}
Jan 20 12:46:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:53.583154  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj"
Jan 20 12:46:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:53.583228  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj"
Jan 20 12:46:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:53.583618  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj"
Jan 20 12:46:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:53.583787  199956 kubelet.go:1503] "syncPod exit" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" podUID=b0713fbc-efc5-4044-9d08-2326a0752f87 isTerminal=false
Jan 20 12:46:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:53.583846  199956 pod_workers.go:988] "Processing pod event done" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" podUID=b0713fbc-efc5-4044-9d08-2326a0752f87 updateType=0
Jan 20 12:46:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:53.587977  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:46:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:53.587989  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:46:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:53.588647  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:46:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:53.589013  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:46:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:53.589060  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:46:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:53.589067  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:46:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:53.589076  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:46:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:53.628589  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" volumeName="confidential-containers-artifacts" volumeSpecName="confidential-containers-artifacts"
Jan 20 12:46:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:53.628732  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" volumeName="etc-systemd-system" volumeSpecName="etc-systemd-system"
Jan 20 12:46:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:53.628814  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" volumeName="dbus" volumeSpecName="dbus"
Jan 20 12:46:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:53.628880  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" volumeName="systemd" volumeSpecName="systemd"
Jan 20 12:46:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:53.629004  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" volumeName="kube-api-access-gcgm6" volumeSpecName="kube-api-access-gcgm6"
Jan 20 12:46:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:53.658761  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-gcgm6\" (UniqueName: \"kubernetes.io/projected/b0713fbc-efc5-4044-9d08-2326a0752f87-kube-api-access-gcgm6\") pod \"cc-operator-pre-install-daemon-qjplj\" (UID: \"b0713fbc-efc5-4044-9d08-2326a0752f87\") Volume is already mounted to pod, but remount was requested." pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj"
Jan 20 12:46:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:53.658949  199956 projected.go:183] Setting up volume kube-api-access-gcgm6 for pod b0713fbc-efc5-4044-9d08-2326a0752f87 at /var/lib/kubelet/pods/b0713fbc-efc5-4044-9d08-2326a0752f87/volumes/kubernetes.io~projected/kube-api-access-gcgm6
Jan 20 12:46:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:53.659375  199956 atomic_writer.go:161] pod confidential-containers-system/cc-operator-pre-install-daemon-qjplj volume kube-api-access-gcgm6: no update required for target directory /var/lib/kubelet/pods/b0713fbc-efc5-4044-9d08-2326a0752f87/volumes/kubernetes.io~projected/kube-api-access-gcgm6
Jan 20 12:46:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:53.659451  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-gcgm6\" (UniqueName: \"kubernetes.io/projected/b0713fbc-efc5-4044-9d08-2326a0752f87-kube-api-access-gcgm6\") pod \"cc-operator-pre-install-daemon-qjplj\" (UID: \"b0713fbc-efc5-4044-9d08-2326a0752f87\") " pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj"
Jan 20 12:46:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:54.483435  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:54.483505  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:54.491461  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[595a0392-9afc-4eeb-b238-dbac83125b71] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:54 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ce8000 2 [] false false map[] 0xc000fcdf00 0xc001235ce0}
Jan 20 12:46:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:54.491508  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:55.483730  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:55.483796  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:55.492265  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[25b696b2-ef7b-48a9-b858-c5ddf0c90e4d] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:55 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0020fcc00 2 [] false false map[] 0xc00105ad00 0xc001235ef0}
Jan 20 12:46:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:55.492295  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:55.582139  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[kube-system/coredns-6d4b75cb6d-zdl2m]
Jan 20 12:46:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:55.582232  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:46:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:55.582364  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e updateType=0
Jan 20 12:46:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:55.582445  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e
Jan 20 12:46:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:55.582479  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:46:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:55.582567  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/coredns-6d4b75cb6d-zdl2m" oldPhase=Running phase=Running
Jan 20 12:46:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:55.582869  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/coredns-6d4b75cb6d-zdl2m" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:44 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:44 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.3 PodIPs:[{IP:10.244.0.3}] StartTime:2023-01-20 12:31:42 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:coredns State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:44 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:k8s.gcr.io/coredns/coredns:v1.8.6 ImageID:k8s.gcr.io/coredns/coredns@sha256:5b6ec0d6de9baaf3e92d0f66cd96a25b9edbce8716f5f15dcd1a616b3abd590e ContainerID:containerd://e489181a7f95431b6d92f037dbe3f788b46a5e024df7aaf29e0115dab1168ab1 Started:0xc0012bb10e}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:46:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:55.583231  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:46:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:55.583300  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:46:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:55.583807  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:46:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:55.583975  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e isTerminal=false
Jan 20 12:46:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:55.584030  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e updateType=0
Jan 20 12:46:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:55.587965  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:46:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:55.587976  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:46:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:55.587982  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:46:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:55.588597  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:46:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:55.588643  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:46:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:55.588649  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:46:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:55.588657  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:46:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:55.643975  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/coredns-6d4b75cb6d-zdl2m" volumeName="config-volume" volumeSpecName="config-volume"
Jan 20 12:46:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:55.644137  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/coredns-6d4b75cb6d-zdl2m" volumeName="kube-api-access-tqzsm" volumeSpecName="kube-api-access-tqzsm"
Jan 20 12:46:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:55.674193  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/cf25b9ea-6823-4eff-b30d-36a09f9d897e-config-volume\") pod \"coredns-6d4b75cb6d-zdl2m\" (UID: \"cf25b9ea-6823-4eff-b30d-36a09f9d897e\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:46:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:55.674315  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-tqzsm\" (UniqueName: \"kubernetes.io/projected/cf25b9ea-6823-4eff-b30d-36a09f9d897e-kube-api-access-tqzsm\") pod \"coredns-6d4b75cb6d-zdl2m\" (UID: \"cf25b9ea-6823-4eff-b30d-36a09f9d897e\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:46:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:55.674454  199956 projected.go:183] Setting up volume kube-api-access-tqzsm for pod cf25b9ea-6823-4eff-b30d-36a09f9d897e at /var/lib/kubelet/pods/cf25b9ea-6823-4eff-b30d-36a09f9d897e/volumes/kubernetes.io~projected/kube-api-access-tqzsm
Jan 20 12:46:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:55.674509  199956 configmap.go:181] Setting up volume config-volume for pod cf25b9ea-6823-4eff-b30d-36a09f9d897e at /var/lib/kubelet/pods/cf25b9ea-6823-4eff-b30d-36a09f9d897e/volumes/kubernetes.io~configmap/config-volume
Jan 20 12:46:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:55.674594  199956 configmap.go:205] Received configMap kube-system/coredns containing (1) pieces of data, 339 total bytes
Jan 20 12:46:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:55.674671  199956 quota_linux.go:271] SupportsQuotas called, but quotas disabled
Jan 20 12:46:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:55.674881  199956 atomic_writer.go:161] pod kube-system/coredns-6d4b75cb6d-zdl2m volume kube-api-access-tqzsm: no update required for target directory /var/lib/kubelet/pods/cf25b9ea-6823-4eff-b30d-36a09f9d897e/volumes/kubernetes.io~projected/kube-api-access-tqzsm
Jan 20 12:46:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:55.674897  199956 atomic_writer.go:161] pod kube-system/coredns-6d4b75cb6d-zdl2m volume config-volume: no update required for target directory /var/lib/kubelet/pods/cf25b9ea-6823-4eff-b30d-36a09f9d897e/volumes/kubernetes.io~configmap/config-volume
Jan 20 12:46:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:55.674951  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-tqzsm\" (UniqueName: \"kubernetes.io/projected/cf25b9ea-6823-4eff-b30d-36a09f9d897e-kube-api-access-tqzsm\") pod \"coredns-6d4b75cb6d-zdl2m\" (UID: \"cf25b9ea-6823-4eff-b30d-36a09f9d897e\") " pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:46:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:55.674975  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/cf25b9ea-6823-4eff-b30d-36a09f9d897e-config-volume\") pod \"coredns-6d4b75cb6d-zdl2m\" (UID: \"cf25b9ea-6823-4eff-b30d-36a09f9d897e\") " pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:46:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:56.164667  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:46:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:56.164751  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:56.172518  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:56 GMT] X-Content-Type-Options:[nosniff]] 0xc0020fd920 2 [] false false map[] 0xc001847100 0xc001c87ce0}
Jan 20 12:46:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:56.172547  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:46:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:56.271842  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:46:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:56.271906  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:56.273105  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:56 GMT]] 0xc0020fda40 29 [] true false map[] 0xc0016ab100 <nil>}
Jan 20 12:46:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:56.273209  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:46:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:56.483598  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:56.483665  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:56.496957  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[d218d912-f13b-43ce-9f38-9478d16561a5] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:56 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0003211c0 2 [] false false map[] 0xc0016ab300 0xc001ad91e0}
Jan 20 12:46:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:56.497107  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:56.895648  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:46:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:56.895714  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:56.903564  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:56 GMT] X-Content-Type-Options:[nosniff]] 0xc0017193e0 2 [] false false map[] 0xc00105b300 0xc001ad9340}
Jan 20 12:46:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:56.903607  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:46:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:57.209180  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:46:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:57.209188  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:46:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:57.209352  199956 interface.go:209] Interface eno2 is up
Jan 20 12:46:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:57.209381  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:46:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:57.209388  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:46:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:57.209393  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:46:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:57.209397  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:46:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:57.209401  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:46:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:57.209568  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:46:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:57.209577  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:46:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:57.209582  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:46:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:57.209586  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:46:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:57.484174  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:57.484240  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:57.492109  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[1f00a88b-95cb-4d77-910a-5bb4de7f4cbc] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:57 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00013dba0 2 [] false false map[] 0xc0014ecf00 0xc0010d2840}
Jan 20 12:46:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:57.492141  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:57.582035  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:46:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:57.589637  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:46:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:57.589699  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:46:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:57.591967  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:46:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:57.593909  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:46:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:57.594132  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:46:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:57.594178  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:46:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:57.594223  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:46:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:57.895450  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:46:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:58.483475  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:58.483542  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:58.491233  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[6825d8a6-0a2f-4acf-952d-61d8ae2be415] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:58 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ffa4c0 2 [] false false map[] 0xc0014ed200 0xc0020908f0}
Jan 20 12:46:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:58.491261  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:58.581504  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[default/unsigned-unencrypted-cc-1]
Jan 20 12:46:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:58.581588  199956 pod_workers.go:888] "Processing pod event" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:46:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:58.581649  199956 kubelet.go:1501] "syncPod enter" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:46:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:58.581681  199956 kubelet_pods.go:1435] "Generating pod status" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:46:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:58.581804  199956 kubelet_pods.go:1447] "Got phase for pod" pod="default/unsigned-unencrypted-cc-1" oldPhase=Pending phase=Pending
Jan 20 12:46:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:58.582084  199956 status_manager.go:535] "Ignoring same status for pod" pod="default/unsigned-unencrypted-cc-1" status={Phase:Pending Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.16 PodIPs:[{IP:10.244.0.16}] StartTime:2023-01-20 12:41:08 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:ci-example256m State:{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "ngcn-registry.sh.intel.com:443/ci-example256m:latest",} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest ImageID: ContainerID: Started:0xc00094eac9}] QOSClass:Guaranteed EphemeralContainerStatuses:[]}
Jan 20 12:46:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:58.582412  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:46:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:58.582491  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:46:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:58.582541  199956 kuberuntime_manager.go:638] "Container of pod is not in the desired state and shall be started" containerName="ci-example256m" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:46:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:58.582582  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:08841bf63919f9c8acff77c51620bd8e684c550feb6b3e8784239bc3d9079457 Attempt:4 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:46:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:58.582883  199956 kuberuntime_manager.go:889] "Creating container in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:46:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:58.584004  199956 kuberuntime_manager.go:903] "Container start failed in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1" containerMessage="Back-off pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\"" err="ImagePullBackOff"
Jan 20 12:46:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:58.584074  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Normal" reason="BackOff" message="Back-off pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\""
Jan 20 12:46:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:58.584089  199956 kubelet.go:1503] "syncPod exit" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d isTerminal=false
Jan 20 12:46:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:58.584123  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Warning" reason="Failed" message="Error: ImagePullBackOff"
Jan 20 12:46:58 zcy-Z390-AORUS-MASTER kubelet[199956]: E0120 12:46:58.584151  199956 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ci-example256m\" with ImagePullBackOff: \"Back-off pulling image \\\"ngcn-registry.sh.intel.com:443/ci-example256m:latest\\\"\"" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:46:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:58.584202  199956 pod_workers.go:988] "Processing pod event done" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:46:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:58.666799  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="default/unsigned-unencrypted-cc-1" volumeName="kube-api-access-qcb8g" volumeSpecName="kube-api-access-qcb8g"
Jan 20 12:46:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:58.694959  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") Volume is already mounted to pod, but remount was requested." pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:46:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:58.695180  199956 projected.go:183] Setting up volume kube-api-access-qcb8g for pod 4f97314d-815b-4787-b174-5c158cd28c9d at /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:46:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:58.695637  199956 atomic_writer.go:161] pod default/unsigned-unencrypted-cc-1 volume kube-api-access-qcb8g: no update required for target directory /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:46:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:58.695707  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") " pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:46:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:58.878906  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:46:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:58.878969  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:58.879003  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/healthz" timeout="1s"
Jan 20 12:46:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:58.879061  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:58.880239  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:58 GMT] X-Content-Type-Options:[nosniff]] 0xc00123f040 2 [] true false map[] 0xc001c0bf00 <nil>}
Jan 20 12:46:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:58.880303  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:58 GMT] X-Content-Type-Options:[nosniff]] 0xc0008e2100 2 [] true false map[] 0xc0014ed500 <nil>}
Jan 20 12:46:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:58.880348  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:46:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:58.880889  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:46:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:59.483200  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:46:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:59.483269  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:46:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:59.491611  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[17d3f812-b7d2-4857-8cc7-e9e54ad2111a] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:46:59 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008e24c0 2 [] false false map[] 0xc0016aa100 0xc000f8c370}
Jan 20 12:46:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:59.491644  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:46:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:59.581830  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:46:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:59.587841  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:46:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:59.587853  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:46:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:59.587859  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:46:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:59.588642  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:46:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:59.588694  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:46:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:59.588701  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:46:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:46:59.588709  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:47:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:00.483384  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:00.483454  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:00.491428  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[3254d259-ec8e-4f8d-abef-c2cd7570cbc3] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:00 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123ec60 2 [] false false map[] 0xc0016aa400 0xc000e2e210}
Jan 20 12:47:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:00.491458  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:00.524970  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:47:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:00.533047  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:47:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:00.542798  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64402044Ki" capacity="65061836Ki" time="2023-01-20 12:47:00.542743708 -0500 EST m=+933.098518532"
Jan 20 12:47:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:00.542810  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902001624Ki" capacity="981310056Ki" time="2023-01-20 12:47:00.527233259 -0500 EST m=+933.083008151"
Jan 20 12:47:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:00.542817  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945631" capacity="62382080" time="2023-01-20 12:47:00.527233259 -0500 EST m=+933.083008151"
Jan 20 12:47:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:00.542823  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902001624Ki" capacity="981310056Ki" time="2023-01-20 12:46:51.262583817 -0500 EST"
Jan 20 12:47:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:00.542828  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945631" capacity="62382080" time="2023-01-20 12:46:51.262583817 -0500 EST"
Jan 20 12:47:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:00.542833  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510776" capacity="511757" time="2023-01-20 12:47:00.54249112 -0500 EST m=+933.098265943"
Jan 20 12:47:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:00.542839  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59532416Ki" capacity="65586124Ki" time="2023-01-20 12:47:00.527233259 -0500 EST m=+933.083008151"
Jan 20 12:47:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:00.542864  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:47:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:01.165913  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:47:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:01.165991  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:01.174398  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[1e04e1fa-070c-4a83-a355-afc8f10ee71d] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:01 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ffb760 2 [] false false map[] 0xc000fcc500 0xc0011d73f0}
Jan 20 12:47:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:01.174446  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:01.483437  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:01.483505  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:01.496493  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[d814e065-a21a-4109-a286-c526fd45d38b] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:01 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008dc640 2 [] false false map[] 0xc0016ab800 0xc0013edce0}
Jan 20 12:47:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:01.496643  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:01.582036  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:47:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:01.587908  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:47:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:01.587920  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:47:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:01.588557  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:47:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:01.588891  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:47:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:01.588935  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:47:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:01.588942  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:47:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:01.588950  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:47:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:02.483850  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:02.483921  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:02.492094  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[7c27f3cf-028f-4f9a-be6f-3c7fd1f2e192] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:02 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008dc680 2 [] false false map[] 0xc0014eca00 0xc0016a8210}
Jan 20 12:47:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:02.492123  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:02.689906  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:47:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:02.689967  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:02.691129  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:02 GMT]] 0xc00129cb00 2 [] true false map[] 0xc0016abc00 <nil>}
Jan 20 12:47:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:02.691235  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:47:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:02.696442  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:47:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:02.696501  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:02.697532  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:02 GMT]] 0xc00129cb40 2 [] true false map[] 0xc0014ecd00 <nil>}
Jan 20 12:47:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:02.697636  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:47:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:02.709847  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:47:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:02.709906  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:02.709915  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:47:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:02.709978  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:02.710876  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:02 GMT]] 0xc00123fd20 2 [] true false map[] 0xc0014ecf00 <nil>}
Jan 20 12:47:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:02.710982  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:47:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:02.711009  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:02 GMT]] 0xc00129cbe0 2 [] true false map[] 0xc0016abe00 <nil>}
Jan 20 12:47:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:02.711119  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:47:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:02.896950  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:47:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:03.483921  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:03.483985  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:03.492254  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[63333e47-c38a-46ec-bf4e-6ace5c5bfce7] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:03 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00129d580 2 [] false false map[] 0xc0011c4000 0xc0016b0370}
Jan 20 12:47:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:03.492284  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:03.581810  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:47:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:03.587863  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:47:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:03.587876  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:47:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:03.587883  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:47:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:03.588586  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:47:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:03.588633  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:47:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:03.588640  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:47:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:03.588648  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:47:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:04.483435  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:04.483502  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:04.491596  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[b5efcfe0-18c4-466b-8f23-18d30f35f456] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:04 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001820200 2 [] false false map[] 0xc0021a3e00 0xc001c07600}
Jan 20 12:47:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:04.491629  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:05.483979  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:05.484050  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:05.492217  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[489346fa-e03b-42d4-9384-7ea6053e800c] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:05 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0013007e0 2 [] false false map[] 0xc0014ec700 0xc0014ae0b0}
Jan 20 12:47:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:05.492251  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:05.582123  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:47:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:05.587924  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:47:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:05.587937  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:47:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:05.588600  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:47:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:05.588940  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:47:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:05.588986  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:47:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:05.588993  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:47:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:05.589002  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:47:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:06.164518  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:47:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:06.164593  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:06.171528  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:06 GMT] X-Content-Type-Options:[nosniff]] 0xc0018204a0 2 [] false false map[] 0xc0002c3e00 0xc001c06420}
Jan 20 12:47:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:06.171557  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:47:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:06.272202  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:47:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:06.272215  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:06.272647  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:06 GMT]] 0xc001301620 29 [] true false map[] 0xc0014ed000 <nil>}
Jan 20 12:47:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:06.272669  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:47:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:06.484176  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:06.484239  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:06.492100  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[1bcf243a-c9eb-40cc-984d-0677c8309ab6] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:06 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001301920 2 [] false false map[] 0xc0014ed200 0xc0014ae370}
Jan 20 12:47:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:06.492129  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:06.895401  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:47:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:06.895466  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:06.902647  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:06 GMT] X-Content-Type-Options:[nosniff]] 0xc001486f20 2 [] false false map[] 0xc0014ed500 0xc0010d20b0}
Jan 20 12:47:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:06.902706  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.484079  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.484152  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.492302  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[6af9ce3b-aeb9-4001-ab08-17f76697e4cd] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:07 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001487f00 2 [] false false map[] 0xc001c0ab00 0xc001458210}
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.492333  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.525890  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/etcd.yaml"
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.528010  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver.yaml"
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.530929  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.531959  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-scheduler.yaml"
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.532298  199956 config.go:279] "Setting pods for source" source="file"
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.553824  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.553832  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.554012  199956 interface.go:209] Interface eno2 is up
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.554045  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.554054  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.554060  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.554064  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.554069  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.554386  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.554396  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.554402  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.554407  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.582109  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr]
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.582189  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.582290  199956 pod_workers.go:888] "Processing pod event" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d updateType=0
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.582376  199956 kubelet.go:1501] "syncPod enter" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.582410  199956 kubelet_pods.go:1435] "Generating pod status" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.582513  199956 kubelet_pods.go:1447] "Got phase for pod" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" oldPhase=Running phase=Running
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.583264  199956 status_manager.go:535] "Ignoring same status for pod" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:58 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:08 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:08 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:58 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.4 PodIPs:[{IP:10.244.0.4}] StartTime:2023-01-20 12:31:58 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:kube-rbac-proxy State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:59 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:gcr.io/kubebuilder/kube-rbac-proxy:v0.13.0 ImageID:gcr.io/kubebuilder/kube-rbac-proxy@sha256:d99a8d144816b951a67648c12c0b988936ccd25cf3754f3cd85ab8c01592248f ContainerID:containerd://1cb44bdbde0b41852e382d7dab7c184af8dd4e5b25d5cfe6a10a9c8ab601659d Started:0xc000b25ae2} {Name:manager State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:59 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:quay.io/confidential-containers/operator:v0.2.0 ImageID:quay.io/confidential-containers/operator@sha256:c965b55253a9abe4c2f7596c42467fa59f2cc741bfafeed1d25629ed6f8df12d ContainerID:containerd://186424b47c7b9022ecfe8996dc73cd9101f7539259cd65914279c84c9a02a288 Started:0xc000b25ae3}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.583656  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.583729  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.584387  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.584573  199956 kubelet.go:1503] "syncPod exit" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d isTerminal=false
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.584631  199956 pod_workers.go:988] "Processing pod event done" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d updateType=0
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.587949  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.587961  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.588593  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.588993  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.589038  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.589044  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.589053  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.633453  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" volumeName="kube-api-access-4pnfq" volumeSpecName="kube-api-access-4pnfq"
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.662648  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-4pnfq\" (UniqueName: \"kubernetes.io/projected/d2688d45-2487-46e7-aecb-e3479626909d-kube-api-access-4pnfq\") pod \"cc-operator-controller-manager-79797456f6-m6znr\" (UID: \"d2688d45-2487-46e7-aecb-e3479626909d\") Volume is already mounted to pod, but remount was requested." pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.662831  199956 projected.go:183] Setting up volume kube-api-access-4pnfq for pod d2688d45-2487-46e7-aecb-e3479626909d at /var/lib/kubelet/pods/d2688d45-2487-46e7-aecb-e3479626909d/volumes/kubernetes.io~projected/kube-api-access-4pnfq
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.663245  199956 atomic_writer.go:161] pod confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr volume kube-api-access-4pnfq: no update required for target directory /var/lib/kubelet/pods/d2688d45-2487-46e7-aecb-e3479626909d/volumes/kubernetes.io~projected/kube-api-access-4pnfq
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.663319  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-4pnfq\" (UniqueName: \"kubernetes.io/projected/d2688d45-2487-46e7-aecb-e3479626909d-kube-api-access-4pnfq\") pod \"cc-operator-controller-manager-79797456f6-m6znr\" (UID: \"d2688d45-2487-46e7-aecb-e3479626909d\") " pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:47:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:07.898920  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:47:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:08.483819  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:08.483890  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:08.492113  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[1f491d30-70d6-481b-9179-facc324e1ef4] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:08 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ffa860 2 [] false false map[] 0xc001f37500 0xc00189fb80}
Jan 20 12:47:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:08.492143  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:08.878732  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:47:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:08.878795  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:08.880013  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:08 GMT] X-Content-Type-Options:[nosniff]] 0xc000ffb2a0 2 [] true false map[] 0xc001f37800 <nil>}
Jan 20 12:47:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:08.880125  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:47:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:09.483710  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:09.483777  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:09.491957  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[469465f0-32a4-4e4a-808a-edaf5672d0d3] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:09 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ffb680 2 [] false false map[] 0xc000fcdd00 0xc0019f9d90}
Jan 20 12:47:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:09.491987  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:09.581913  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:47:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:09.587869  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:47:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:09.587882  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:47:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:09.587888  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:47:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:09.588639  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:47:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:09.588690  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:47:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:09.588697  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:47:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:09.588706  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:47:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:10.483590  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:10.483659  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:10.496767  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[4eff09ae-688d-4f87-9518-2fa79bbacd00] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:10 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ffbf60 2 [] false false map[] 0xc001c0bb00 0xc001c8f4a0}
Jan 20 12:47:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:10.496905  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:10.544014  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:47:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:10.552078  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:47:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:10.561369  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945631" capacity="62382080" time="2023-01-20 12:47:01.262366566 -0500 EST"
Jan 20 12:47:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:10.561381  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510777" capacity="511757" time="2023-01-20 12:47:10.561068693 -0500 EST m=+943.116843517"
Jan 20 12:47:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:10.561388  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59532776Ki" capacity="65586124Ki" time="2023-01-20 12:47:10.546217404 -0500 EST m=+943.101992291"
Jan 20 12:47:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:10.561394  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64401884Ki" capacity="65061836Ki" time="2023-01-20 12:47:10.561316089 -0500 EST m=+943.117090913"
Jan 20 12:47:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:10.561400  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902001584Ki" capacity="981310056Ki" time="2023-01-20 12:47:10.546217404 -0500 EST m=+943.101992291"
Jan 20 12:47:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:10.561405  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945631" capacity="62382080" time="2023-01-20 12:47:10.546217404 -0500 EST m=+943.101992291"
Jan 20 12:47:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:10.561411  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902001584Ki" capacity="981310056Ki" time="2023-01-20 12:47:01.262366566 -0500 EST"
Jan 20 12:47:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:10.561436  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:47:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:10.581797  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[kube-system/etcd-zcy-z390-aorus-master]
Jan 20 12:47:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:10.581824  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d updateType=0
Jan 20 12:47:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:10.581844  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d
Jan 20 12:47:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:10.581855  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/etcd-zcy-z390-aorus-master"
Jan 20 12:47:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:10.581885  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/etcd-zcy-z390-aorus-master" oldPhase=Running phase=Running
Jan 20 12:47:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:10.581980  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/etcd-zcy-z390-aorus-master" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:28 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:37 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:37 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:28 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.239.159.53 PodIPs:[{IP:10.239.159.53}] StartTime:2023-01-20 12:31:28 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:etcd State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:22 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:6 Image:k8s.gcr.io/etcd:3.5.3-0 ImageID:k8s.gcr.io/etcd@sha256:13f53ed1d91e2e11aac476ee9a0269fdda6cc4874eba903efd40daf50c55eee5 ContainerID:containerd://9b11acac1b891eafc7ff2b244f1c42938f71a575ce81ff917e3b7ebf61d2e045 Started:0xc002133bce}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:47:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:10.582083  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/etcd-zcy-z390-aorus-master"
Jan 20 12:47:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:10.582104  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/etcd-zcy-z390-aorus-master"
Jan 20 12:47:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:10.582300  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/etcd-zcy-z390-aorus-master"
Jan 20 12:47:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:10.582352  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d isTerminal=false
Jan 20 12:47:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:10.582369  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d updateType=0
Jan 20 12:47:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:10.653241  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/etcd-zcy-z390-aorus-master" volumeName="etcd-certs" volumeSpecName="etcd-certs"
Jan 20 12:47:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:10.653342  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/etcd-zcy-z390-aorus-master" volumeName="etcd-data" volumeSpecName="etcd-data"
Jan 20 12:47:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:11.166379  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:47:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:11.166441  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:11.180082  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[7548f237-08e2-4a3c-9ef2-28f8ee377c02] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:11 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0020fc3e0 2 [] false false map[] 0xc0014c7000 0xc00208e0b0}
Jan 20 12:47:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:11.180204  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:11.483377  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:11.483441  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:11.491992  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[86127476-f85e-4de2-863f-49cd09bce08e] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:11 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001718000 2 [] false false map[] 0xc001f37a00 0xc0021078c0}
Jan 20 12:47:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:11.492030  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:11.581485  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:47:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:11.588851  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:47:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:11.588915  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:47:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:11.591398  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:47:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:11.593357  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:47:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:11.593589  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:47:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:11.593622  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:47:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:11.593665  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:47:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:12.483654  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:12.483723  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:12.497146  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[ae42b027-44c0-478d-bc6c-eaa40ac1f336] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:12 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001300000 2 [] false false map[] 0xc0014c6800 0xc00112eb00}
Jan 20 12:47:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:12.497309  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:12.624315  199956 handler.go:293] error while reading "/proc/199956/fd/34" link: readlink /proc/199956/fd/34: no such file or directory
Jan 20 12:47:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:12.689427  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:47:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:12.689491  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:12.690652  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:12 GMT]] 0xc00129cc20 2 [] true false map[] 0xc001a4a400 <nil>}
Jan 20 12:47:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:12.690765  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:47:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:12.696181  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:47:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:12.696242  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:12.697288  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:12 GMT]] 0xc001821100 2 [] true false map[] 0xc0014c6b00 <nil>}
Jan 20 12:47:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:12.697388  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:47:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:12.710610  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:47:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:12.710672  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:12.710687  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:47:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:12.710744  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:12.711692  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:12 GMT]] 0xc0013006e0 2 [] true false map[] 0xc001a4a600 <nil>}
Jan 20 12:47:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:12.711746  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:12 GMT]] 0xc001821160 2 [] true false map[] 0xc001f37800 <nil>}
Jan 20 12:47:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:12.711797  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:47:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:12.711843  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:47:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:12.900518  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:47:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:13.484042  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:13.484114  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:13.492053  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[3417f809-a180-4062-bb92-ca96c910b436] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:13 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001301060 2 [] false false map[] 0xc001a4aa00 0xc000f8de40}
Jan 20 12:47:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:13.492083  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:13.581995  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[default/unsigned-unencrypted-cc-1]
Jan 20 12:47:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:13.582091  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:47:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:13.582153  199956 pod_workers.go:888] "Processing pod event" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:47:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:13.582241  199956 kubelet.go:1501] "syncPod enter" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:47:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:13.582275  199956 kubelet_pods.go:1435] "Generating pod status" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:47:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:13.582377  199956 kubelet_pods.go:1447] "Got phase for pod" pod="default/unsigned-unencrypted-cc-1" oldPhase=Pending phase=Pending
Jan 20 12:47:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:13.582680  199956 status_manager.go:535] "Ignoring same status for pod" pod="default/unsigned-unencrypted-cc-1" status={Phase:Pending Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.16 PodIPs:[{IP:10.244.0.16}] StartTime:2023-01-20 12:41:08 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:ci-example256m State:{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "ngcn-registry.sh.intel.com:443/ci-example256m:latest",} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest ImageID: ContainerID: Started:0xc000c6c1d9}] QOSClass:Guaranteed EphemeralContainerStatuses:[]}
Jan 20 12:47:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:13.583019  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:47:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:13.583090  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:47:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:13.583138  199956 kuberuntime_manager.go:638] "Container of pod is not in the desired state and shall be started" containerName="ci-example256m" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:47:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:13.583180  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:08841bf63919f9c8acff77c51620bd8e684c550feb6b3e8784239bc3d9079457 Attempt:4 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:47:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:13.583460  199956 kuberuntime_manager.go:889] "Creating container in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:47:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:13.584593  199956 kuberuntime_manager.go:903] "Container start failed in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1" containerMessage="Back-off pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\"" err="ImagePullBackOff"
Jan 20 12:47:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:13.584601  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Normal" reason="BackOff" message="Back-off pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\""
Jan 20 12:47:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:13.584667  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Warning" reason="Failed" message="Error: ImagePullBackOff"
Jan 20 12:47:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:13.584670  199956 kubelet.go:1503] "syncPod exit" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d isTerminal=false
Jan 20 12:47:13 zcy-Z390-AORUS-MASTER kubelet[199956]: E0120 12:47:13.584743  199956 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ci-example256m\" with ImagePullBackOff: \"Back-off pulling image \\\"ngcn-registry.sh.intel.com:443/ci-example256m:latest\\\"\"" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:47:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:13.584792  199956 pod_workers.go:988] "Processing pod event done" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:47:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:13.587928  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:47:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:13.587940  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:47:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:13.587946  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:47:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:13.588399  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:47:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:13.588445  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:47:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:13.588451  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:47:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:13.588460  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:47:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:13.676089  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="default/unsigned-unencrypted-cc-1" volumeName="kube-api-access-qcb8g" volumeSpecName="kube-api-access-qcb8g"
Jan 20 12:47:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:13.701308  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") Volume is already mounted to pod, but remount was requested." pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:47:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:13.701488  199956 projected.go:183] Setting up volume kube-api-access-qcb8g for pod 4f97314d-815b-4787-b174-5c158cd28c9d at /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:47:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:13.701907  199956 atomic_writer.go:161] pod default/unsigned-unencrypted-cc-1 volume kube-api-access-qcb8g: no update required for target directory /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:47:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:13.701982  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") " pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:47:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:14.483563  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:14.483638  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:14.497073  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[221dcce3-422f-426f-9e2f-49c38a31654d] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:14 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0018212c0 2 [] false false map[] 0xc0014c6e00 0xc0012ff3f0}
Jan 20 12:47:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:14.497208  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:15.483680  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:15.483748  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:15.496464  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[6ecc94f6-24e8-4282-837f-bd1139887b8e] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:15 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001301f60 2 [] false false map[] 0xc001a4af00 0xc0013f1c30}
Jan 20 12:47:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:15.496652  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:15.582046  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:47:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:15.589420  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:47:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:15.589486  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:47:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:15.591684  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:47:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:15.593644  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:47:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:15.593859  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:47:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:15.593891  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:47:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:15.593934  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:47:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:16.164994  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:47:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:16.165067  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:16.175896  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:16 GMT] X-Content-Type-Options:[nosniff]] 0xc001487c20 2 [] false false map[] 0xc0000dc400 0xc0018f9ef0}
Jan 20 12:47:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:16.176041  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:47:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:16.272186  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:47:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:16.272248  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:16.273567  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:16 GMT]] 0xc0008e2000 29 [] true false map[] 0xc0016aa100 <nil>}
Jan 20 12:47:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:16.273671  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:47:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:16.483702  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:16.483761  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:16.496442  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[16ca84e6-a1f8-4925-a6d8-91817fb6bbda] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:16 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123e620 2 [] false false map[] 0xc0016aa400 0xc001773970}
Jan 20 12:47:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:16.496596  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:16.895595  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:47:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:16.895683  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:16.902836  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:16 GMT] X-Content-Type-Options:[nosniff]] 0xc0008dc620 2 [] false false map[] 0xc0016ab700 0xc001b53c30}
Jan 20 12:47:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:16.902863  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:47:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:17.483793  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:17.483859  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:17.492288  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[978d7089-ec9c-43ee-b55c-eb3f2dcd0836] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:17 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008e27a0 2 [] false false map[] 0xc000fcc300 0xc001c20f20}
Jan 20 12:47:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:17.492315  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:17.581618  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:47:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:17.587050  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:47:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:17.587062  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:47:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:17.587069  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:47:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:17.588898  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:47:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:17.589071  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:47:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:17.589079  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:47:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:17.589088  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:47:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:17.901615  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:47:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:17.954791  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:47:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:17.954797  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:47:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:17.955016  199956 interface.go:209] Interface eno2 is up
Jan 20 12:47:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:17.955041  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:47:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:17.955048  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:47:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:17.955053  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:47:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:17.955056  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:47:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:17.955060  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:47:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:17.955373  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:47:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:17.955399  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:47:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:17.955417  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:47:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:17.955421  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:47:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:18.483382  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:18.483449  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:18.491353  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[b9c29ca7-660a-415b-ba33-b3f8d54a2fe7] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:18 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00129d2c0 2 [] false false map[] 0xc0011c4200 0xc000b1e0b0}
Jan 20 12:47:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:18.491383  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:18.879162  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/healthz" timeout="1s"
Jan 20 12:47:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:18.879231  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:18.879233  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:47:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:18.879318  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:18.880395  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:18 GMT] X-Content-Type-Options:[nosniff]] 0xc00123e0a0 2 [] true false map[] 0xc0011c4400 <nil>}
Jan 20 12:47:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:18.880438  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:18 GMT] X-Content-Type-Options:[nosniff]] 0xc001821100 2 [] true false map[] 0xc001f37a00 <nil>}
Jan 20 12:47:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:18.880505  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:47:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:18.880551  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:47:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:19.483795  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:19.483856  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:19.493284  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[b005d967-e133-4be8-89e4-f7a1f6432497] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:19 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008dc280 2 [] false false map[] 0xc001f37c00 0xc0021c40b0}
Jan 20 12:47:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:19.493319  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:19.582127  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:47:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:19.587935  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:47:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:19.587947  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:47:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:19.588671  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:47:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:19.589169  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:47:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:19.589216  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:47:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:19.589223  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:47:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:19.589232  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:47:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:20.484199  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:20.484267  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:20.492260  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[b0685d5d-6469-4afe-ab1b-90bf8e265e5b] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:20 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0017189e0 2 [] false false map[] 0xc001a4a100 0xc000bba210}
Jan 20 12:47:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:20.492287  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:20.561952  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:47:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:20.570026  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:47:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:20.580266  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59533040Ki" capacity="65586124Ki" time="2023-01-20 12:47:20.564258392 -0500 EST m=+953.120033284"
Jan 20 12:47:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:20.580280  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64402176Ki" capacity="65061836Ki" time="2023-01-20 12:47:20.580213632 -0500 EST m=+953.135988457"
Jan 20 12:47:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:20.580286  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902001540Ki" capacity="981310056Ki" time="2023-01-20 12:47:20.564258392 -0500 EST m=+953.120033284"
Jan 20 12:47:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:20.580293  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945631" capacity="62382080" time="2023-01-20 12:47:20.564258392 -0500 EST m=+953.120033284"
Jan 20 12:47:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:20.580299  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902001540Ki" capacity="981310056Ki" time="2023-01-20 12:47:11.262671547 -0500 EST"
Jan 20 12:47:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:20.580305  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945631" capacity="62382080" time="2023-01-20 12:47:11.262671547 -0500 EST"
Jan 20 12:47:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:20.580310  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510787" capacity="511757" time="2023-01-20 12:47:20.579945441 -0500 EST m=+953.135720266"
Jan 20 12:47:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:20.580338  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:47:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:21.166143  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:47:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:21.166210  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:21.174567  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[788e19ff-8b4d-49ee-ab4f-c54fd1bc2cc6] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:21 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0020fd8c0 2 [] false false map[] 0xc0014eda00 0xc0012e13f0}
Jan 20 12:47:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:21.174596  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:21.483848  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:21.483920  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:21.492012  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[f15b730a-c8ad-4593-8ba3-751db7c3623f] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:21 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001718b80 2 [] false false map[] 0xc001a4a600 0xc001644bb0}
Jan 20 12:47:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:21.492041  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:21.581858  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:47:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:21.589254  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:47:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:21.589316  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:47:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:21.591606  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:47:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:21.593719  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:47:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:21.593943  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:47:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:21.593977  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:47:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:21.594022  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:47:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:22.484074  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:22.484156  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:22.492080  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[b51c3406-972e-4ff1-af1a-f775e89e40d3] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:22 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001486920 2 [] false false map[] 0xc001a4ac00 0xc0018ee6e0}
Jan 20 12:47:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:22.492110  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:22.689909  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:47:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:22.689978  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:22.691015  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:22 GMT]] 0xc000321d60 2 [] true false map[] 0xc001a4af00 <nil>}
Jan 20 12:47:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:22.691126  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:47:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:22.696302  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:47:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:22.696366  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:22.697473  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:22 GMT]] 0xc0017191e0 2 [] true false map[] 0xc001a4b100 <nil>}
Jan 20 12:47:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:22.697582  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:47:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:22.709809  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:47:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:22.709871  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:22.709882  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:47:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:22.709940  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:22.710902  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:22 GMT]] 0xc001486ba0 2 [] true false map[] 0xc000fcc900 <nil>}
Jan 20 12:47:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:22.710903  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:22 GMT]] 0xc001719220 2 [] true false map[] 0xc001a4b300 <nil>}
Jan 20 12:47:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:22.711003  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:47:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:22.711022  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:47:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:22.903037  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:47:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:23.483227  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:23.483291  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:23.491543  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[a64444e9-db8c-454e-b396-6c8a3d481f6f] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:23 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0009788a0 2 [] false false map[] 0xc0016aa200 0xc001c86000}
Jan 20 12:47:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:23.491576  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:23.581528  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[kube-system/kube-scheduler-zcy-z390-aorus-master]
Jan 20 12:47:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:23.581619  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:47:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:23.581757  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 updateType=0
Jan 20 12:47:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:23.581840  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3
Jan 20 12:47:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:23.581876  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/kube-scheduler-zcy-z390-aorus-master"
Jan 20 12:47:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:23.581968  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" oldPhase=Running phase=Running
Jan 20 12:47:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:23.582246  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:27 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:41 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:41 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:27 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.239.159.53 PodIPs:[{IP:10.239.159.53}] StartTime:2023-01-20 12:31:27 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:kube-scheduler State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:22 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:6 Image:k8s.gcr.io/kube-scheduler:v1.24.0 ImageID:k8s.gcr.io/kube-scheduler@sha256:db842a7c431fd51db7e1911f6d1df27a7b6b6963ceda24852b654d2cd535b776 ContainerID:containerd://13149330f3752d0ff65bcac86c7b522b2040811e815fbc87e56b40b612875d5a Started:0xc00013e54e}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:47:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:23.582609  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-scheduler-zcy-z390-aorus-master"
Jan 20 12:47:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:23.582681  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/kube-scheduler-zcy-z390-aorus-master"
Jan 20 12:47:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:23.583218  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/kube-scheduler-zcy-z390-aorus-master"
Jan 20 12:47:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:23.583382  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 isTerminal=false
Jan 20 12:47:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:23.583438  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 updateType=0
Jan 20 12:47:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:23.587076  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:47:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:23.587087  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:47:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:23.587093  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:47:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:23.587854  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:47:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:23.587929  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:47:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:23.587937  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:47:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:23.587959  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:47:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:23.651285  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Jan 20 12:47:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:24.483450  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:24.483518  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:24.491374  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[e27727bf-af62-41c5-af26-f395667fbbe9] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:24 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001719ea0 2 [] false false map[] 0xc000fccc00 0xc001e488f0}
Jan 20 12:47:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:24.491404  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:25.483384  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:25.483445  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:25.491543  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[fcbb5579-4b82-4af5-bfe1-6aa75495b8e0] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:25 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ce8740 2 [] false false map[] 0xc001846500 0xc002286000}
Jan 20 12:47:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:25.491575  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:25.581170  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:47:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:25.587054  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:47:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:25.587066  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:47:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:25.589091  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:47:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:25.589497  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:47:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:25.589545  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:47:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:25.589553  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:47:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:25.589562  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:47:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:26.164898  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:47:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:26.164917  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:26.167059  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:26 GMT] X-Content-Type-Options:[nosniff]] 0xc00123e200 2 [] false false map[] 0xc0000dd000 0xc0015f22c0}
Jan 20 12:47:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:26.167088  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:47:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:26.272279  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:47:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:26.272347  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:26.273559  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:26 GMT]] 0xc0009789e0 29 [] true false map[] 0xc0000dd400 <nil>}
Jan 20 12:47:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:26.273666  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:47:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:26.483346  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:26.483405  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:26.496642  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[a0fe8154-c2f9-4703-b2e9-845136189d5c] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:26 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e9160 2 [] false false map[] 0xc0000dd800 0xc000afe790}
Jan 20 12:47:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:26.496798  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:26.895551  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:47:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:26.895618  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:26.902561  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:26 GMT] X-Content-Type-Options:[nosniff]] 0xc0014863c0 2 [] false false map[] 0xc000fcc800 0xc00211c370}
Jan 20 12:47:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:26.902586  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.483876  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.483945  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.492418  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[4c0d5a20-54d5-49e1-99c0-40a5d82bcf31] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:27 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001486900 2 [] false false map[] 0xc0000ddd00 0xc000fd6630}
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.492446  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.525915  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/etcd.yaml"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.526699  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver.yaml"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.527788  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.528831  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-scheduler.yaml"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.529424  199956 config.go:279] "Setting pods for source" source="file"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.541825  199956 kubelet_getters.go:176] "Pod status updated" pod="kube-system/etcd-zcy-z390-aorus-master" status=Running
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.541911  199956 kubelet_getters.go:176] "Pod status updated" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" status=Running
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.541934  199956 kubelet_getters.go:176] "Pod status updated" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" status=Running
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.541953  199956 kubelet_getters.go:176] "Pod status updated" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" status=Running
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.581188  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.586989  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.587001  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.587007  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.587753  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.587801  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.587807  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.587816  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631134  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631145  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-rootfs.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631153  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631157  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2dd678d9bc\\x2d8fcb\\x2d9fbe\\x2d1142\\x2ddede54862bab.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631161  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2dd678d9bc\\x2d8fcb\\x2d9fbe\\x2d1142\\x2ddede54862bab.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631166  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2dd678d9bc\\x2d8fcb\\x2d9fbe\\x2d1142\\x2ddede54862bab.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631170  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-user-sessions.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631173  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-user-sessions.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631177  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-user-sessions.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631181  199956 manager.go:925] ignoring container "/system.slice/systemd-user-sessions.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631184  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-user-1000-gvfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631187  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-user-1000-gvfs.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631191  199956 manager.go:925] ignoring container "/system.slice/run-user-1000-gvfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631194  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631198  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-rootfs.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631203  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631207  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-machine-id-commit.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631210  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-machine-id-commit.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631213  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-machine-id-commit.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631217  199956 manager.go:925] ignoring container "/system.slice/systemd-machine-id-commit.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631220  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/cups.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631223  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/cups.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631226  199956 factory.go:255] Factory "raw" can handle container "/system.slice/cups.socket", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631230  199956 manager.go:925] ignoring container "/system.slice/cups.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631233  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/ufw.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631235  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/ufw.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631239  199956 factory.go:255] Factory "raw" can handle container "/system.slice/ufw.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631243  199956 manager.go:925] ignoring container "/system.slice/ufw.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631246  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631249  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-rootfs.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631255  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631258  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631261  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631264  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631268  199956 manager.go:925] ignoring container "/user.slice/user-0.slice"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631270  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-1000.slice/user-runtime-dir@1000.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631273  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-1000.slice/user-runtime-dir@1000.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631277  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-1000.slice/user-runtime-dir@1000.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631281  199956 manager.go:925] ignoring container "/user.slice/user-1000.slice/user-runtime-dir@1000.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631284  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-0f9ae677fe935afcf43e145281deae0d663ba95fda6759ff24f0dd3e1cee17a9-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631288  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-0f9ae677fe935afcf43e145281deae0d663ba95fda6759ff24f0dd3e1cee17a9-rootfs.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631293  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-0f9ae677fe935afcf43e145281deae0d663ba95fda6759ff24f0dd3e1cee17a9-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631297  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/-.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631300  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/-.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631304  199956 manager.go:925] ignoring container "/system.slice/-.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631306  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/blk-availability.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631309  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/blk-availability.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631312  199956 factory.go:255] Factory "raw" can handle container "/system.slice/blk-availability.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631317  199956 manager.go:925] ignoring container "/system.slice/blk-availability.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631319  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/thermald.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631322  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/thermald.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631325  199956 factory.go:255] Factory "raw" can handle container "/system.slice/thermald.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631329  199956 manager.go:925] ignoring container "/system.slice/thermald.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631332  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dev-mqueue.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631335  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/dev-mqueue.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631339  199956 manager.go:925] ignoring container "/system.slice/dev-mqueue.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631341  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-fsckd.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631344  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-fsckd.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631347  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-fsckd.socket", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631352  199956 manager.go:925] ignoring container "/system.slice/systemd-fsckd.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631355  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-resolved.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631358  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-resolved.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631361  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-resolved.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631365  199956 manager.go:925] ignoring container "/system.slice/systemd-resolved.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631368  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-lvm2\\x2dpvscan.slice/lvm2-pvscan@8:10.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631371  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-lvm2\\x2dpvscan.slice/lvm2-pvscan@8:10.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631375  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-lvm2\\x2dpvscan.slice/lvm2-pvscan@8:10.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631379  199956 manager.go:925] ignoring container "/system.slice/system-lvm2\\x2dpvscan.slice/lvm2-pvscan@8:10.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631382  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f8443d2868215ec42d3fa335663976e33df166c5e98369aa3f9d7ddb9235bead-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631386  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f8443d2868215ec42d3fa335663976e33df166c5e98369aa3f9d7ddb9235bead-rootfs.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631391  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f8443d2868215ec42d3fa335663976e33df166c5e98369aa3f9d7ddb9235bead-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631395  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e10efaa459a32161059fda75d4bcaf3bbdb896781f772b508e43fe00bc7c9003-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631399  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e10efaa459a32161059fda75d4bcaf3bbdb896781f772b508e43fe00bc7c9003-rootfs.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631404  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e10efaa459a32161059fda75d4bcaf3bbdb896781f772b508e43fe00bc7c9003-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631408  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/whoopsie.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631410  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/whoopsie.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631413  199956 factory.go:255] Factory "raw" can handle container "/system.slice/whoopsie.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631417  199956 manager.go:925] ignoring container "/system.slice/whoopsie.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631420  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/rtkit-daemon.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631423  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/rtkit-daemon.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631426  199956 factory.go:255] Factory "raw" can handle container "/system.slice/rtkit-daemon.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631430  199956 manager.go:925] ignoring container "/system.slice/rtkit-daemon.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631433  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-user-0.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631436  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-user-0.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631440  199956 manager.go:925] ignoring container "/system.slice/run-user-0.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631443  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/networkd-dispatcher.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631446  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/networkd-dispatcher.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631449  199956 factory.go:255] Factory "raw" can handle container "/system.slice/networkd-dispatcher.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631453  199956 manager.go:925] ignoring container "/system.slice/networkd-dispatcher.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631456  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631460  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-rootfs.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631465  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631468  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/NetworkManager-wait-online.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631471  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/NetworkManager-wait-online.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631475  199956 factory.go:255] Factory "raw" can handle container "/system.slice/NetworkManager-wait-online.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631479  199956 manager.go:925] ignoring container "/system.slice/NetworkManager-wait-online.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631481  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631484  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631487  199956 factory.go:255] Factory "raw" can handle container "/system.slice", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631491  199956 manager.go:925] ignoring container "/system.slice"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631493  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/switcheroo-control.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631496  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/switcheroo-control.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631499  199956 factory.go:255] Factory "raw" can handle container "/system.slice/switcheroo-control.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631503  199956 manager.go:925] ignoring container "/system.slice/switcheroo-control.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631506  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2d719b045b\\x2df019\\x2d025c\\x2d185d\\x2da976163f375a.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631510  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2d719b045b\\x2df019\\x2d025c\\x2d185d\\x2da976163f375a.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631514  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2d719b045b\\x2df019\\x2d025c\\x2d185d\\x2da976163f375a.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631518  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-1000.slice/user@1000.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631521  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-1000.slice/user@1000.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631524  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-1000.slice/user@1000.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631528  199956 manager.go:925] ignoring container "/user.slice/user-1000.slice/user@1000.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631531  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/wpa_supplicant.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631534  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/wpa_supplicant.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631537  199956 factory.go:255] Factory "raw" can handle container "/system.slice/wpa_supplicant.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631542  199956 manager.go:925] ignoring container "/system.slice/wpa_supplicant.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631544  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/kerneloops.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631547  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/kerneloops.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631550  199956 factory.go:255] Factory "raw" can handle container "/system.slice/kerneloops.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631554  199956 manager.go:925] ignoring container "/system.slice/kerneloops.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631557  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-logind.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631559  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-logind.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631563  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-logind.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631566  199956 manager.go:925] ignoring container "/system.slice/systemd-logind.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631569  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-d94e1927\\x2dd22d\\x2d4831\\x2da764\\x2d0170eae346a1-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d9qh7j.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631574  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-d94e1927\\x2dd22d\\x2d4831\\x2da764\\x2d0170eae346a1-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d9qh7j.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631579  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-d94e1927\\x2dd22d\\x2d4831\\x2da764\\x2d0170eae346a1-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d9qh7j.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631583  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-snapd-ns.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631587  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-snapd-ns.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631591  199956 manager.go:925] ignoring container "/system.slice/run-snapd-ns.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631594  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-shm.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631598  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-shm.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631602  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-shm.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631606  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-1c057a0e\\x2d3ee3\\x2d44f3\\x2db294\\x2d434acc8f9491-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d2sbqp.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631611  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-1c057a0e\\x2d3ee3\\x2d44f3\\x2db294\\x2d434acc8f9491-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d2sbqp.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631617  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-1c057a0e\\x2d3ee3\\x2d44f3\\x2db294\\x2d434acc8f9491-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d2sbqp.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631621  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/alsa-restore.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631624  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/alsa-restore.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631627  199956 factory.go:255] Factory "raw" can handle container "/system.slice/alsa-restore.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631631  199956 manager.go:925] ignoring container "/system.slice/alsa-restore.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631634  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-1cb44bdbde0b41852e382d7dab7c184af8dd4e5b25d5cfe6a10a9c8ab601659d-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631638  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-1cb44bdbde0b41852e382d7dab7c184af8dd4e5b25d5cfe6a10a9c8ab601659d-rootfs.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631643  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-1cb44bdbde0b41852e382d7dab7c184af8dd4e5b25d5cfe6a10a9c8ab601659d-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631646  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/session-40.scope"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631649  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/session-40.scope"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631652  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/session-40.scope", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631656  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/session-40.scope"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631660  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-shm.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631664  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-shm.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631668  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-shm.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631672  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-fs-fuse-connections.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631675  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-fs-fuse-connections.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631679  199956 manager.go:925] ignoring container "/system.slice/sys-fs-fuse-connections.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631682  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/boot-efi.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631686  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/boot-efi.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631690  199956 manager.go:925] ignoring container "/system.slice/boot-efi.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631692  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/acpid.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631695  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/acpid.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631698  199956 factory.go:255] Factory "raw" can handle container "/system.slice/acpid.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631702  199956 manager.go:925] ignoring container "/system.slice/acpid.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631705  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dbus.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631707  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/dbus.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631710  199956 factory.go:255] Factory "raw" can handle container "/system.slice/dbus.socket", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631714  199956 manager.go:925] ignoring container "/system.slice/dbus.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631717  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/keyboard-setup.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631720  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/keyboard-setup.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631723  199956 factory.go:255] Factory "raw" can handle container "/system.slice/keyboard-setup.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631727  199956 manager.go:925] ignoring container "/system.slice/keyboard-setup.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631730  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/openvpn.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631732  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/openvpn.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631736  199956 factory.go:255] Factory "raw" can handle container "/system.slice/openvpn.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631739  199956 manager.go:925] ignoring container "/system.slice/openvpn.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631742  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/cron.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631745  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/cron.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631748  199956 factory.go:255] Factory "raw" can handle container "/system.slice/cron.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631752  199956 manager.go:925] ignoring container "/system.slice/cron.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631755  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/gdm.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631757  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/gdm.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631761  199956 factory.go:255] Factory "raw" can handle container "/system.slice/gdm.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631765  199956 manager.go:925] ignoring container "/system.slice/gdm.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631768  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-shm.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631772  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-shm.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631777  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-shm.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631780  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f78ed441ff85d669a403a76c0987f2d86913431bd96d454b1a3605bdcf0474f2-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631784  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f78ed441ff85d669a403a76c0987f2d86913431bd96d454b1a3605bdcf0474f2-rootfs.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631790  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f78ed441ff85d669a403a76c0987f2d86913431bd96d454b1a3605bdcf0474f2-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631793  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journald.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631796  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journald.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631799  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journald.socket", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631803  199956 manager.go:925] ignoring container "/system.slice/systemd-journald.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631806  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/docker.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631808  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/docker.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631812  199956 factory.go:255] Factory "raw" can handle container "/system.slice/docker.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631815  199956 manager.go:925] ignoring container "/system.slice/docker.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631818  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-rfkill.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631821  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-rfkill.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631824  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-rfkill.socket", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631828  199956 manager.go:925] ignoring container "/system.slice/systemd-rfkill.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631831  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/ssh.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631833  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/ssh.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631836  199956 factory.go:255] Factory "raw" can handle container "/system.slice/ssh.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631840  199956 manager.go:925] ignoring container "/system.slice/ssh.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631843  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/syslog.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631846  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/syslog.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631849  199956 factory.go:255] Factory "raw" can handle container "/system.slice/syslog.socket", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631853  199956 manager.go:925] ignoring container "/system.slice/syslog.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631855  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/colord.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631858  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/colord.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631861  199956 factory.go:255] Factory "raw" can handle container "/system.slice/colord.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631865  199956 manager.go:925] ignoring container "/system.slice/colord.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631868  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dm-event.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631871  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/dm-event.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631874  199956 factory.go:255] Factory "raw" can handle container "/system.slice/dm-event.socket", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631878  199956 manager.go:925] ignoring container "/system.slice/dm-event.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631881  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/user@0.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631884  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/user@0.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631887  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/user@0.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631891  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/user@0.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631894  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/lvm2-monitor.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631896  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/lvm2-monitor.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631899  199956 factory.go:255] Factory "raw" can handle container "/system.slice/lvm2-monitor.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631903  199956 manager.go:925] ignoring container "/system.slice/lvm2-monitor.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631906  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-9b11acac1b891eafc7ff2b244f1c42938f71a575ce81ff917e3b7ebf61d2e045-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631911  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-9b11acac1b891eafc7ff2b244f1c42938f71a575ce81ff917e3b7ebf61d2e045-rootfs.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631915  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-9b11acac1b891eafc7ff2b244f1c42938f71a575ce81ff917e3b7ebf61d2e045-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631919  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-snapd-16292.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631922  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-snapd-16292.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631926  199956 manager.go:925] ignoring container "/system.slice/snap-snapd-16292.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631929  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-core20-1611.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631932  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-core20-1611.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631936  199956 manager.go:925] ignoring container "/system.slice/snap-core20-1611.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631939  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snapd.seeded.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631942  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/snapd.seeded.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631945  199956 factory.go:255] Factory "raw" can handle container "/system.slice/snapd.seeded.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631949  199956 manager.go:925] ignoring container "/system.slice/snapd.seeded.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631951  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/irqbalance.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631954  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/irqbalance.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631957  199956 factory.go:255] Factory "raw" can handle container "/system.slice/irqbalance.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631961  199956 manager.go:925] ignoring container "/system.slice/irqbalance.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631964  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-kernel-config.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631967  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-kernel-config.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631972  199956 manager.go:925] ignoring container "/system.slice/sys-kernel-config.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631975  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-remount-fs.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631978  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-remount-fs.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631981  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-remount-fs.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631985  199956 manager.go:925] ignoring container "/system.slice/systemd-remount-fs.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631988  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631992  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-rootfs.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.631997  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632001  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2d562f9e70\\x2db2b7\\x2d3ac4\\x2d2311\\x2db91510a5a9ac.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632005  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2d562f9e70\\x2db2b7\\x2d3ac4\\x2d2311\\x2db91510a5a9ac.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632009  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2d562f9e70\\x2db2b7\\x2d3ac4\\x2d2311\\x2db91510a5a9ac.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632012  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-shm.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632016  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-shm.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632021  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-shm.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632025  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-systemd\\x2dfsck.slice/systemd-fsck@dev-disk-by\\x2duuid-4B5A\\x2dDC89.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632029  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-systemd\\x2dfsck.slice/systemd-fsck@dev-disk-by\\x2duuid-4B5A\\x2dDC89.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632032  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-systemd\\x2dfsck.slice/systemd-fsck@dev-disk-by\\x2duuid-4B5A\\x2dDC89.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632037  199956 manager.go:925] ignoring container "/system.slice/system-systemd\\x2dfsck.slice/systemd-fsck@dev-disk-by\\x2duuid-4B5A\\x2dDC89.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632040  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-udevd-control.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632043  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-udevd-control.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632046  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-udevd-control.socket", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632050  199956 manager.go:925] ignoring container "/system.slice/systemd-udevd-control.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632053  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-udevd.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632055  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-udevd.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632059  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-udevd.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632063  199956 manager.go:925] ignoring container "/system.slice/systemd-udevd.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632065  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-timesyncd.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632068  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-timesyncd.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632072  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-timesyncd.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632076  199956 manager.go:925] ignoring container "/system.slice/systemd-timesyncd.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632079  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/user@0.service/dbus.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632082  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/user@0.service/dbus.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632085  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/user@0.service/dbus.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632089  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/user@0.service/dbus.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632092  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-initctl.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632095  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-initctl.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632098  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-initctl.socket", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632102  199956 manager.go:925] ignoring container "/system.slice/systemd-initctl.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632105  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-1000.slice/session-1.scope"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632107  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-1000.slice/session-1.scope"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632111  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-1000.slice/session-1.scope", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632115  199956 manager.go:925] ignoring container "/user.slice/user-1000.slice/session-1.scope"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632117  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-shm.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632121  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-shm.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632126  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-shm.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632130  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/bluetooth.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632133  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/bluetooth.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632136  199956 factory.go:255] Factory "raw" can handle container "/system.slice/bluetooth.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632140  199956 manager.go:925] ignoring container "/system.slice/bluetooth.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632143  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632147  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-rootfs.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632152  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632156  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-tmpfiles-setup-dev.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632159  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-tmpfiles-setup-dev.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632162  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-tmpfiles-setup-dev.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632166  199956 manager.go:925] ignoring container "/system.slice/systemd-tmpfiles-setup-dev.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632169  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632172  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632175  199956 factory.go:255] Factory "raw" can handle container "/user.slice", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632178  199956 manager.go:925] ignoring container "/user.slice"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632181  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2d58469e7a\\x2dc331\\x2d785c\\x2dc760\\x2d93c7232334bd.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632185  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2d58469e7a\\x2dc331\\x2d785c\\x2dc760\\x2d93c7232334bd.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632189  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2d58469e7a\\x2dc331\\x2d785c\\x2dc760\\x2d93c7232334bd.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632193  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/accounts-daemon.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632195  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/accounts-daemon.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632198  199956 factory.go:255] Factory "raw" can handle container "/system.slice/accounts-daemon.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632202  199956 manager.go:925] ignoring container "/system.slice/accounts-daemon.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632205  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/unattended-upgrades.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632208  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/unattended-upgrades.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632211  199956 factory.go:255] Factory "raw" can handle container "/system.slice/unattended-upgrades.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632216  199956 manager.go:925] ignoring container "/system.slice/unattended-upgrades.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632219  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/avahi-daemon.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632221  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/avahi-daemon.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632225  199956 factory.go:255] Factory "raw" can handle container "/system.slice/avahi-daemon.socket", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632228  199956 manager.go:925] ignoring container "/system.slice/avahi-daemon.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632232  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snapd.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632234  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/snapd.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632237  199956 factory.go:255] Factory "raw" can handle container "/system.slice/snapd.socket", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632241  199956 manager.go:925] ignoring container "/system.slice/snapd.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632244  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-sysusers.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632247  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-sysusers.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632250  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-sysusers.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632254  199956 manager.go:925] ignoring container "/system.slice/systemd-sysusers.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632257  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-shm.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632261  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-shm.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632266  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-shm.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632270  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-snap\\x2dstore-558.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632273  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-snap\\x2dstore-558.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632277  199956 manager.go:925] ignoring container "/system.slice/snap-snap\\x2dstore-558.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632280  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-udev-trigger.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632283  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-udev-trigger.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632286  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-udev-trigger.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632290  199956 manager.go:925] ignoring container "/system.slice/systemd-udev-trigger.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632293  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-gnome\\x2d3\\x2d38\\x2d2004-115.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632296  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-gnome\\x2d3\\x2d38\\x2d2004-115.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632300  199956 manager.go:925] ignoring container "/system.slice/snap-gnome\\x2d3\\x2d38\\x2d2004-115.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632303  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/ModemManager.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632306  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/ModemManager.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632309  199956 factory.go:255] Factory "raw" can handle container "/system.slice/ModemManager.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632313  199956 manager.go:925] ignoring container "/system.slice/ModemManager.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632316  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-user-1000.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632319  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-user-1000.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632323  199956 manager.go:925] ignoring container "/system.slice/run-user-1000.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632326  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-7af065b7\\x2d9095\\x2d4d91\\x2d9b9e\\x2d2644e7b1f4bf-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dx6vjr.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632330  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-7af065b7\\x2d9095\\x2d4d91\\x2d9b9e\\x2d2644e7b1f4bf-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dx6vjr.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632336  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-7af065b7\\x2d9095\\x2d4d91\\x2d9b9e\\x2d2644e7b1f4bf-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dx6vjr.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632340  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snapd.apparmor.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632343  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/snapd.apparmor.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632346  199956 factory.go:255] Factory "raw" can handle container "/system.slice/snapd.apparmor.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632350  199956 manager.go:925] ignoring container "/system.slice/snapd.apparmor.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632353  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-shm.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632357  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-shm.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632362  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-shm.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632365  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-1000.slice"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632368  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-1000.slice"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632371  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-1000.slice", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632375  199956 manager.go:925] ignoring container "/user.slice/user-1000.slice"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632378  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-udevd-kernel.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632380  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-udevd-kernel.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632384  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-udevd-kernel.socket", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632388  199956 manager.go:925] ignoring container "/system.slice/systemd-udevd-kernel.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632390  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-04e472cb\\x2db6f9\\x2d4065\\x2db509\\x2dd7e1579fc48d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dhqj8d.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632395  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-04e472cb\\x2db6f9\\x2d4065\\x2db509\\x2dd7e1579fc48d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dhqj8d.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632400  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-04e472cb\\x2db6f9\\x2d4065\\x2db509\\x2dd7e1579fc48d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dhqj8d.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632404  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b3af3b285c950aba4fcd0176473261f7f4700aeaafe9c73ae15b15a7d6304092-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632408  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b3af3b285c950aba4fcd0176473261f7f4700aeaafe9c73ae15b15a7d6304092-rootfs.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632413  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b3af3b285c950aba4fcd0176473261f7f4700aeaafe9c73ae15b15a7d6304092-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632417  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/apparmor.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632420  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/apparmor.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632423  199956 factory.go:255] Factory "raw" can handle container "/system.slice/apparmor.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632427  199956 manager.go:925] ignoring container "/system.slice/apparmor.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632429  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/session-44.scope"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632432  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/session-44.scope"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632435  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/session-44.scope", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632439  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/session-44.scope"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632442  199956 factory.go:262] Factory "containerd" was unable to handle container "/init.scope"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632445  199956 factory.go:262] Factory "systemd" was unable to handle container "/init.scope"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632448  199956 factory.go:255] Factory "raw" can handle container "/init.scope", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632452  199956 manager.go:925] ignoring container "/init.scope"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632454  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-cf25b9ea\\x2d6823\\x2d4eff\\x2db30d\\x2d36a09f9d897e-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dtqzsm.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632459  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-cf25b9ea\\x2d6823\\x2d4eff\\x2db30d\\x2d36a09f9d897e-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dtqzsm.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632464  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-cf25b9ea\\x2d6823\\x2d4eff\\x2db30d\\x2d36a09f9d897e-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dtqzsm.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632468  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dev-hugepages.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632471  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/dev-hugepages.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632476  199956 manager.go:925] ignoring container "/system.slice/dev-hugepages.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632478  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dbus.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632481  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/dbus.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632484  199956 factory.go:255] Factory "raw" can handle container "/system.slice/dbus.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632488  199956 manager.go:925] ignoring container "/system.slice/dbus.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632491  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/console-setup.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632494  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/console-setup.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632497  199956 factory.go:255] Factory "raw" can handle container "/system.slice/console-setup.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632501  199956 manager.go:925] ignoring container "/system.slice/console-setup.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632503  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/lvm2-lvmpolld.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632506  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/lvm2-lvmpolld.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632509  199956 factory.go:255] Factory "raw" can handle container "/system.slice/lvm2-lvmpolld.socket", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632513  199956 manager.go:925] ignoring container "/system.slice/lvm2-lvmpolld.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632516  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-lvm2\\x2dpvscan.slice"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632518  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-lvm2\\x2dpvscan.slice"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632521  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-lvm2\\x2dpvscan.slice", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632525  199956 manager.go:925] ignoring container "/system.slice/system-lvm2\\x2dpvscan.slice"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632528  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e0c5b991f81d5128566686552e45a9bbc8c584e4f6c94909edb3a42720dacc90-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632532  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e0c5b991f81d5128566686552e45a9bbc8c584e4f6c94909edb3a42720dacc90-rootfs.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632537  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e0c5b991f81d5128566686552e45a9bbc8c584e4f6c94909edb3a42720dacc90-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632541  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632545  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-rootfs.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632550  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632553  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/setvtrgb.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632556  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/setvtrgb.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632559  199956 factory.go:255] Factory "raw" can handle container "/system.slice/setvtrgb.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632563  199956 manager.go:925] ignoring container "/system.slice/setvtrgb.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632566  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-systemd\\x2dfsck.slice"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632569  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-systemd\\x2dfsck.slice"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632573  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-systemd\\x2dfsck.slice", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632577  199956 manager.go:925] ignoring container "/system.slice/system-systemd\\x2dfsck.slice"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632580  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/acpid.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632583  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/acpid.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632586  199956 factory.go:255] Factory "raw" can handle container "/system.slice/acpid.socket", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632590  199956 manager.go:925] ignoring container "/system.slice/acpid.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632592  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-shm.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632596  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-shm.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632601  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-shm.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632605  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-random-seed.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632608  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-random-seed.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632611  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-random-seed.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632615  199956 manager.go:925] ignoring container "/system.slice/systemd-random-seed.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632618  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/rsyslog.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632621  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/rsyslog.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632624  199956 factory.go:255] Factory "raw" can handle container "/system.slice/rsyslog.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632628  199956 manager.go:925] ignoring container "/system.slice/rsyslog.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632630  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/user@0.service/dbus.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632633  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/user@0.service/dbus.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632636  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/user@0.service/dbus.socket", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632641  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/user@0.service/dbus.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632643  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/cups.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632646  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/cups.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632649  199956 factory.go:255] Factory "raw" can handle container "/system.slice/cups.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632653  199956 manager.go:925] ignoring container "/system.slice/cups.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632655  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-d2688d45\\x2d2487\\x2d46e7\\x2daecb\\x2de3479626909d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d4pnfq.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632660  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-d2688d45\\x2d2487\\x2d46e7\\x2daecb\\x2de3479626909d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d4pnfq.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632665  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-d2688d45\\x2d2487\\x2d46e7\\x2daecb\\x2de3479626909d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d4pnfq.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632669  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-snapd-ns-snap\\x2dstore.mnt.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632672  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-snapd-ns-snap\\x2dstore.mnt.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632677  199956 manager.go:925] ignoring container "/system.slice/run-snapd-ns-snap\\x2dstore.mnt.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632680  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-shm.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632683  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-shm.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632695  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-shm.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632699  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-bare-5.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632703  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-bare-5.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632707  199956 manager.go:925] ignoring container "/system.slice/snap-bare-5.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632710  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-sysctl.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632713  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-sysctl.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632716  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-sysctl.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632720  199956 manager.go:925] ignoring container "/system.slice/systemd-sysctl.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632723  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/kmod-static-nodes.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632726  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/kmod-static-nodes.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632729  199956 factory.go:255] Factory "raw" can handle container "/system.slice/kmod-static-nodes.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632733  199956 manager.go:925] ignoring container "/system.slice/kmod-static-nodes.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632736  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632740  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-rootfs.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632745  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632749  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/udisks2.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632751  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/udisks2.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632754  199956 factory.go:255] Factory "raw" can handle container "/system.slice/udisks2.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632758  199956 manager.go:925] ignoring container "/system.slice/udisks2.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632761  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-b0713fbc\\x2defc5\\x2d4044\\x2d9d08\\x2d2326a0752f87-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dgcgm6.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632766  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-b0713fbc\\x2defc5\\x2d4044\\x2d9d08\\x2d2326a0752f87-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dgcgm6.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632772  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-b0713fbc\\x2defc5\\x2d4044\\x2d9d08\\x2d2326a0752f87-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dgcgm6.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632776  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-getty.slice"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632779  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-getty.slice"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632782  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-getty.slice", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632786  199956 manager.go:925] ignoring container "/system.slice/system-getty.slice"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632788  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/session-42.scope"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632791  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/session-42.scope"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632794  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/session-42.scope", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632798  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/session-42.scope"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632801  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journald-audit.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632804  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journald-audit.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632807  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journald-audit.socket", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632811  199956 manager.go:925] ignoring container "/system.slice/systemd-journald-audit.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632814  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-modules-load.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632817  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-modules-load.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632820  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-modules-load.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632824  199956 manager.go:925] ignoring container "/system.slice/systemd-modules-load.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632827  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-186424b47c7b9022ecfe8996dc73cd9101f7539259cd65914279c84c9a02a288-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632831  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-186424b47c7b9022ecfe8996dc73cd9101f7539259cd65914279c84c9a02a288-rootfs.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632836  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-186424b47c7b9022ecfe8996dc73cd9101f7539259cd65914279c84c9a02a288-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632840  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/user-runtime-dir@0.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632843  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/user-runtime-dir@0.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632846  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/user-runtime-dir@0.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632850  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/user-runtime-dir@0.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632853  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-kernel-debug.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632856  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-kernel-debug.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632860  199956 manager.go:925] ignoring container "/system.slice/sys-kernel-debug.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632863  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e489181a7f95431b6d92f037dbe3f788b46a5e024df7aaf29e0115dab1168ab1-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632867  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e489181a7f95431b6d92f037dbe3f788b46a5e024df7aaf29e0115dab1168ab1-rootfs.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632872  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e489181a7f95431b6d92f037dbe3f788b46a5e024df7aaf29e0115dab1168ab1-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632876  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-modprobe.slice"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632879  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-modprobe.slice"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632882  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-modprobe.slice", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632886  199956 manager.go:925] ignoring container "/system.slice/system-modprobe.slice"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632888  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/cups-browsed.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632891  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/cups-browsed.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632894  199956 factory.go:255] Factory "raw" can handle container "/system.slice/cups-browsed.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632898  199956 manager.go:925] ignoring container "/system.slice/cups-browsed.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632901  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-gtk\\x2dcommon\\x2dthemes-1535.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632904  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-gtk\\x2dcommon\\x2dthemes-1535.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632909  199956 manager.go:925] ignoring container "/system.slice/snap-gtk\\x2dcommon\\x2dthemes-1535.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632911  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/avahi-daemon.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632914  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/avahi-daemon.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632917  199956 factory.go:255] Factory "raw" can handle container "/system.slice/avahi-daemon.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632921  199956 manager.go:925] ignoring container "/system.slice/avahi-daemon.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632924  199956 factory.go:262] Factory "containerd" was unable to handle container "/kata_overhead"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632926  199956 factory.go:262] Factory "systemd" was unable to handle container "/kata_overhead"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632929  199956 factory.go:255] Factory "raw" can handle container "/kata_overhead", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632933  199956 manager.go:925] ignoring container "/kata_overhead"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632936  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2d8b7da23a\\x2d3511\\x2dfe06\\x2d72d7\\x2d26a549eb607d.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632939  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2d8b7da23a\\x2d3511\\x2dfe06\\x2d72d7\\x2d26a549eb607d.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632944  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2d8b7da23a\\x2d3511\\x2dfe06\\x2d72d7\\x2d26a549eb607d.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632947  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/polkit.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632950  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/polkit.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632953  199956 factory.go:255] Factory "raw" can handle container "/system.slice/polkit.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632957  199956 manager.go:925] ignoring container "/system.slice/polkit.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632960  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/NetworkManager.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632963  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/NetworkManager.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632966  199956 factory.go:255] Factory "raw" can handle container "/system.slice/NetworkManager.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632970  199956 manager.go:925] ignoring container "/system.slice/NetworkManager.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632973  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632977  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-rootfs.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632981  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632985  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journal-flush.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632988  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journal-flush.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632991  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journal-flush.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632995  199956 manager.go:925] ignoring container "/system.slice/systemd-journal-flush.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.632998  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-5900ac5c1383a321a6ae837b57d6d29d7a660a102c0806210a9ea57290673062-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633002  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-5900ac5c1383a321a6ae837b57d6d29d7a660a102c0806210a9ea57290673062-rootfs.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633007  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-5900ac5c1383a321a6ae837b57d6d29d7a660a102c0806210a9ea57290673062-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633011  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-kernel-tracing.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633014  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-kernel-tracing.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633019  199956 manager.go:925] ignoring container "/system.slice/sys-kernel-tracing.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633021  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/proc-sys-fs-binfmt_misc.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633024  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/proc-sys-fs-binfmt_misc.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633028  199956 manager.go:925] ignoring container "/system.slice/proc-sys-fs-binfmt_misc.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633031  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-shm.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633035  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-shm.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633040  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-shm.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633044  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-13149330f3752d0ff65bcac86c7b522b2040811e815fbc87e56b40b612875d5a-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633048  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-13149330f3752d0ff65bcac86c7b522b2040811e815fbc87e56b40b612875d5a-rootfs.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633052  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-13149330f3752d0ff65bcac86c7b522b2040811e815fbc87e56b40b612875d5a-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633056  199956 factory.go:262] Factory "containerd" was unable to handle container "/docker"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633058  199956 factory.go:262] Factory "systemd" was unable to handle container "/docker"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633062  199956 factory.go:255] Factory "raw" can handle container "/docker", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633065  199956 manager.go:925] ignoring container "/docker"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633068  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journald.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633071  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journald.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633074  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journald.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633078  199956 manager.go:925] ignoring container "/system.slice/systemd-journald.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633081  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/uuidd.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633083  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/uuidd.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633087  199956 factory.go:255] Factory "raw" can handle container "/system.slice/uuidd.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633091  199956 manager.go:925] ignoring container "/system.slice/uuidd.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633094  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/uuidd.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633097  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/uuidd.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633100  199956 factory.go:255] Factory "raw" can handle container "/system.slice/uuidd.socket", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633104  199956 manager.go:925] ignoring container "/system.slice/uuidd.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633106  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-kernel-debug-tracing.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633110  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-kernel-debug-tracing.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633114  199956 manager.go:925] ignoring container "/system.slice/sys-kernel-debug-tracing.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633117  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/upower.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633119  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/upower.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633122  199956 factory.go:255] Factory "raw" can handle container "/system.slice/upower.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633126  199956 manager.go:925] ignoring container "/system.slice/upower.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633129  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/containerd.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633131  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/containerd.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633134  199956 factory.go:255] Factory "raw" can handle container "/system.slice/containerd.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633139  199956 manager.go:925] ignoring container "/system.slice/containerd.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633142  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633146  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-rootfs.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633150  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633154  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-shm.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633158  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-shm.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633163  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-shm.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633166  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-tmpfiles-setup.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633169  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-tmpfiles-setup.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633172  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-tmpfiles-setup.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633176  199956 manager.go:925] ignoring container "/system.slice/systemd-tmpfiles-setup.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633179  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snapd.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633182  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/snapd.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633185  199956 factory.go:255] Factory "raw" can handle container "/system.slice/snapd.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633189  199956 manager.go:925] ignoring container "/system.slice/snapd.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633192  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633196  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-rootfs.mount", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633201  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-rootfs.mount"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633204  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/apport.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633207  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/apport.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633210  199956 factory.go:255] Factory "raw" can handle container "/system.slice/apport.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633214  199956 manager.go:925] ignoring container "/system.slice/apport.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633217  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/docker.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633220  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/docker.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633224  199956 factory.go:255] Factory "raw" can handle container "/system.slice/docker.socket", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633228  199956 manager.go:925] ignoring container "/system.slice/docker.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633231  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-update-utmp.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633234  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-update-utmp.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633237  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-update-utmp.service", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633241  199956 manager.go:925] ignoring container "/system.slice/systemd-update-utmp.service"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633244  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journald-dev-log.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633247  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journald-dev-log.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633250  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journald-dev-log.socket", but ignoring.
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.633254  199956 manager.go:925] ignoring container "/system.slice/systemd-journald-dev-log.socket"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.648917  199956 qos_container_manager_linux.go:379] "Updated QoS cgroup configuration"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.720658  199956 kuberuntime_gc.go:171] "Removing sandbox" sandboxID="3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:47:27.721135548-05:00" level=info msg="StopPodSandbox for \"3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af\""
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:47:27.733171000-05:00" level=info msg="TearDown network for sandbox \"3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af\" successfully"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:47:27.733239274-05:00" level=info msg="StopPodSandbox for \"3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af\" returns successfully"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:47:27.733933612-05:00" level=info msg="RemovePodSandbox for \"3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af\""
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:47:27.734031992-05:00" level=info msg="Forcibly stopping sandbox \"3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af\""
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:47:27.745757740-05:00" level=info msg="TearDown network for sandbox \"3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af\" successfully"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:47:27.751649586-05:00" level=info msg="RemovePodSandbox \"3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af\" returns successfully"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.753099  199956 kubelet.go:1280] "Container garbage collection succeeded"
Jan 20 12:47:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:27.904379  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.255717  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.255726  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.255887  199956 interface.go:209] Interface eno2 is up
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.255924  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.255932  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.255937  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.255940  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.255944  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.256222  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.256232  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.256237  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.256241  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.295357  199956 generic.go:155] "GenericPLEG" podUID=4f97314d-815b-4787-b174-5c158cd28c9d containerID="3ed76f963cd74e0749f7d7c6b6d26685b4e0abd10181738afbe786f27e1703af" oldState=exited newState=non-existent
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.295715  199956 kuberuntime_manager.go:1037] "getSandboxIDByPodUID got sandbox IDs for pod" podSandboxID=[08841bf63919f9c8acff77c51620bd8e684c550feb6b3e8784239bc3d9079457] pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.296237  199956 generic.go:421] "PLEG: Write status" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.484028  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.484096  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.492024  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[f8d892e9-4dbc-434a-bfe2-a93f7e161969] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:28 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000cb2b20 2 [] false false map[] 0xc0017b8700 0xc0018956b0}
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.492055  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.581380  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[default/unsigned-unencrypted-cc-1]
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.581466  199956 pod_workers.go:888] "Processing pod event" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.581526  199956 kubelet.go:1501] "syncPod enter" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.581557  199956 kubelet_pods.go:1435] "Generating pod status" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.581648  199956 kubelet_pods.go:1447] "Got phase for pod" pod="default/unsigned-unencrypted-cc-1" oldPhase=Pending phase=Pending
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.581935  199956 status_manager.go:535] "Ignoring same status for pod" pod="default/unsigned-unencrypted-cc-1" status={Phase:Pending Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.16 PodIPs:[{IP:10.244.0.16}] StartTime:2023-01-20 12:41:08 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:ci-example256m State:{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "ngcn-registry.sh.intel.com:443/ci-example256m:latest",} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest ImageID: ContainerID: Started:0xc00147b5be}] QOSClass:Guaranteed EphemeralContainerStatuses:[]}
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.582246  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.582313  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.582362  199956 kuberuntime_manager.go:638] "Container of pod is not in the desired state and shall be started" containerName="ci-example256m" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.582403  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:08841bf63919f9c8acff77c51620bd8e684c550feb6b3e8784239bc3d9079457 Attempt:4 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.582681  199956 kuberuntime_manager.go:889] "Creating container in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.583788  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Normal" reason="BackOff" message="Back-off pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\""
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.583814  199956 kuberuntime_manager.go:903] "Container start failed in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1" containerMessage="Back-off pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\"" err="ImagePullBackOff"
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.583841  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Warning" reason="Failed" message="Error: ImagePullBackOff"
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.583875  199956 kubelet.go:1503] "syncPod exit" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d isTerminal=false
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: E0120 12:47:28.583914  199956 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ci-example256m\" with ImagePullBackOff: \"Back-off pulling image \\\"ngcn-registry.sh.intel.com:443/ci-example256m:latest\\\"\"" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.583964  199956 pod_workers.go:988] "Processing pod event done" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.594914  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="default/unsigned-unencrypted-cc-1" volumeName="kube-api-access-qcb8g" volumeSpecName="kube-api-access-qcb8g"
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.614442  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") Volume is already mounted to pod, but remount was requested." pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.614655  199956 projected.go:183] Setting up volume kube-api-access-qcb8g for pod 4f97314d-815b-4787-b174-5c158cd28c9d at /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.615075  199956 atomic_writer.go:161] pod default/unsigned-unencrypted-cc-1 volume kube-api-access-qcb8g: no update required for target directory /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.615148  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") " pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.879613  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.879683  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.880845  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:28 GMT] X-Content-Type-Options:[nosniff]] 0xc00123e220 2 [] true false map[] 0xc001808200 <nil>}
Jan 20 12:47:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:28.880967  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:47:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:29.484144  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:29.484216  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:29.492465  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[cf02bb85-ebec-4195-b650-fbfd96cf8fc0] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:29 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0007062a0 2 [] false false map[] 0xc0000dc400 0xc0015f3810}
Jan 20 12:47:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:29.492496  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:29.581397  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:47:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:29.587020  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:47:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:29.587032  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:47:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:29.587573  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:47:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:29.587918  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:47:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:29.587964  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:47:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:29.587971  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:47:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:29.587979  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:47:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:30.010709  199956 handler.go:293] error while reading "/proc/199956/fd/34" link: readlink /proc/199956/fd/34: no such file or directory
Jan 20 12:47:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:30.483999  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:30.484070  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:30.492167  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[c3888f1a-a4af-47a4-bfbb-9cd7947a68b3] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:30 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123eec0 2 [] false false map[] 0xc000fccc00 0xc000aff810}
Jan 20 12:47:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:30.492194  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:30.580866  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:47:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:30.589014  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:47:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:30.598891  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902001416Ki" capacity="981310056Ki" time="2023-01-20 12:47:21.261930289 -0500 EST"
Jan 20 12:47:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:30.598905  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:47:21.261930289 -0500 EST"
Jan 20 12:47:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:30.598911  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510787" capacity="511757" time="2023-01-20 12:47:30.598563484 -0500 EST m=+963.154338309"
Jan 20 12:47:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:30.598918  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59534656Ki" capacity="65586124Ki" time="2023-01-20 12:47:30.583084404 -0500 EST m=+963.138859292"
Jan 20 12:47:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:30.598923  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64402016Ki" capacity="65061836Ki" time="2023-01-20 12:47:30.598828748 -0500 EST m=+963.154603573"
Jan 20 12:47:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:30.598929  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902001416Ki" capacity="981310056Ki" time="2023-01-20 12:47:30.583084404 -0500 EST m=+963.138859292"
Jan 20 12:47:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:30.598935  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:47:30.583084404 -0500 EST m=+963.138859292"
Jan 20 12:47:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:30.598961  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:47:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:31.166790  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:47:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:31.166864  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:31.175200  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[0f9adbbb-791b-4b57-a8a0-42539641c513] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:31 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000706e20 2 [] false false map[] 0xc000338d00 0xc000fd4370}
Jan 20 12:47:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:31.175259  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:31.483909  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:31.483976  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:31.497193  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[888473b8-6153-4686-b8e1-677e42ea0aba] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:31 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000707ae0 2 [] false false map[] 0xc0000dd800 0xc0013f06e0}
Jan 20 12:47:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:31.497333  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:31.582165  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:47:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:31.587980  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:47:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:31.587993  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:47:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:31.588629  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:47:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:31.589072  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:47:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:31.589135  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:47:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:31.589142  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:47:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:31.589151  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:47:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:32.483712  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:32.483777  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:32.492107  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[1d548b4d-7727-46c9-9de2-cad0d8419f74] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:32 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123f6c0 2 [] false false map[] 0xc0014c6a00 0xc000ff4370}
Jan 20 12:47:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:32.492137  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:32.689798  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:47:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:32.689863  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:32.690922  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:32 GMT]] 0xc001487de0 2 [] true false map[] 0xc000339100 <nil>}
Jan 20 12:47:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:32.691028  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:47:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:32.696282  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:47:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:32.696348  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:32.697380  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:32 GMT]] 0xc00123f7a0 2 [] true false map[] 0xc0014ec800 <nil>}
Jan 20 12:47:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:32.697482  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:47:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:32.709612  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:47:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:32.709676  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:32.709682  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:47:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:32.709748  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:32.710825  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:32 GMT]] 0xc000ffa0a0 2 [] true false map[] 0xc0014c6c00 <nil>}
Jan 20 12:47:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:32.710825  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:32 GMT]] 0xc001487e80 2 [] true false map[] 0xc0014eca00 <nil>}
Jan 20 12:47:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:32.710927  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:47:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:32.710944  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:47:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:32.906218  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:47:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:33.483442  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:33.483508  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:33.491441  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[5d5689e1-20a7-478a-81d2-9d83091ed077] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:33 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e85a0 2 [] false false map[] 0xc0014ecc00 0xc0016a8210}
Jan 20 12:47:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:33.491470  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:33.582194  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[kube-system/coredns-6d4b75cb6d-48zl2]
Jan 20 12:47:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:33.582284  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:47:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:33.582419  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 updateType=0
Jan 20 12:47:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:33.582498  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1
Jan 20 12:47:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:33.582535  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:47:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:33.582625  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/coredns-6d4b75cb6d-48zl2" oldPhase=Running phase=Running
Jan 20 12:47:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:33.582907  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/coredns-6d4b75cb6d-48zl2" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:45 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:45 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.2 PodIPs:[{IP:10.244.0.2}] StartTime:2023-01-20 12:31:42 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:coredns State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:43 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:k8s.gcr.io/coredns/coredns:v1.8.6 ImageID:k8s.gcr.io/coredns/coredns@sha256:5b6ec0d6de9baaf3e92d0f66cd96a25b9edbce8716f5f15dcd1a616b3abd590e ContainerID:containerd://b3af3b285c950aba4fcd0176473261f7f4700aeaafe9c73ae15b15a7d6304092 Started:0xc001ea6aae}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:47:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:33.583276  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:47:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:33.583349  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:47:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:33.583847  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:47:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:33.584026  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 isTerminal=false
Jan 20 12:47:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:33.584080  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 updateType=0
Jan 20 12:47:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:33.587994  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:47:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:33.588005  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:47:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:33.588520  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:47:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:33.588850  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:47:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:33.588894  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:47:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:33.588900  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:47:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:33.588908  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:47:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:33.631073  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/coredns-6d4b75cb6d-48zl2" volumeName="config-volume" volumeSpecName="config-volume"
Jan 20 12:47:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:33.631138  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/coredns-6d4b75cb6d-48zl2" volumeName="kube-api-access-9qh7j" volumeSpecName="kube-api-access-9qh7j"
Jan 20 12:47:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:33.651400  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-9qh7j\" (UniqueName: \"kubernetes.io/projected/d94e1927-d22d-4831-a764-0170eae346a1-kube-api-access-9qh7j\") pod \"coredns-6d4b75cb6d-48zl2\" (UID: \"d94e1927-d22d-4831-a764-0170eae346a1\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:47:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:33.651529  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/d94e1927-d22d-4831-a764-0170eae346a1-config-volume\") pod \"coredns-6d4b75cb6d-48zl2\" (UID: \"d94e1927-d22d-4831-a764-0170eae346a1\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:47:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:33.651674  199956 configmap.go:181] Setting up volume config-volume for pod d94e1927-d22d-4831-a764-0170eae346a1 at /var/lib/kubelet/pods/d94e1927-d22d-4831-a764-0170eae346a1/volumes/kubernetes.io~configmap/config-volume
Jan 20 12:47:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:33.651734  199956 configmap.go:205] Received configMap kube-system/coredns containing (1) pieces of data, 339 total bytes
Jan 20 12:47:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:33.651758  199956 projected.go:183] Setting up volume kube-api-access-9qh7j for pod d94e1927-d22d-4831-a764-0170eae346a1 at /var/lib/kubelet/pods/d94e1927-d22d-4831-a764-0170eae346a1/volumes/kubernetes.io~projected/kube-api-access-9qh7j
Jan 20 12:47:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:33.651801  199956 quota_linux.go:271] SupportsQuotas called, but quotas disabled
Jan 20 12:47:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:33.652028  199956 atomic_writer.go:161] pod kube-system/coredns-6d4b75cb6d-48zl2 volume config-volume: no update required for target directory /var/lib/kubelet/pods/d94e1927-d22d-4831-a764-0170eae346a1/volumes/kubernetes.io~configmap/config-volume
Jan 20 12:47:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:33.652091  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/d94e1927-d22d-4831-a764-0170eae346a1-config-volume\") pod \"coredns-6d4b75cb6d-48zl2\" (UID: \"d94e1927-d22d-4831-a764-0170eae346a1\") " pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:47:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:33.652193  199956 atomic_writer.go:161] pod kube-system/coredns-6d4b75cb6d-48zl2 volume kube-api-access-9qh7j: no update required for target directory /var/lib/kubelet/pods/d94e1927-d22d-4831-a764-0170eae346a1/volumes/kubernetes.io~projected/kube-api-access-9qh7j
Jan 20 12:47:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:33.652267  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-9qh7j\" (UniqueName: \"kubernetes.io/projected/d94e1927-d22d-4831-a764-0170eae346a1-kube-api-access-9qh7j\") pod \"coredns-6d4b75cb6d-48zl2\" (UID: \"d94e1927-d22d-4831-a764-0170eae346a1\") " pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:47:34 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:34.483174  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:34 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:34.483242  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:34 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:34.496218  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[68f8a151-97d9-4bfe-90e5-e65c31325404] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:34 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001300be0 2 [] false false map[] 0xc0014ecf00 0xc0016a8420}
Jan 20 12:47:34 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:34.496359  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:35.483137  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:35.483201  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:35.491336  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[3c176cfb-b1d5-4acd-932f-2a3496f3eefc] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:35 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ffa0c0 2 [] false false map[] 0xc0016aa400 0xc001e05550}
Jan 20 12:47:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:35.491369  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:35.582101  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:47:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:35.587965  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:47:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:35.587977  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:47:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:35.587983  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:47:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:35.588505  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:47:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:35.588551  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:47:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:35.588558  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:47:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:35.588567  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:47:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:36.164786  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:47:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:36.164860  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:36.172628  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:36 GMT] X-Content-Type-Options:[nosniff]] 0xc0008dc020 2 [] false false map[] 0xc0016ab100 0xc001e294a0}
Jan 20 12:47:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:36.172660  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:47:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:36.271608  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:47:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:36.271674  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:36.272873  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:36 GMT]] 0xc000320840 29 [] true false map[] 0xc0016ab500 <nil>}
Jan 20 12:47:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:36.272980  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:47:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:36.483109  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:36.483174  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:36.491258  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[cbb05164-49aa-411a-a306-0be68e693b23] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:36 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008dc7e0 2 [] false false map[] 0xc001c0b900 0xc00210d550}
Jan 20 12:47:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:36.491285  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:36.895339  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:47:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:36.895401  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:36.902747  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:36 GMT] X-Content-Type-Options:[nosniff]] 0xc000320ac0 2 [] false false map[] 0xc001c0bb00 0xc0020b5a20}
Jan 20 12:47:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:36.902797  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:47:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:37.484217  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:37.484284  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:37.492766  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[46fe938a-482e-4f2f-9505-2406a825480d] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:37 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001718000 2 [] false false map[] 0xc000cf6100 0xc0023538c0}
Jan 20 12:47:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:37.492822  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:37.582144  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:47:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:37.587999  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:47:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:37.588011  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:47:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:37.588762  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:47:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:37.589178  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:47:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:37.589228  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:47:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:37.589235  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:47:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:37.589243  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:47:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:37.907947  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:47:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:38.318396  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:47:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:38.318404  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:47:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:38.318573  199956 interface.go:209] Interface eno2 is up
Jan 20 12:47:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:38.318601  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:47:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:38.318609  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:47:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:38.318613  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:47:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:38.318617  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:47:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:38.318621  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:47:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:38.319092  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:47:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:38.319100  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:47:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:38.319105  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:47:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:38.319134  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:47:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:38.483925  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:38.483987  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:38.496781  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[e7427fc7-c776-484d-8f11-9370b430d667] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:38 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ffb160 2 [] false false map[] 0xc001808500 0xc001361340}
Jan 20 12:47:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:38.496916  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:38.879427  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:47:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:38.879497  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:38.879526  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/healthz" timeout="1s"
Jan 20 12:47:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:38.879591  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:38.880791  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:38 GMT] X-Content-Type-Options:[nosniff]] 0xc000ffba20 2 [] true false map[] 0xc0014c6e00 <nil>}
Jan 20 12:47:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:38.880839  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:38 GMT] X-Content-Type-Options:[nosniff]] 0xc000ce8020 2 [] true false map[] 0xc001847500 <nil>}
Jan 20 12:47:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:38.880914  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:47:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:38.880961  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:47:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:39.483479  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:39.483545  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:39.491446  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[2d3cca61-da14-40dc-a26f-b6ae58e2ca46] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:39 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001702820 2 [] false false map[] 0xc000fcc100 0xc0011c16b0}
Jan 20 12:47:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:39.491496  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:39.581165  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:47:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:39.587019  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:47:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:39.587031  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:47:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:39.587037  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:47:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:39.587757  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:47:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:39.587833  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:47:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:39.587839  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:47:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:39.587848  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:47:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:40.483628  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:40.483696  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:40.491327  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[851982ab-0fbd-4117-8d44-9db73fa439bc] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:40 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001486400 2 [] false false map[] 0xc00105bc00 0xc001501340}
Jan 20 12:47:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:40.491356  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:40.599493  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:47:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:40.607220  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:47:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:40.617682  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:47:31.262461086 -0500 EST"
Jan 20 12:47:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:40.617695  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510788" capacity="511757" time="2023-01-20 12:47:40.617359882 -0500 EST m=+973.173134707"
Jan 20 12:47:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:40.617703  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59532196Ki" capacity="65586124Ki" time="2023-01-20 12:47:40.601693355 -0500 EST m=+973.157468246"
Jan 20 12:47:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:40.617709  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64401984Ki" capacity="65061836Ki" time="2023-01-20 12:47:40.617623997 -0500 EST m=+973.173398822"
Jan 20 12:47:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:40.617715  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902001380Ki" capacity="981310056Ki" time="2023-01-20 12:47:40.601693355 -0500 EST m=+973.157468246"
Jan 20 12:47:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:40.617721  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:47:40.601693355 -0500 EST m=+973.157468246"
Jan 20 12:47:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:40.617727  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902001380Ki" capacity="981310056Ki" time="2023-01-20 12:47:31.262461086 -0500 EST"
Jan 20 12:47:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:40.617754  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:47:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:41.166659  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:47:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:41.166728  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:41.175076  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[d1e17566-65cf-42a5-aa71-f4f7d6a3e60d] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:41 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000979c40 2 [] false false map[] 0xc0000dc100 0xc001884b00}
Jan 20 12:47:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:41.175132  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:41.484106  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:41.484172  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:41.492121  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[f1a59b38-b924-41e5-821b-2d0ab45086d4] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:41 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000979fe0 2 [] false false map[] 0xc0000dd000 0xc00198b1e0}
Jan 20 12:47:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:41.492152  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:41.581440  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:47:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:41.587040  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:47:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:41.587053  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:47:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:41.587725  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:47:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:41.588063  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:47:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:41.588110  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:47:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:41.588116  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:47:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:41.588125  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.483972  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.484041  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.492072  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[ec22dcc6-c902-4d4b-a22c-c5a3bdef1e1f] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:42 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ce9be0 2 [] false false map[] 0xc0021a2d00 0xc0018f8bb0}
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.492101  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.582084  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[default/unsigned-unencrypted-cc-1]
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.582170  199956 pod_workers.go:888] "Processing pod event" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.582230  199956 kubelet.go:1501] "syncPod enter" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.582261  199956 kubelet_pods.go:1435] "Generating pod status" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.582357  199956 kubelet_pods.go:1447] "Got phase for pod" pod="default/unsigned-unencrypted-cc-1" oldPhase=Pending phase=Pending
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.582638  199956 status_manager.go:535] "Ignoring same status for pod" pod="default/unsigned-unencrypted-cc-1" status={Phase:Pending Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.16 PodIPs:[{IP:10.244.0.16}] StartTime:2023-01-20 12:41:08 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:ci-example256m State:{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "ngcn-registry.sh.intel.com:443/ci-example256m:latest",} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest ImageID: ContainerID: Started:0xc00094f649}] QOSClass:Guaranteed EphemeralContainerStatuses:[]}
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.582951  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.583018  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.583064  199956 kuberuntime_manager.go:638] "Container of pod is not in the desired state and shall be started" containerName="ci-example256m" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.583103  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:08841bf63919f9c8acff77c51620bd8e684c550feb6b3e8784239bc3d9079457 Attempt:4 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.583384  199956 kuberuntime_manager.go:889] "Creating container in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.584512  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Normal" reason="BackOff" message="Back-off pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\""
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.584530  199956 kuberuntime_manager.go:903] "Container start failed in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1" containerMessage="Back-off pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\"" err="ImagePullBackOff"
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.584581  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Warning" reason="Failed" message="Error: ImagePullBackOff"
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.584616  199956 kubelet.go:1503] "syncPod exit" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d isTerminal=false
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: E0120 12:47:42.584658  199956 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ci-example256m\" with ImagePullBackOff: \"Back-off pulling image \\\"ngcn-registry.sh.intel.com:443/ci-example256m:latest\\\"\"" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.584745  199956 pod_workers.go:988] "Processing pod event done" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.597090  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="default/unsigned-unencrypted-cc-1" volumeName="kube-api-access-qcb8g" volumeSpecName="kube-api-access-qcb8g"
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.617279  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") Volume is already mounted to pod, but remount was requested." pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.617482  199956 projected.go:183] Setting up volume kube-api-access-qcb8g for pod 4f97314d-815b-4787-b174-5c158cd28c9d at /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.617890  199956 atomic_writer.go:161] pod default/unsigned-unencrypted-cc-1 volume kube-api-access-qcb8g: no update required for target directory /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.617962  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") " pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.689475  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.689536  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.690669  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:42 GMT]] 0xc000ffbc40 2 [] true false map[] 0xc0021a3100 <nil>}
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.690778  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.695988  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.696046  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.697022  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:42 GMT]] 0xc0012e89a0 2 [] true false map[] 0xc0000dd400 <nil>}
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.697125  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.710532  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.710591  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.710609  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.710669  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.711644  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:42 GMT]] 0xc00123eb40 2 [] true false map[] 0xc0000dd800 <nil>}
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.711653  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:42 GMT]] 0xc000ce9fe0 2 [] true false map[] 0xc001808800 <nil>}
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.711749  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.711765  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:47:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:42.909572  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:47:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:43.483355  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:43.483430  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:43.491399  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[34469cd1-f688-4c02-8e77-215055f43c3b] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:43 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ffbfc0 2 [] false false map[] 0xc001808a00 0xc001e0b3f0}
Jan 20 12:47:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:43.491428  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:43.582207  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:47:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:43.588029  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:47:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:43.588042  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:47:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:43.588568  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:47:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:43.588957  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:47:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:43.589003  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:47:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:43.589010  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:47:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:43.589019  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:47:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:44.483423  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:44.483491  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:44.491260  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[af5ae8c2-3025-4b89-9e5f-706bda43a880] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:44 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00129c440 2 [] false false map[] 0xc0014c7400 0xc001e86e70}
Jan 20 12:47:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:44.491326  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:45.483153  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:45.483221  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:45.491619  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[fb563f98-aa0f-467e-bbda-c7346ec27f5d] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:45 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123f740 2 [] false false map[] 0xc0014c7800 0xc001e87080}
Jan 20 12:47:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:45.491651  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:45.581555  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:47:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:45.587750  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:47:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:45.587770  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:47:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:45.587777  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:47:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:45.588653  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:47:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:45.588703  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:47:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:45.588723  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:47:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:45.588732  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:47:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:46.164406  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:47:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:46.164485  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:46.171681  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:46 GMT] X-Content-Type-Options:[nosniff]] 0xc00123eda0 2 [] false false map[] 0xc001846200 0xc00112e4d0}
Jan 20 12:47:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:46.171710  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:47:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:46.271805  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:47:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:46.271865  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:46.273130  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:46 GMT]] 0xc00123ede0 29 [] true false map[] 0xc0000dcf00 <nil>}
Jan 20 12:47:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:46.273240  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:47:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:46.483137  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:46.483202  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:46.491130  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[9efa740b-35c2-4e6c-a511-f35dc23932d7] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:46 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ce8420 2 [] false false map[] 0xc001846400 0xc0002b6c60}
Jan 20 12:47:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:46.491189  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:46.895263  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:47:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:46.895330  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:46.902673  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:46 GMT] X-Content-Type-Options:[nosniff]] 0xc00123f320 2 [] false false map[] 0xc0014c6e00 0xc00237d810}
Jan 20 12:47:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:46.902728  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:47:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:47.484130  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:47.484193  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:47.492001  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[aa3e4a54-52d8-425b-bbb7-4bc87cde83f6] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:47 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ce85a0 2 [] false false map[] 0xc0000dd200 0xc0005fa8f0}
Jan 20 12:47:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:47.492034  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:47.525490  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/etcd.yaml"
Jan 20 12:47:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:47.525975  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver.yaml"
Jan 20 12:47:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:47.526507  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Jan 20 12:47:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:47.527036  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-scheduler.yaml"
Jan 20 12:47:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:47.527361  199956 config.go:279] "Setting pods for source" source="file"
Jan 20 12:47:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:47.581491  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:47:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:47.587040  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:47:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:47.587052  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:47:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:47.587762  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:47:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:47.588168  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:47:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:47.588215  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:47:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:47.588223  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:47:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:47.588231  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:47:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:47.911417  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:47:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:48.483603  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:48.483670  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:48.491135  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[10a6cb52-da11-486d-83e3-c6b41ea63aae] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:48 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ce9160 2 [] false false map[] 0xc001846900 0xc0010f8bb0}
Jan 20 12:47:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:48.491166  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:48.716774  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:47:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:48.716782  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:47:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:48.716957  199956 interface.go:209] Interface eno2 is up
Jan 20 12:47:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:48.716984  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:47:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:48.716992  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:47:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:48.716996  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:47:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:48.717000  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:47:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:48.717003  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:47:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:48.717301  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:47:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:48.717310  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:47:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:48.717329  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:47:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:48.717332  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:47:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:48.879419  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:47:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:48.879484  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:48.880643  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:48 GMT] X-Content-Type-Options:[nosniff]] 0xc0003209a0 2 [] true false map[] 0xc001a4a300 <nil>}
Jan 20 12:47:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:48.880788  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:47:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:48.885552  199956 handler.go:293] error while reading "/proc/199956/fd/34" link: readlink /proc/199956/fd/34: no such file or directory
Jan 20 12:47:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:49.483473  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:49.483533  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:49.492476  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[4816e28c-de69-4c7e-933e-7f7ae3815aa9] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:49 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0009a1d40 2 [] false false map[] 0xc001a4a600 0xc0016a3d90}
Jan 20 12:47:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:49.492513  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:49.581144  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:47:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:49.587025  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:47:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:49.587038  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:47:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:49.587045  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:47:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:49.587811  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:47:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:49.587857  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:47:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:49.587863  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:47:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:49.587886  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:47:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:50.483502  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:50.483570  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:50.491088  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[b10fab37-ca03-4739-9c12-5138d27752ee] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:50 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001702220 2 [] false false map[] 0xc001a4aa00 0xc00189c9a0}
Jan 20 12:47:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:50.491141  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:50.618707  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:47:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:50.627011  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:47:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:50.636875  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:47:41.262577399 -0500 EST"
Jan 20 12:47:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:50.636889  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510788" capacity="511757" time="2023-01-20 12:47:50.636532616 -0500 EST m=+983.192307439"
Jan 20 12:47:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:50.636897  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59532536Ki" capacity="65586124Ki" time="2023-01-20 12:47:50.62079653 -0500 EST m=+983.176571422"
Jan 20 12:47:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:50.636903  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64401948Ki" capacity="65061836Ki" time="2023-01-20 12:47:50.636818559 -0500 EST m=+983.192593385"
Jan 20 12:47:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:50.636909  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="902001340Ki" capacity="981310056Ki" time="2023-01-20 12:47:50.62079653 -0500 EST m=+983.176571422"
Jan 20 12:47:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:50.636915  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:47:50.62079653 -0500 EST m=+983.176571422"
Jan 20 12:47:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:50.636921  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="902001340Ki" capacity="981310056Ki" time="2023-01-20 12:47:41.262577399 -0500 EST"
Jan 20 12:47:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:50.636947  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:47:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:51.166568  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:47:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:51.166635  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:51.174517  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[63d2250a-3e24-4cc8-92f9-f3986ac8d715] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:51 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0017036c0 2 [] false false map[] 0xc0002c3900 0xc001d2c8f0}
Jan 20 12:47:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:51.174558  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:51.483375  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:51.483439  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:51.491307  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[99a77cd3-6355-4991-8acc-4d954a312648] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:51 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001703aa0 2 [] false false map[] 0xc0002c3e00 0xc001d429a0}
Jan 20 12:47:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:51.491337  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:51.581923  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:47:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:51.585180  199956 reflector.go:536] vendor/k8s.io/client-go/informers/factory.go:134: Watch close - *v1.Node total 10 items received
Jan 20 12:47:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:51.587969  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:47:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:51.587981  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:47:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:51.588621  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:47:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:51.588960  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:47:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:51.589006  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:47:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:51.589013  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:47:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:51.589022  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:47:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:52.484070  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:52.484139  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:52.492082  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[ee142e60-7854-4326-a097-d50d98dab9fa] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:52 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001487dc0 2 [] false false map[] 0xc001a4b000 0xc001e0e2c0}
Jan 20 12:47:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:52.492114  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:52.689363  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:47:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:52.689423  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:52.690578  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:52 GMT]] 0xc001820520 2 [] true false map[] 0xc0014ed300 <nil>}
Jan 20 12:47:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:52.690681  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:47:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:52.696103  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:47:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:52.696161  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:52.697214  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:52 GMT]] 0xc000ffa560 2 [] true false map[] 0xc0014ed500 <nil>}
Jan 20 12:47:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:52.697316  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:47:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:52.710533  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:47:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:52.710592  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:52.710609  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:47:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:52.710664  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:52.711647  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:52 GMT]] 0xc000ffa5c0 2 [] true false map[] 0xc001a4b300 <nil>}
Jan 20 12:47:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:52.711684  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:52 GMT]] 0xc001300040 2 [] true false map[] 0xc0011c4300 <nil>}
Jan 20 12:47:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:52.711752  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:47:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:52.711790  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:47:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:52.912425  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:47:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:53.294601  199956 reflector.go:536] object-"kube-system"/"kube-proxy": Watch close - *v1.ConfigMap total 0 items received
Jan 20 12:47:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:53.483650  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:53.483718  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:53.492085  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[2b1429e2-35bd-41bb-8606-84df918d227a] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:53 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ffa380 2 [] false false map[] 0xc001a4a100 0xc000d420b0}
Jan 20 12:47:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:53.492112  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:53.581807  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:47:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:53.587960  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:47:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:53.587972  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:47:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:53.587979  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:47:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:53.588673  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:47:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:53.588725  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:47:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:53.588732  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:47:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:53.588740  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:47:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:54.483450  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:54.483520  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:54.496165  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[8c70e234-ff71-4fcc-83f6-c0a6c0e66065] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:54 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e9180 2 [] false false map[] 0xc001716100 0xc001982370}
Jan 20 12:47:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:54.496293  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:55.483991  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:55.484058  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:55.491975  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[ca863b18-d363-420d-8ede-c0475262a82b] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:55 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123e640 2 [] false false map[] 0xc001716400 0xc000f08370}
Jan 20 12:47:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:55.492004  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:55.581743  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:47:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:55.587157  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:47:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:55.587169  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:47:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:55.587902  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:47:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:55.588336  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:47:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:55.588382  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:47:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:55.588388  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:47:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:55.588396  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:47:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:56.165202  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:47:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:56.165272  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:56.172669  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:56 GMT] X-Content-Type-Options:[nosniff]] 0xc0008e37a0 2 [] false false map[] 0xc001a4a700 0xc000f084d0}
Jan 20 12:47:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:56.172703  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:47:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:56.271612  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:47:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:56.271674  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:56.272869  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:56 GMT]] 0xc0012e9a60 29 [] true false map[] 0xc001846800 <nil>}
Jan 20 12:47:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:56.272973  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:47:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:56.484029  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:56.484094  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:56.491996  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[a6fffd2a-2d38-4441-a0b1-906112976b52] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:56 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008e3e20 2 [] false false map[] 0xc001846a00 0xc0011db970}
Jan 20 12:47:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:56.492025  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:56.895594  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:47:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:56.895659  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:56.902811  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:56 GMT] X-Content-Type-Options:[nosniff]] 0xc0012e9de0 2 [] false false map[] 0xc001a4a900 0xc001488000}
Jan 20 12:47:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:56.902835  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:47:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:57.483739  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:57.483806  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:57.492288  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[e7feb8a9-b336-4ae2-97ca-bc91254061de] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:57 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0020fc1c0 2 [] false false map[] 0xc001716800 0xc001562000}
Jan 20 12:47:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:57.492315  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:57.581651  199956 reflector.go:536] vendor/k8s.io/client-go/informers/factory.go:134: Watch close - *v1.CSIDriver total 0 items received
Jan 20 12:47:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:57.581754  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[default/unsigned-unencrypted-cc-1]
Jan 20 12:47:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:57.581861  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:47:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:57.581987  199956 pod_workers.go:888] "Processing pod event" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:47:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:57.582066  199956 kubelet.go:1501] "syncPod enter" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:47:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:57.582100  199956 kubelet_pods.go:1435] "Generating pod status" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:47:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:57.582202  199956 kubelet_pods.go:1447] "Got phase for pod" pod="default/unsigned-unencrypted-cc-1" oldPhase=Pending phase=Pending
Jan 20 12:47:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:57.582862  199956 status_manager.go:535] "Ignoring same status for pod" pod="default/unsigned-unencrypted-cc-1" status={Phase:Pending Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.16 PodIPs:[{IP:10.244.0.16}] StartTime:2023-01-20 12:41:08 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:ci-example256m State:{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "ngcn-registry.sh.intel.com:443/ci-example256m:latest",} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest ImageID: ContainerID: Started:0xc001d2f6b2}] QOSClass:Guaranteed EphemeralContainerStatuses:[]}
Jan 20 12:47:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:57.583225  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:47:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:57.583300  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:47:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:57.583351  199956 kuberuntime_manager.go:638] "Container of pod is not in the desired state and shall be started" containerName="ci-example256m" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:47:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:57.583392  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:08841bf63919f9c8acff77c51620bd8e684c550feb6b3e8784239bc3d9079457 Attempt:4 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:47:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:57.583666  199956 kuberuntime_manager.go:889] "Creating container in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:47:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:57.584778  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Normal" reason="BackOff" message="Back-off pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\""
Jan 20 12:47:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:57.584842  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Warning" reason="Failed" message="Error: ImagePullBackOff"
Jan 20 12:47:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:57.584780  199956 kuberuntime_manager.go:903] "Container start failed in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1" containerMessage="Back-off pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\"" err="ImagePullBackOff"
Jan 20 12:47:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:57.584913  199956 kubelet.go:1503] "syncPod exit" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d isTerminal=false
Jan 20 12:47:57 zcy-Z390-AORUS-MASTER kubelet[199956]: E0120 12:47:57.584959  199956 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ci-example256m\" with ImagePullBackOff: \"Back-off pulling image \\\"ngcn-registry.sh.intel.com:443/ci-example256m:latest\\\"\"" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:47:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:57.585014  199956 pod_workers.go:988] "Processing pod event done" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:47:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:57.587206  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:47:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:57.587218  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:47:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:57.587669  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:47:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:57.588011  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:47:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:57.588056  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:47:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:57.588063  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:47:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:57.588071  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:47:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:57.611850  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="default/unsigned-unencrypted-cc-1" volumeName="kube-api-access-qcb8g" volumeSpecName="kube-api-access-qcb8g"
Jan 20 12:47:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:57.629090  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") Volume is already mounted to pod, but remount was requested." pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:47:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:57.629162  199956 projected.go:183] Setting up volume kube-api-access-qcb8g for pod 4f97314d-815b-4787-b174-5c158cd28c9d at /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:47:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:57.629326  199956 atomic_writer.go:161] pod default/unsigned-unencrypted-cc-1 volume kube-api-access-qcb8g: no update required for target directory /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:47:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:57.629353  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") " pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:47:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:57.913608  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:47:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:58.483830  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:58.483896  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:58.491922  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[f3fec2e9-f6eb-46fb-ac4e-1740b557ab1e] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:58 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0003209e0 2 [] false false map[] 0xc001847b00 0xc0022680b0}
Jan 20 12:47:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:58.491954  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:58.879341  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/healthz" timeout="1s"
Jan 20 12:47:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:58.879404  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:47:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:58.879408  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:58.879466  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:58.880661  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:58 GMT] X-Content-Type-Options:[nosniff]] 0xc001301e00 2 [] true false map[] 0xc0014c6400 <nil>}
Jan 20 12:47:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:58.880746  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:58 GMT] X-Content-Type-Options:[nosniff]] 0xc001719a20 2 [] true false map[] 0xc001847d00 <nil>}
Jan 20 12:47:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:58.880809  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:47:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:58.880855  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:47:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:59.102155  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:47:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:59.102161  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:47:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:59.102341  199956 interface.go:209] Interface eno2 is up
Jan 20 12:47:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:59.102381  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:47:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:59.102388  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:47:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:59.102393  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:47:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:59.102396  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:47:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:59.102400  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:47:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:59.102839  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:47:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:59.102847  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:47:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:59.102851  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:47:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:59.102867  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:47:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:59.484161  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:47:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:59.484177  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:47:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:59.487261  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[60c1c996-1807-4e8c-bf09-a583a0312073] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:47:59 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000320980 2 [] false false map[] 0xc000cf6100 0xc001a46210}
Jan 20 12:47:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:59.487290  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:47:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:59.582213  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:47:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:59.588009  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:47:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:59.588021  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:47:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:59.588699  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:47:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:59.589127  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:47:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:59.589175  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:47:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:59.589181  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:47:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:47:59.589189  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.483698  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.483764  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.491067  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[01b49b1e-2f27-4a1f-b444-b5d32407f088] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:00 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000321320 2 [] false false map[] 0xc001846400 0xc001361d90}
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.491094  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.582028  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=2 pods=[kube-system/kube-proxy-prhfv kube-system/kube-controller-manager-zcy-z390-aorus-master]
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.582114  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce updateType=0
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.582177  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.582185  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/kube-proxy-prhfv" podUID=1c057a0e-3ee3-44f3-b294-434acc8f9491 updateType=0
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.582210  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master"
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.582266  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/kube-proxy-prhfv" podUID=1c057a0e-3ee3-44f3-b294-434acc8f9491
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.582298  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/kube-proxy-prhfv"
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.582309  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" oldPhase=Running phase=Running
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.582378  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/kube-proxy-prhfv" oldPhase=Running phase=Running
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.582588  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:28 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:37 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:37 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:28 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.239.159.53 PodIPs:[{IP:10.239.159.53}] StartTime:2023-01-20 12:31:28 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:kube-controller-manager State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:22 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:6 Image:k8s.gcr.io/kube-controller-manager:v1.24.0 ImageID:k8s.gcr.io/kube-controller-manager@sha256:df044a154e79a18f749d3cd9d958c3edde2b6a00c815176472002b7bbf956637 ContainerID:containerd://f8443d2868215ec42d3fa335663976e33df166c5e98369aa3f9d7ddb9235bead Started:0xc001ea6ed9}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.582918  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master"
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.582989  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master"
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.583008  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/kube-proxy-prhfv" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:43 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:43 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.239.159.53 PodIPs:[{IP:10.239.159.53}] StartTime:2023-01-20 12:31:42 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:kube-proxy State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:43 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:k8s.gcr.io/kube-proxy:v1.24.0 ImageID:k8s.gcr.io/kube-proxy@sha256:c957d602267fa61082ab8847914b2118955d0739d592cc7b01e278513478d6a8 ContainerID:containerd://f78ed441ff85d669a403a76c0987f2d86913431bd96d454b1a3605bdcf0474f2 Started:0xc0015ffc1e}] QOSClass:BestEffort EphemeralContainerStatuses:[]}
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.583332  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-proxy-prhfv"
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.583405  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/kube-proxy-prhfv"
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.583818  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/kube-proxy-prhfv"
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.583892  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/kube-controller-manager-zcy-z390-aorus-master"
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.583975  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/kube-proxy-prhfv" podUID=1c057a0e-3ee3-44f3-b294-434acc8f9491 isTerminal=false
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.584029  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/kube-proxy-prhfv" podUID=1c057a0e-3ee3-44f3-b294-434acc8f9491 updateType=0
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.584053  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce isTerminal=false
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.584107  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce updateType=0
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.634542  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="ca-certs" volumeSpecName="ca-certs"
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.634648  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.634719  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="etc-pki" volumeSpecName="etc-pki"
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.634789  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="flexvolume-dir" volumeSpecName="flexvolume-dir"
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.634853  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.634918  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.634982  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.635050  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.635168  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-proxy-prhfv" volumeName="kube-proxy" volumeSpecName="kube-proxy"
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.635232  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-proxy-prhfv" volumeName="xtables-lock" volumeSpecName="xtables-lock"
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.635294  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-proxy-prhfv" volumeName="lib-modules" volumeSpecName="lib-modules"
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.635405  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-proxy-prhfv" volumeName="kube-api-access-2sbqp" volumeSpecName="kube-api-access-2sbqp"
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.637579  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.645269  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.650852  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/1c057a0e-3ee3-44f3-b294-434acc8f9491-kube-proxy\") pod \"kube-proxy-prhfv\" (UID: \"1c057a0e-3ee3-44f3-b294-434acc8f9491\") Volume is already mounted to pod, but remount was requested." pod="kube-system/kube-proxy-prhfv"
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.650887  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-2sbqp\" (UniqueName: \"kubernetes.io/projected/1c057a0e-3ee3-44f3-b294-434acc8f9491-kube-api-access-2sbqp\") pod \"kube-proxy-prhfv\" (UID: \"1c057a0e-3ee3-44f3-b294-434acc8f9491\") Volume is already mounted to pod, but remount was requested." pod="kube-system/kube-proxy-prhfv"
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.650922  199956 projected.go:183] Setting up volume kube-api-access-2sbqp for pod 1c057a0e-3ee3-44f3-b294-434acc8f9491 at /var/lib/kubelet/pods/1c057a0e-3ee3-44f3-b294-434acc8f9491/volumes/kubernetes.io~projected/kube-api-access-2sbqp
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.650956  199956 configmap.go:181] Setting up volume kube-proxy for pod 1c057a0e-3ee3-44f3-b294-434acc8f9491 at /var/lib/kubelet/pods/1c057a0e-3ee3-44f3-b294-434acc8f9491/volumes/kubernetes.io~configmap/kube-proxy
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.650974  199956 configmap.go:205] Received configMap kube-system/kube-proxy containing (2) pieces of data, 1458 total bytes
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.650990  199956 quota_linux.go:271] SupportsQuotas called, but quotas disabled
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.651025  199956 atomic_writer.go:161] pod kube-system/kube-proxy-prhfv volume kube-api-access-2sbqp: no update required for target directory /var/lib/kubelet/pods/1c057a0e-3ee3-44f3-b294-434acc8f9491/volumes/kubernetes.io~projected/kube-api-access-2sbqp
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.651039  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-2sbqp\" (UniqueName: \"kubernetes.io/projected/1c057a0e-3ee3-44f3-b294-434acc8f9491-kube-api-access-2sbqp\") pod \"kube-proxy-prhfv\" (UID: \"1c057a0e-3ee3-44f3-b294-434acc8f9491\") " pod="kube-system/kube-proxy-prhfv"
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.651048  199956 atomic_writer.go:161] pod kube-system/kube-proxy-prhfv volume kube-proxy: no update required for target directory /var/lib/kubelet/pods/1c057a0e-3ee3-44f3-b294-434acc8f9491/volumes/kubernetes.io~configmap/kube-proxy
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.651062  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/1c057a0e-3ee3-44f3-b294-434acc8f9491-kube-proxy\") pod \"kube-proxy-prhfv\" (UID: \"1c057a0e-3ee3-44f3-b294-434acc8f9491\") " pod="kube-system/kube-proxy-prhfv"
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.655098  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510788" capacity="511757" time="2023-01-20 12:48:00.654779417 -0500 EST m=+993.210554242"
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.655111  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59533828Ki" capacity="65586124Ki" time="2023-01-20 12:48:00.639752242 -0500 EST m=+993.195527129"
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.655118  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64401956Ki" capacity="65061836Ki" time="2023-01-20 12:48:00.655045594 -0500 EST m=+993.210820419"
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.655124  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="901993096Ki" capacity="981310056Ki" time="2023-01-20 12:48:00.639752242 -0500 EST m=+993.195527129"
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.655130  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:48:00.639752242 -0500 EST m=+993.195527129"
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.655136  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="901993096Ki" capacity="981310056Ki" time="2023-01-20 12:47:51.262222518 -0500 EST"
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.655142  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:47:51.262222518 -0500 EST"
Jan 20 12:48:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:00.655166  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:48:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:01.166227  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:48:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:01.166294  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:01.174583  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[0831143e-cd01-4581-8286-f9a853804f2c] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:01 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001719180 2 [] false false map[] 0xc0014ed300 0xc00144adc0}
Jan 20 12:48:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:01.174653  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:01.483487  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:01.483554  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:01.496646  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[e5182302-2052-475e-b586-340141059c66] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:01 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123eb80 2 [] false false map[] 0xc0014c6d00 0xc0011db080}
Jan 20 12:48:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:01.496807  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:01.581213  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:48:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:01.587063  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:48:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:01.587075  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:48:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:01.587081  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:48:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:01.587761  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:48:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:01.587821  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:48:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:01.587828  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:48:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:01.587836  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:48:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:02.483466  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:02.483534  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:02.491074  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[8045d7a9-9e22-42bf-b29f-35c07fa21ed2] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:02 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e93a0 2 [] false false map[] 0xc0014c7300 0xc0013e0c60}
Jan 20 12:48:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:02.491106  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:02.689170  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:48:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:02.689238  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:02.690403  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:02 GMT]] 0xc00123f7a0 2 [] true false map[] 0xc0016aba00 <nil>}
Jan 20 12:48:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:02.690514  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:48:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:02.695746  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:48:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:02.695803  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:02.696839  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:02 GMT]] 0xc0012e9480 2 [] true false map[] 0xc0014ed800 <nil>}
Jan 20 12:48:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:02.696946  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:48:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:02.710474  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:48:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:02.710535  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:02.710542  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:48:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:02.710603  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:02.711536  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:02 GMT]] 0xc0012e94c0 2 [] true false map[] 0xc0014c7600 <nil>}
Jan 20 12:48:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:02.711642  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:48:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:02.711639  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:02 GMT]] 0xc001820040 2 [] true false map[] 0xc0014eda00 <nil>}
Jan 20 12:48:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:02.711744  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:48:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:02.914730  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:48:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:03.270784  199956 handler.go:293] error while reading "/proc/199956/fd/34" link: readlink /proc/199956/fd/34: no such file or directory
Jan 20 12:48:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:03.483382  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:03.483447  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:03.491595  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[206bbfd7-4db6-405a-882f-de460820bb2b] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:03 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008e2740 2 [] false false map[] 0xc0000dcf00 0xc0017e69a0}
Jan 20 12:48:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:03.491625  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:03.581859  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:48:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:03.587982  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:48:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:03.587994  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:48:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:03.588637  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:48:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:03.589073  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:48:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:03.589116  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:48:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:03.589124  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:48:03 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:03.589133  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:48:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:04.483135  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:04.483209  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:04.491030  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[4bb9e162-03c8-4af4-baf9-16a560d359de] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:04 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008dccc0 2 [] false false map[] 0xc0000dd300 0xc00183a8f0}
Jan 20 12:48:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:04.491058  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:04.581969  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[confidential-containers-system/cc-operator-pre-install-daemon-qjplj]
Jan 20 12:48:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:04.582053  199956 pod_workers.go:888] "Processing pod event" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" podUID=b0713fbc-efc5-4044-9d08-2326a0752f87 updateType=0
Jan 20 12:48:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:04.582112  199956 kubelet.go:1501] "syncPod enter" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" podUID=b0713fbc-efc5-4044-9d08-2326a0752f87
Jan 20 12:48:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:04.582148  199956 kubelet_pods.go:1435] "Generating pod status" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj"
Jan 20 12:48:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:04.582238  199956 kubelet_pods.go:1447] "Got phase for pod" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" oldPhase=Running phase=Running
Jan 20 12:48:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:04.582532  199956 status_manager.go:535] "Ignoring same status for pod" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:01 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:05 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:05 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:01 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.5 PodIPs:[{IP:10.244.0.5}] StartTime:2023-01-20 12:32:01 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:cc-runtime-pre-install-pod State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:32:03 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:quay.io/confidential-containers/container-engine-for-cc-payload:1034f9fcf947b22eea080a6f77d8e164e2369849 ImageID:quay.io/confidential-containers/container-engine-for-cc-payload@sha256:f86f078b3a47026a066e65c7d836d9b9a43bf177555c276624d90f42e50279a1 ContainerID:containerd://e0c5b991f81d5128566686552e45a9bbc8c584e4f6c94909edb3a42720dacc90 Started:0xc001200ebe}] QOSClass:BestEffort EphemeralContainerStatuses:[]}
Jan 20 12:48:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:04.582891  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj"
Jan 20 12:48:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:04.582960  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj"
Jan 20 12:48:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:04.583363  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj"
Jan 20 12:48:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:04.583531  199956 kubelet.go:1503] "syncPod exit" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" podUID=b0713fbc-efc5-4044-9d08-2326a0752f87 isTerminal=false
Jan 20 12:48:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:04.583587  199956 pod_workers.go:988] "Processing pod event done" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" podUID=b0713fbc-efc5-4044-9d08-2326a0752f87 updateType=0
Jan 20 12:48:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:04.667501  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" volumeName="confidential-containers-artifacts" volumeSpecName="confidential-containers-artifacts"
Jan 20 12:48:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:04.667612  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" volumeName="etc-systemd-system" volumeSpecName="etc-systemd-system"
Jan 20 12:48:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:04.667691  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" volumeName="dbus" volumeSpecName="dbus"
Jan 20 12:48:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:04.667760  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" volumeName="systemd" volumeSpecName="systemd"
Jan 20 12:48:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:04.667882  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj" volumeName="kube-api-access-gcgm6" volumeSpecName="kube-api-access-gcgm6"
Jan 20 12:48:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:04.679697  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-gcgm6\" (UniqueName: \"kubernetes.io/projected/b0713fbc-efc5-4044-9d08-2326a0752f87-kube-api-access-gcgm6\") pod \"cc-operator-pre-install-daemon-qjplj\" (UID: \"b0713fbc-efc5-4044-9d08-2326a0752f87\") Volume is already mounted to pod, but remount was requested." pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj"
Jan 20 12:48:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:04.679902  199956 projected.go:183] Setting up volume kube-api-access-gcgm6 for pod b0713fbc-efc5-4044-9d08-2326a0752f87 at /var/lib/kubelet/pods/b0713fbc-efc5-4044-9d08-2326a0752f87/volumes/kubernetes.io~projected/kube-api-access-gcgm6
Jan 20 12:48:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:04.680310  199956 atomic_writer.go:161] pod confidential-containers-system/cc-operator-pre-install-daemon-qjplj volume kube-api-access-gcgm6: no update required for target directory /var/lib/kubelet/pods/b0713fbc-efc5-4044-9d08-2326a0752f87/volumes/kubernetes.io~projected/kube-api-access-gcgm6
Jan 20 12:48:04 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:04.680384  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-gcgm6\" (UniqueName: \"kubernetes.io/projected/b0713fbc-efc5-4044-9d08-2326a0752f87-kube-api-access-gcgm6\") pod \"cc-operator-pre-install-daemon-qjplj\" (UID: \"b0713fbc-efc5-4044-9d08-2326a0752f87\") " pod="confidential-containers-system/cc-operator-pre-install-daemon-qjplj"
Jan 20 12:48:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:05.483322  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:05.483389  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:05.491588  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[14ae30c2-7b37-49b1-ae72-e713f211b165] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:05 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008dd420 2 [] false false map[] 0xc0000dd800 0xc00183ab00}
Jan 20 12:48:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:05.491617  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:05.582230  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:48:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:05.588079  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:48:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:05.588090  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:48:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:05.588750  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:48:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:05.589074  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:48:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:05.589123  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:48:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:05.589129  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:48:05 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:05.589138  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:48:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:06.164703  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:48:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:06.164780  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:06.175637  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:06 GMT] X-Content-Type-Options:[nosniff]] 0xc001702020 2 [] false false map[] 0xc000338700 0xc001f094a0}
Jan 20 12:48:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:06.175764  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:48:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:06.271863  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:48:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:06.271925  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:06.273164  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:06 GMT]] 0xc0017020a0 29 [] true false map[] 0xc000338d00 <nil>}
Jan 20 12:48:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:06.273264  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:48:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:06.483691  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:06.483766  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:06.491024  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[8121f5cd-9dec-40b0-a373-3d34d57b3cc2] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:06 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008dd480 2 [] false false map[] 0xc001c0ad00 0xc002085340}
Jan 20 12:48:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:06.491054  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:06.895101  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:48:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:06.895168  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:06.902812  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:06 GMT] X-Content-Type-Options:[nosniff]] 0xc001702560 2 [] false false map[] 0xc001c0b000 0xc001be6b00}
Jan 20 12:48:06 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:06.902864  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:48:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:07.483905  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:07.483925  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:07.488162  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[1bb4f102-8867-4b27-8efe-dbe17348d149] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:07 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001702680 2 [] false false map[] 0xc000fcd000 0xc002191340}
Jan 20 12:48:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:07.488207  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:07.525389  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/etcd.yaml"
Jan 20 12:48:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:07.527575  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver.yaml"
Jan 20 12:48:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:07.530632  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Jan 20 12:48:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:07.531762  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-scheduler.yaml"
Jan 20 12:48:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:07.532494  199956 config.go:279] "Setting pods for source" source="file"
Jan 20 12:48:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:07.581758  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:48:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:07.585572  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:48:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:07.585607  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:48:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:07.585623  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:48:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:07.586974  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:48:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:07.587124  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:48:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:07.587145  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:48:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:07.587173  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:48:07 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:07.916062  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.483111  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.483180  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.491064  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[2325745d-c344-4001-8aaf-7793ac621600] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:08 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001486900 2 [] false false map[] 0xc000339300 0xc000de6000}
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.491097  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.581561  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=2 pods=[kube-flannel/kube-flannel-ds-hprn4 default/unsigned-unencrypted-cc-1]
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.581649  199956 pod_workers.go:888] "Processing pod event" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d updateType=0
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.581711  199956 kubelet.go:1501] "syncPod enter" pod="default/unsigned-unencrypted-cc-1" podUID=4f97314d-815b-4787-b174-5c158cd28c9d
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.581717  199956 pod_workers.go:888] "Processing pod event" pod="kube-flannel/kube-flannel-ds-hprn4" podUID=04e472cb-b6f9-4065-b509-d7e1579fc48d updateType=0
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.581745  199956 kubelet_pods.go:1435] "Generating pod status" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.581802  199956 kubelet.go:1501] "syncPod enter" pod="kube-flannel/kube-flannel-ds-hprn4" podUID=04e472cb-b6f9-4065-b509-d7e1579fc48d
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.581834  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.581843  199956 kubelet_pods.go:1447] "Got phase for pod" pod="default/unsigned-unencrypted-cc-1" oldPhase=Pending phase=Pending
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.581939  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-flannel/kube-flannel-ds-hprn4" oldPhase=Running phase=Running
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.582118  199956 status_manager.go:535] "Ignoring same status for pod" pod="default/unsigned-unencrypted-cc-1" status={Phase:Pending Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason:ContainersNotReady Message:containers with unready status: [ci-example256m]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:41:08 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.16 PodIPs:[{IP:10.244.0.16}] StartTime:2023-01-20 12:41:08 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:ci-example256m State:{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "ngcn-registry.sh.intel.com:443/ci-example256m:latest",} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest ImageID: ContainerID: Started:0xc0016598c9}] QOSClass:Guaranteed EphemeralContainerStatuses:[]}
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.582422  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.582495  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.582544  199956 kuberuntime_manager.go:638] "Container of pod is not in the desired state and shall be started" containerName="ci-example256m" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.582584  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:08841bf63919f9c8acff77c51620bd8e684c550feb6b3e8784239bc3d9079457 Attempt:4 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.582654  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-flannel/kube-flannel-ds-hprn4" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:44 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:45 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:45 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.239.159.53 PodIPs:[{IP:10.239.159.53}] StartTime:2023-01-20 12:31:42 -0500 EST InitContainerStatuses:[{Name:install-cni-plugin State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2023-01-20 12:31:43 -0500 EST,FinishedAt:2023-01-20 12:31:43 -0500 EST,ContainerID:containerd://349eee1bf5a2d95206979822c956ea6f555dbb786434c7b9fe46524872f1a18d,}} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:docker.io/rancher/mirrored-flannelcni-flannel-cni-plugin:v1.1.0 ImageID:docker.io/rancher/mirrored-flannelcni-flannel-cni-plugin@sha256:28d3a6be9f450282bf42e4dad143d41da23e3d91f66f19c01ee7fd21fd17cb2b ContainerID:containerd://349eee1bf5a2d95206979822c956ea6f555dbb786434c7b9fe46524872f1a18d Started:<nil>} {Name:install-cni State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2023-01-20 12:31:43 -0500 EST,FinishedAt:2023-01-20 12:31:43 -0500 EST,ContainerID:containerd://85a4ede91a47b8d7df192dc39004a2a7fa45e095191a7335eaed8ec826cd9f36,}} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:docker.io/rancher/mirrored-flannelcni-flannel:v0.19.1 ImageID:docker.io/rancher/mirrored-flannelcni-flannel@sha256:e3b0f06121b0f7a11a684e910bba89b3ab49cd6e73aeb6c719f83b2456946366 ContainerID:containerd://85a4ede91a47b8d7df192dc39004a2a7fa45e095191a7335eaed8ec826cd9f36 Started:<nil>}] ContainerStatuses:[{Name:kube-flannel State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:44 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:docker.io/rancher/mirrored-flannelcni-flannel:v0.19.1 ImageID:docker.io/rancher/mirrored-flannelcni-flannel@sha256:e3b0f06121b0f7a11a684e910bba89b3ab49cd6e73aeb6c719f83b2456946366 ContainerID:containerd://0f9ae677fe935afcf43e145281deae0d663ba95fda6759ff24f0dd3e1cee17a9 Started:0xc00114ad99}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.582857  199956 kuberuntime_manager.go:889] "Creating container in pod" containerType="container" container="&Container{Name:ci-example256m,Image:ngcn-registry.sh.intel.com:443/ci-example256m:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},Requests:ResourceList{cpu: {{25 -3} {<nil>} 25m DecimalSI},memory: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcb8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,}" pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.582994  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.583068  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.583489  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.583647  199956 kubelet.go:1503] "syncPod exit" pod="kube-flannel/kube-flannel-ds-hprn4" podUID=04e472cb-b6f9-4065-b509-d7e1579fc48d isTerminal=false
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.583700  199956 pod_workers.go:988] "Processing pod event done" pod="kube-flannel/kube-flannel-ds-hprn4" podUID=04e472cb-b6f9-4065-b509-d7e1579fc48d updateType=0
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.583967  199956 kuberuntime_image.go:47] "Pulling image without credentials" image="ngcn-registry.sh.intel.com:443/ci-example256m:latest"
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.583981  199956 event.go:294] "Event occurred" object="default/unsigned-unencrypted-cc-1" fieldPath="spec.containers{ci-example256m}" kind="Pod" apiVersion="v1" type="Normal" reason="Pulling" message="Pulling image \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\""
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:48:08.584453278-05:00" level=info msg="PullImage \"ngcn-registry.sh.intel.com:443/ci-example256m:latest\""
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER containerd[201983]: time="2023-01-20T12:48:08.584558575-05:00" level=info msg="TaskManager get ImageService succeed." id=08841bf63919f9c8acff77c51620bd8e684c550feb6b3e8784239bc3d9079457
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.598087  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="default/unsigned-unencrypted-cc-1" volumeName="kube-api-access-qcb8g" volumeSpecName="kube-api-access-qcb8g"
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.598229  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="run" volumeSpecName="run"
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.598323  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="cni-plugin" volumeSpecName="cni-plugin"
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.598388  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="cni" volumeSpecName="cni"
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.598464  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="flannel-cfg" volumeSpecName="flannel-cfg"
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.598529  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="xtables-lock" volumeSpecName="xtables-lock"
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.598639  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-flannel/kube-flannel-ds-hprn4" volumeName="kube-api-access-hqj8d" volumeSpecName="kube-api-access-hqj8d"
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.607235  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"flannel-cfg\" (UniqueName: \"kubernetes.io/configmap/04e472cb-b6f9-4065-b509-d7e1579fc48d-flannel-cfg\") pod \"kube-flannel-ds-hprn4\" (UID: \"04e472cb-b6f9-4065-b509-d7e1579fc48d\") Volume is already mounted to pod, but remount was requested." pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.607359  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") Volume is already mounted to pod, but remount was requested." pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.607508  199956 projected.go:183] Setting up volume kube-api-access-qcb8g for pod 4f97314d-815b-4787-b174-5c158cd28c9d at /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.607550  199956 configmap.go:181] Setting up volume flannel-cfg for pod 04e472cb-b6f9-4065-b509-d7e1579fc48d at /var/lib/kubelet/pods/04e472cb-b6f9-4065-b509-d7e1579fc48d/volumes/kubernetes.io~configmap/flannel-cfg
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.607567  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-hqj8d\" (UniqueName: \"kubernetes.io/projected/04e472cb-b6f9-4065-b509-d7e1579fc48d-kube-api-access-hqj8d\") pod \"kube-flannel-ds-hprn4\" (UID: \"04e472cb-b6f9-4065-b509-d7e1579fc48d\") Volume is already mounted to pod, but remount was requested." pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.607633  199956 configmap.go:205] Received configMap kube-flannel/kube-flannel-cfg containing (2) pieces of data, 365 total bytes
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.607710  199956 quota_linux.go:271] SupportsQuotas called, but quotas disabled
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.607781  199956 projected.go:183] Setting up volume kube-api-access-hqj8d for pod 04e472cb-b6f9-4065-b509-d7e1579fc48d at /var/lib/kubelet/pods/04e472cb-b6f9-4065-b509-d7e1579fc48d/volumes/kubernetes.io~projected/kube-api-access-hqj8d
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.607930  199956 atomic_writer.go:161] pod default/unsigned-unencrypted-cc-1 volume kube-api-access-qcb8g: no update required for target directory /var/lib/kubelet/pods/4f97314d-815b-4787-b174-5c158cd28c9d/volumes/kubernetes.io~projected/kube-api-access-qcb8g
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.607980  199956 atomic_writer.go:161] pod kube-flannel/kube-flannel-ds-hprn4 volume flannel-cfg: no update required for target directory /var/lib/kubelet/pods/04e472cb-b6f9-4065-b509-d7e1579fc48d/volumes/kubernetes.io~configmap/flannel-cfg
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.607997  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-qcb8g\" (UniqueName: \"kubernetes.io/projected/4f97314d-815b-4787-b174-5c158cd28c9d-kube-api-access-qcb8g\") pod \"unsigned-unencrypted-cc-1\" (UID: \"4f97314d-815b-4787-b174-5c158cd28c9d\") " pod="default/unsigned-unencrypted-cc-1"
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.608053  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"flannel-cfg\" (UniqueName: \"kubernetes.io/configmap/04e472cb-b6f9-4065-b509-d7e1579fc48d-flannel-cfg\") pod \"kube-flannel-ds-hprn4\" (UID: \"04e472cb-b6f9-4065-b509-d7e1579fc48d\") " pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.608208  199956 atomic_writer.go:161] pod kube-flannel/kube-flannel-ds-hprn4 volume kube-api-access-hqj8d: no update required for target directory /var/lib/kubelet/pods/04e472cb-b6f9-4065-b509-d7e1579fc48d/volumes/kubernetes.io~projected/kube-api-access-hqj8d
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.608281  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-hqj8d\" (UniqueName: \"kubernetes.io/projected/04e472cb-b6f9-4065-b509-d7e1579fc48d-kube-api-access-hqj8d\") pod \"kube-flannel-ds-hprn4\" (UID: \"04e472cb-b6f9-4065-b509-d7e1579fc48d\") " pod="kube-flannel/kube-flannel-ds-hprn4"
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.879144  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.879213  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.880411  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:08 GMT] X-Content-Type-Options:[nosniff]] 0xc0014877c0 2 [] true false map[] 0xc000339b00 <nil>}
Jan 20 12:48:08 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:08.880530  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.394560  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.394567  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.394717  199956 interface.go:209] Interface eno2 is up
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.394759  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.394767  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.394772  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.394776  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.394779  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.395146  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.395155  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.395172  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.395177  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.483546  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.483607  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.491278  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[200a7c10-1f8b-4a2e-8902-7dbd4f17a93f] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:09 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00013cb00 2 [] false false map[] 0xc001716200 0xc001f5c160}
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.491307  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.582110  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr]
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.582197  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.582260  199956 pod_workers.go:888] "Processing pod event" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d updateType=0
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.582348  199956 kubelet.go:1501] "syncPod enter" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.582383  199956 kubelet_pods.go:1435] "Generating pod status" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.582487  199956 kubelet_pods.go:1447] "Got phase for pod" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" oldPhase=Running phase=Running
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.582816  199956 status_manager.go:535] "Ignoring same status for pod" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:58 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:08 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:08 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:58 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.4 PodIPs:[{IP:10.244.0.4}] StartTime:2023-01-20 12:31:58 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:kube-rbac-proxy State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:59 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:gcr.io/kubebuilder/kube-rbac-proxy:v0.13.0 ImageID:gcr.io/kubebuilder/kube-rbac-proxy@sha256:d99a8d144816b951a67648c12c0b988936ccd25cf3754f3cd85ab8c01592248f ContainerID:containerd://1cb44bdbde0b41852e382d7dab7c184af8dd4e5b25d5cfe6a10a9c8ab601659d Started:0xc0017be6ff} {Name:manager State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:59 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:quay.io/confidential-containers/operator:v0.2.0 ImageID:quay.io/confidential-containers/operator@sha256:c965b55253a9abe4c2f7596c42467fa59f2cc741bfafeed1d25629ed6f8df12d ContainerID:containerd://186424b47c7b9022ecfe8996dc73cd9101f7539259cd65914279c84c9a02a288 Started:0xc0017be710}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.583181  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.583255  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.583899  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.584085  199956 kubelet.go:1503] "syncPod exit" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d isTerminal=false
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.584142  199956 pod_workers.go:988] "Processing pod event done" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d updateType=0
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.589514  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.589576  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.591752  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.593811  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.594032  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.594066  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.594109  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.607214  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" volumeName="kube-api-access-4pnfq" volumeSpecName="kube-api-access-4pnfq"
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.613390  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-4pnfq\" (UniqueName: \"kubernetes.io/projected/d2688d45-2487-46e7-aecb-e3479626909d-kube-api-access-4pnfq\") pod \"cc-operator-controller-manager-79797456f6-m6znr\" (UID: \"d2688d45-2487-46e7-aecb-e3479626909d\") Volume is already mounted to pod, but remount was requested." pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.613604  199956 projected.go:183] Setting up volume kube-api-access-4pnfq for pod d2688d45-2487-46e7-aecb-e3479626909d at /var/lib/kubelet/pods/d2688d45-2487-46e7-aecb-e3479626909d/volumes/kubernetes.io~projected/kube-api-access-4pnfq
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.614014  199956 atomic_writer.go:161] pod confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr volume kube-api-access-4pnfq: no update required for target directory /var/lib/kubelet/pods/d2688d45-2487-46e7-aecb-e3479626909d/volumes/kubernetes.io~projected/kube-api-access-4pnfq
Jan 20 12:48:09 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:09.614089  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-4pnfq\" (UniqueName: \"kubernetes.io/projected/d2688d45-2487-46e7-aecb-e3479626909d-kube-api-access-4pnfq\") pod \"cc-operator-controller-manager-79797456f6-m6znr\" (UID: \"d2688d45-2487-46e7-aecb-e3479626909d\") " pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr"
Jan 20 12:48:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:10.483160  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:10.483234  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:10.491139  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[4a764654-06fa-4c2a-8e64-85dabe813f8d] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:10 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001718080 2 [] false false map[] 0xc000cf6300 0xc001424000}
Jan 20 12:48:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:10.491184  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:10.655760  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:48:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:10.660211  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:48:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:10.672893  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59521512Ki" capacity="65586124Ki" time="2023-01-20 12:48:10.656293736 -0500 EST m=+1003.212068561"
Jan 20 12:48:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:10.672912  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64401812Ki" capacity="65061836Ki" time="2023-01-20 12:48:10.672813619 -0500 EST m=+1003.228588452"
Jan 20 12:48:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:10.672922  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="901993032Ki" capacity="981310056Ki" time="2023-01-20 12:48:10.656293736 -0500 EST m=+1003.212068561"
Jan 20 12:48:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:10.672931  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:48:10.656293736 -0500 EST m=+1003.212068561"
Jan 20 12:48:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:10.672940  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="901993032Ki" capacity="981310056Ki" time="2023-01-20 12:48:01.261611671 -0500 EST"
Jan 20 12:48:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:10.672949  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:48:01.261611671 -0500 EST"
Jan 20 12:48:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:10.672957  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510788" capacity="511757" time="2023-01-20 12:48:10.672416939 -0500 EST m=+1003.228191773"
Jan 20 12:48:10 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:10.672995  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:48:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:11.166523  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:48:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:11.166546  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:11.170576  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[4aef7363-230d-4e08-9bf9-fc6ab66a597b] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:11 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e8480 2 [] false false map[] 0xc001716500 0xc0017c16b0}
Jan 20 12:48:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:11.170620  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:11.483725  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:11.483794  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:11.491397  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[743f12fe-8369-4ad8-b2bf-69f008f601f5] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:11 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123e180 2 [] false false map[] 0xc001a4bf00 0xc0017c18c0}
Jan 20 12:48:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:11.491427  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:11.581219  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:48:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:11.587049  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:48:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:11.587062  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:48:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:11.587069  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:48:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:11.587811  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:48:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:11.587854  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:48:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:11.587860  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:48:11 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:11.587868  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.484177  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.484249  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.492047  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[909eba0a-b01c-40c0-961c-f93b0e581ef7] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:12 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123e1e0 2 [] false false map[] 0xc000fcc900 0xc001ae6210}
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.492079  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.581825  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=2 pods=[kube-system/coredns-6d4b75cb6d-zdl2m kube-system/kube-apiserver-zcy-z390-aorus-master]
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.581904  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd updateType=0
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.581962  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.581977  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e updateType=0
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.581997  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/kube-apiserver-zcy-z390-aorus-master"
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.582058  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.582090  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.582105  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" oldPhase=Running phase=Running
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.582176  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/coredns-6d4b75cb6d-zdl2m" oldPhase=Running phase=Running
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.582384  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:28 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:41 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:41 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:28 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.239.159.53 PodIPs:[{IP:10.239.159.53}] StartTime:2023-01-20 12:31:28 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:kube-apiserver State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:22 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:6 Image:k8s.gcr.io/kube-apiserver:v1.24.0 ImageID:k8s.gcr.io/kube-apiserver@sha256:a04522b882e919de6141b47d72393fb01226c78e7388400f966198222558c955 ContainerID:containerd://5900ac5c1383a321a6ae837b57d6d29d7a660a102c0806210a9ea57290673062 Started:0xc001ea7a19}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.582731  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-apiserver-zcy-z390-aorus-master"
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.582799  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/coredns-6d4b75cb6d-zdl2m" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:44 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:44 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.3 PodIPs:[{IP:10.244.0.3}] StartTime:2023-01-20 12:31:42 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:coredns State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:44 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:k8s.gcr.io/coredns/coredns:v1.8.6 ImageID:k8s.gcr.io/coredns/coredns@sha256:5b6ec0d6de9baaf3e92d0f66cd96a25b9edbce8716f5f15dcd1a616b3abd590e ContainerID:containerd://e489181a7f95431b6d92f037dbe3f788b46a5e024df7aaf29e0115dab1168ab1 Started:0xc00094ead9}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.582806  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/kube-apiserver-zcy-z390-aorus-master"
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.583131  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.583199  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.583693  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.583861  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e isTerminal=false
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.583916  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e updateType=0
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.583925  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/kube-apiserver-zcy-z390-aorus-master"
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.584085  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd isTerminal=false
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.584136  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd updateType=0
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.629241  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/coredns-6d4b75cb6d-zdl2m" volumeName="config-volume" volumeSpecName="config-volume"
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.629392  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/coredns-6d4b75cb6d-zdl2m" volumeName="kube-api-access-tqzsm" volumeSpecName="kube-api-access-tqzsm"
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.629505  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" volumeName="ca-certs" volumeSpecName="ca-certs"
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.629574  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" volumeName="etc-ca-certificates" volumeSpecName="etc-ca-certificates"
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.629640  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" volumeName="etc-pki" volumeSpecName="etc-pki"
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.629705  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" volumeName="k8s-certs" volumeSpecName="k8s-certs"
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.629771  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" volumeName="usr-local-share-ca-certificates" volumeSpecName="usr-local-share-ca-certificates"
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.629837  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" volumeName="usr-share-ca-certificates" volumeSpecName="usr-share-ca-certificates"
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.633443  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/cf25b9ea-6823-4eff-b30d-36a09f9d897e-config-volume\") pod \"coredns-6d4b75cb6d-zdl2m\" (UID: \"cf25b9ea-6823-4eff-b30d-36a09f9d897e\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.633560  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-tqzsm\" (UniqueName: \"kubernetes.io/projected/cf25b9ea-6823-4eff-b30d-36a09f9d897e-kube-api-access-tqzsm\") pod \"coredns-6d4b75cb6d-zdl2m\" (UID: \"cf25b9ea-6823-4eff-b30d-36a09f9d897e\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.633695  199956 projected.go:183] Setting up volume kube-api-access-tqzsm for pod cf25b9ea-6823-4eff-b30d-36a09f9d897e at /var/lib/kubelet/pods/cf25b9ea-6823-4eff-b30d-36a09f9d897e/volumes/kubernetes.io~projected/kube-api-access-tqzsm
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.633791  199956 configmap.go:181] Setting up volume config-volume for pod cf25b9ea-6823-4eff-b30d-36a09f9d897e at /var/lib/kubelet/pods/cf25b9ea-6823-4eff-b30d-36a09f9d897e/volumes/kubernetes.io~configmap/config-volume
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.633916  199956 configmap.go:205] Received configMap kube-system/coredns containing (1) pieces of data, 339 total bytes
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.633994  199956 quota_linux.go:271] SupportsQuotas called, but quotas disabled
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.634110  199956 atomic_writer.go:161] pod kube-system/coredns-6d4b75cb6d-zdl2m volume kube-api-access-tqzsm: no update required for target directory /var/lib/kubelet/pods/cf25b9ea-6823-4eff-b30d-36a09f9d897e/volumes/kubernetes.io~projected/kube-api-access-tqzsm
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.634177  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-tqzsm\" (UniqueName: \"kubernetes.io/projected/cf25b9ea-6823-4eff-b30d-36a09f9d897e-kube-api-access-tqzsm\") pod \"coredns-6d4b75cb6d-zdl2m\" (UID: \"cf25b9ea-6823-4eff-b30d-36a09f9d897e\") " pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.634179  199956 atomic_writer.go:161] pod kube-system/coredns-6d4b75cb6d-zdl2m volume config-volume: no update required for target directory /var/lib/kubelet/pods/cf25b9ea-6823-4eff-b30d-36a09f9d897e/volumes/kubernetes.io~configmap/config-volume
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.634256  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/cf25b9ea-6823-4eff-b30d-36a09f9d897e-config-volume\") pod \"coredns-6d4b75cb6d-zdl2m\" (UID: \"cf25b9ea-6823-4eff-b30d-36a09f9d897e\") " pod="kube-system/coredns-6d4b75cb6d-zdl2m"
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.689632  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.689691  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.690798  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:12 GMT]] 0xc0012e9d60 2 [] true false map[] 0xc0014c6300 <nil>}
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.690906  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.696069  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.696131  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.697202  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:12 GMT]] 0xc0012e9da0 2 [] true false map[] 0xc0014c6800 <nil>}
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.697314  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.710529  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.710595  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.710599  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.710656  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.711620  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:12 GMT]] 0xc000ffa120 2 [] true false map[] 0xc0021a2700 <nil>}
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.711721  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.711689  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:12 GMT]] 0xc0012e9e00 2 [] true false map[] 0xc0014c6a00 <nil>}
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.711797  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:48:12 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:12.918000  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:48:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:13.483158  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:13.483229  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:13.491409  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[5218de2c-ddd0-44d7-bc3f-6f4b0d2e6b1c] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:13 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008e2a40 2 [] false false map[] 0xc0014ed000 0xc001485550}
Jan 20 12:48:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:13.491474  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:13.581972  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[confidential-containers-system/cc-operator-daemon-install-t6mp7]
Jan 20 12:48:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:13.582058  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:48:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:13.582191  199956 pod_workers.go:888] "Processing pod event" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" podUID=7af065b7-9095-4d91-9b9e-2644e7b1f4bf updateType=0
Jan 20 12:48:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:13.582278  199956 kubelet.go:1501] "syncPod enter" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" podUID=7af065b7-9095-4d91-9b9e-2644e7b1f4bf
Jan 20 12:48:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:13.582319  199956 kubelet_pods.go:1435] "Generating pod status" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7"
Jan 20 12:48:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:13.582405  199956 kubelet_pods.go:1447] "Got phase for pod" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" oldPhase=Running phase=Running
Jan 20 12:48:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:13.582693  199956 status_manager.go:535] "Ignoring same status for pod" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:04 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:20 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:20 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:32:04 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.6 PodIPs:[{IP:10.244.0.6}] StartTime:2023-01-20 12:32:04 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:cc-runtime-install-pod State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:32:20 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:quay.io/confidential-containers/runtime-payload-ci:kata-containers-amd64 ImageID:quay.io/confidential-containers/runtime-payload-ci@sha256:4736ba274765c889404fb98f01de0a997e68d2d7e5acca2440488f0e1337032b ContainerID:containerd://e10efaa459a32161059fda75d4bcaf3bbdb896781f772b508e43fe00bc7c9003 Started:0xc001d74a5e}] QOSClass:BestEffort EphemeralContainerStatuses:[]}
Jan 20 12:48:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:13.583049  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7"
Jan 20 12:48:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:13.583120  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7"
Jan 20 12:48:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:13.583541  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="confidential-containers-system/cc-operator-daemon-install-t6mp7"
Jan 20 12:48:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:13.583704  199956 kubelet.go:1503] "syncPod exit" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" podUID=7af065b7-9095-4d91-9b9e-2644e7b1f4bf isTerminal=false
Jan 20 12:48:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:13.583761  199956 pod_workers.go:988] "Processing pod event done" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" podUID=7af065b7-9095-4d91-9b9e-2644e7b1f4bf updateType=0
Jan 20 12:48:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:13.589426  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:48:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:13.589486  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:48:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:13.591701  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:48:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:13.593732  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:48:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:13.593948  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:48:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:13.593979  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:48:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:13.594020  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:48:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:13.638207  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" volumeName="containerd-conf" volumeSpecName="containerd-conf"
Jan 20 12:48:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:13.638318  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" volumeName="kata-artifacts" volumeSpecName="kata-artifacts"
Jan 20 12:48:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:13.638393  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" volumeName="dbus" volumeSpecName="dbus"
Jan 20 12:48:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:13.638460  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" volumeName="systemd" volumeSpecName="systemd"
Jan 20 12:48:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:13.638529  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" volumeName="local-bin" volumeSpecName="local-bin"
Jan 20 12:48:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:13.638690  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="confidential-containers-system/cc-operator-daemon-install-t6mp7" volumeName="kube-api-access-x6vjr" volumeSpecName="kube-api-access-x6vjr"
Jan 20 12:48:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:13.640316  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-x6vjr\" (UniqueName: \"kubernetes.io/projected/7af065b7-9095-4d91-9b9e-2644e7b1f4bf-kube-api-access-x6vjr\") pod \"cc-operator-daemon-install-t6mp7\" (UID: \"7af065b7-9095-4d91-9b9e-2644e7b1f4bf\") Volume is already mounted to pod, but remount was requested." pod="confidential-containers-system/cc-operator-daemon-install-t6mp7"
Jan 20 12:48:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:13.640500  199956 projected.go:183] Setting up volume kube-api-access-x6vjr for pod 7af065b7-9095-4d91-9b9e-2644e7b1f4bf at /var/lib/kubelet/pods/7af065b7-9095-4d91-9b9e-2644e7b1f4bf/volumes/kubernetes.io~projected/kube-api-access-x6vjr
Jan 20 12:48:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:13.640940  199956 atomic_writer.go:161] pod confidential-containers-system/cc-operator-daemon-install-t6mp7 volume kube-api-access-x6vjr: no update required for target directory /var/lib/kubelet/pods/7af065b7-9095-4d91-9b9e-2644e7b1f4bf/volumes/kubernetes.io~projected/kube-api-access-x6vjr
Jan 20 12:48:13 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:13.641019  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-x6vjr\" (UniqueName: \"kubernetes.io/projected/7af065b7-9095-4d91-9b9e-2644e7b1f4bf-kube-api-access-x6vjr\") pod \"cc-operator-daemon-install-t6mp7\" (UID: \"7af065b7-9095-4d91-9b9e-2644e7b1f4bf\") " pod="confidential-containers-system/cc-operator-daemon-install-t6mp7"
Jan 20 12:48:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:14.483132  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:14.483202  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:14.491146  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[4efa1406-a960-4066-a66e-b26c6d44b3c5] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:14 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0009797e0 2 [] false false map[] 0xc0014c6c00 0xc001d418c0}
Jan 20 12:48:14 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:14.491178  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:15.483413  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:15.483485  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:15.491765  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[32c2c2fa-37f7-4303-a6c8-153f884e0748] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:15 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000978680 2 [] false false map[] 0xc0021a2100 0xc000de6160}
Jan 20 12:48:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:15.491794  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:15.581718  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:48:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:15.587126  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:48:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:15.587138  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:48:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:15.587144  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:48:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:15.587924  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:48:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:15.587972  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:48:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:15.587979  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:48:15 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:15.587988  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:48:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:16.164758  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:48:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:16.164782  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:16.167861  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:16 GMT] X-Content-Type-Options:[nosniff]] 0xc0012e9640 2 [] false false map[] 0xc001f37500 0xc000afe6e0}
Jan 20 12:48:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:16.167905  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:48:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:16.271969  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:48:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:16.272028  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:16.273237  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:16 GMT]] 0xc0008dc860 29 [] true false map[] 0xc001f37700 <nil>}
Jan 20 12:48:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:16.273343  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:48:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:16.484058  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:16.484122  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:16.492091  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[34fda139-f4a6-4a27-ad18-669dab3a497f] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:16 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0009790e0 2 [] false false map[] 0xc000fcc100 0xc001982630}
Jan 20 12:48:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:16.492137  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:16.895772  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:48:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:16.895845  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:16.903764  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:16 GMT] X-Content-Type-Options:[nosniff]] 0xc0008dcf00 2 [] false false map[] 0xc000fcc500 0xc001982840}
Jan 20 12:48:16 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:16.903824  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:48:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:17.483219  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:17.483288  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:17.491626  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[d39f2c6d-229e-4042-888e-99bb14acad09] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:17 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e9c00 2 [] false false map[] 0xc000fcc800 0xc001019a20}
Jan 20 12:48:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:17.491697  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:17.581802  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:48:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:17.587998  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:48:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:17.588010  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:48:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:17.588660  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:48:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:17.589073  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:48:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:17.589116  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:48:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:17.589122  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:48:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:17.589130  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:48:17 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:17.919403  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:48:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:18.483769  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:18.483836  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:18.492290  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[abb48a98-ab30-40ee-b542-f4eb5f6fc5f4] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:18 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0009797a0 2 [] false false map[] 0xc000fcca00 0xc0012096b0}
Jan 20 12:48:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:18.492321  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:18.879199  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/healthz" timeout="1s"
Jan 20 12:48:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:18.879252  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:48:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:18.879271  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:18.879322  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:18.880514  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:18 GMT] X-Content-Type-Options:[nosniff]] 0xc0009a0180 2 [] true false map[] 0xc000fcd500 <nil>}
Jan 20 12:48:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:18.880542  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:18 GMT] X-Content-Type-Options:[nosniff]] 0xc0014860a0 2 [] true false map[] 0xc0021a3200 <nil>}
Jan 20 12:48:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:18.880634  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:48:18 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:18.880658  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:48:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:19.483214  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:19.483287  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:19.491386  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[2b033109-2f11-4e9e-8389-786f32f6c369] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:19 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0009a06e0 2 [] false false map[] 0xc000338700 0xc0016ec210}
Jan 20 12:48:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:19.491418  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:19.579193  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:48:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:19.579230  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:48:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:19.579854  199956 interface.go:209] Interface eno2 is up
Jan 20 12:48:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:19.579981  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:48:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:19.580015  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:48:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:19.580040  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:48:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:19.580055  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:48:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:19.580074  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:48:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:19.581050  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:48:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:19.581099  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:48:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:19.581128  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:48:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:19.581147  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:48:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:19.581169  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:48:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:19.587069  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:48:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:19.587083  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:48:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:19.587090  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:48:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:19.587530  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:48:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:19.587576  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:48:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:19.587582  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:48:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:19.587590  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:48:19 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:19.939096  199956 handler.go:293] error while reading "/proc/199956/fd/34" link: readlink /proc/199956/fd/34: no such file or directory
Jan 20 12:48:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:20.483206  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:20.483270  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:20.491513  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[eabea85a-ed67-458e-95cb-1bb2fe1a78c0] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:20 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ce9a60 2 [] false false map[] 0xc000cf7400 0xc0014b1970}
Jan 20 12:48:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:20.491566  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:20.673908  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:48:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:20.682469  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:48:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:20.692357  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="901992980Ki" capacity="981310056Ki" time="2023-01-20 12:48:11.262152067 -0500 EST"
Jan 20 12:48:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:20.692371  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:48:11.262152067 -0500 EST"
Jan 20 12:48:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:20.692378  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510787" capacity="511757" time="2023-01-20 12:48:20.692039141 -0500 EST m=+1013.247813966"
Jan 20 12:48:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:20.692384  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59492560Ki" capacity="65586124Ki" time="2023-01-20 12:48:20.675910065 -0500 EST m=+1013.231684957"
Jan 20 12:48:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:20.692390  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64401244Ki" capacity="65061836Ki" time="2023-01-20 12:48:20.692300456 -0500 EST m=+1013.248075280"
Jan 20 12:48:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:20.692397  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="901992980Ki" capacity="981310056Ki" time="2023-01-20 12:48:20.675910065 -0500 EST m=+1013.231684957"
Jan 20 12:48:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:20.692403  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:48:20.675910065 -0500 EST m=+1013.231684957"
Jan 20 12:48:20 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:20.692428  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:48:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:21.165842  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:48:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:21.165918  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:21.174460  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[57775293-e734-4305-a6c8-2fedec06af57] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:21 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0020fc040 2 [] false false map[] 0xc001716700 0xc001e87810}
Jan 20 12:48:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:21.174491  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:21.483488  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:21.483557  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:21.491479  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[f1f85fa0-ab18-421f-a188-243305293a21] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:21 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0020fc140 2 [] false false map[] 0xc000fcc100 0xc0002b7c30}
Jan 20 12:48:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:21.491507  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:21.581831  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:48:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:21.589175  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:48:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:21.589236  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:48:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:21.591489  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:48:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:21.593596  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:48:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:21.593825  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:48:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:21.593858  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:48:21 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:21.593900  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:48:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:22.484150  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:22.484219  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:22.492457  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[dd35d46a-44f1-476a-a45e-704fa6fb0a7f] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:22 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0020fc860 2 [] false false map[] 0xc001a4a300 0xc000b1fce0}
Jan 20 12:48:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:22.492484  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:22.689979  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:48:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:22.690054  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:22.691170  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:22 GMT]] 0xc001703e60 2 [] true false map[] 0xc000338a00 <nil>}
Jan 20 12:48:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:22.691284  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:48:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:22.696372  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:48:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:22.696435  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:22.697498  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:22 GMT]] 0xc00123f9e0 2 [] true false map[] 0xc001a4a500 <nil>}
Jan 20 12:48:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:22.697605  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:48:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:22.709838  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:48:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:22.709901  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:22.709917  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:48:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:22.709975  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:22.710927  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:22 GMT]] 0xc001703ea0 2 [] true false map[] 0xc0014c6100 <nil>}
Jan 20 12:48:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:22.710946  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:22 GMT]] 0xc00123fa40 2 [] true false map[] 0xc001a4a700 <nil>}
Jan 20 12:48:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:22.711030  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:48:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:22.711057  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:48:22 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:22.921283  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:48:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:23.484190  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:23.484255  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:23.492541  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[1a4ded9e-a90c-4fd9-8dcc-d9671697af46] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:23 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ce8940 2 [] false false map[] 0xc001716b00 0xc0014f3550}
Jan 20 12:48:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:23.492568  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:23.581202  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:48:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:23.588565  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:48:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:23.588627  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:48:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:23.588657  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:48:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:23.590882  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:48:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:23.591101  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:48:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:23.591135  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:48:23 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:23.591178  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:48:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:24.483686  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:24.483718  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:24.489752  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[4879377b-eb3f-48d2-847e-7bda3932e6cb] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:24 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e9340 2 [] false false map[] 0xc001f37400 0xc001209130}
Jan 20 12:48:24 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:24.489825  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:25.484020  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:25.484086  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:25.492131  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[cfc02af3-1a26-400c-bf52-f3dab48ef3e9] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:25 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001821b80 2 [] false false map[] 0xc001a4a900 0xc001209290}
Jan 20 12:48:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:25.492164  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:25.581729  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:48:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:25.585006  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:48:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:25.585034  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:48:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:25.586002  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:48:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:25.586810  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:48:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:25.586930  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:48:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:25.586948  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:48:25 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:25.586970  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:48:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:26.164836  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:48:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:26.164914  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:26.172644  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:26 GMT] X-Content-Type-Options:[nosniff]] 0xc001821c00 2 [] false false map[] 0xc0016aa400 0xc001b4c210}
Jan 20 12:48:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:26.172675  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:48:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:26.272495  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:48:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:26.272520  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:26.272956  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:26 GMT]] 0xc001821c80 29 [] true false map[] 0xc001717200 <nil>}
Jan 20 12:48:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:26.272994  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:48:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:26.483776  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:26.483839  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:26.491704  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[1009b6a7-7cc8-4a2a-8eb7-099b01e35a02] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:26 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000978920 2 [] false false map[] 0xc001717400 0xc001acfad0}
Jan 20 12:48:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:26.491734  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:26.895619  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:48:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:26.895690  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:26.902952  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:26 GMT] X-Content-Type-Options:[nosniff]] 0xc0009a07e0 2 [] false false map[] 0xc001f37a00 0xc001e00fd0}
Jan 20 12:48:26 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:26.902990  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.483185  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.483252  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.491694  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[b2596430-f3e9-436f-97dc-291ad8e04e97] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:27 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0014869a0 2 [] false false map[] 0xc0016ab200 0xc001dec160}
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.491737  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.526029  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/etcd.yaml"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.528120  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver.yaml"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.531100  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.533972  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-scheduler.yaml"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.535584  199956 config.go:279] "Setting pods for source" source="file"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.542948  199956 kubelet_getters.go:176] "Pod status updated" pod="kube-system/etcd-zcy-z390-aorus-master" status=Running
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.543036  199956 kubelet_getters.go:176] "Pod status updated" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" status=Running
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.543060  199956 kubelet_getters.go:176] "Pod status updated" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" status=Running
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.543080  199956 kubelet_getters.go:176] "Pod status updated" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" status=Running
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.581194  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[kube-system/etcd-zcy-z390-aorus-master]
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.581282  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.581375  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d updateType=0
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.581452  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.581486  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/etcd-zcy-z390-aorus-master"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.581573  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/etcd-zcy-z390-aorus-master" oldPhase=Running phase=Running
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.582301  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/etcd-zcy-z390-aorus-master" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:28 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:37 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:37 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:28 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.239.159.53 PodIPs:[{IP:10.239.159.53}] StartTime:2023-01-20 12:31:28 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:etcd State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:22 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:6 Image:k8s.gcr.io/etcd:3.5.3-0 ImageID:k8s.gcr.io/etcd@sha256:13f53ed1d91e2e11aac476ee9a0269fdda6cc4874eba903efd40daf50c55eee5 ContainerID:containerd://9b11acac1b891eafc7ff2b244f1c42938f71a575ce81ff917e3b7ebf61d2e045 Started:0xc0018f6292}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.582666  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/etcd-zcy-z390-aorus-master"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.582741  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/etcd-zcy-z390-aorus-master"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.583376  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/etcd-zcy-z390-aorus-master"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.583538  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d isTerminal=false
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.583594  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d updateType=0
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.587137  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.587149  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.587928  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.588398  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.588445  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.588452  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.588461  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.629485  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/etcd-zcy-z390-aorus-master" volumeName="etcd-certs" volumeSpecName="etcd-certs"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.629509  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/etcd-zcy-z390-aorus-master" volumeName="etcd-data" volumeSpecName="etcd-data"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.629929  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-systemd\\x2dfsck.slice"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.629938  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-systemd\\x2dfsck.slice"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.629944  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-systemd\\x2dfsck.slice", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.629950  199956 manager.go:925] ignoring container "/system.slice/system-systemd\\x2dfsck.slice"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.629953  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-snapd-ns.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.629957  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-snapd-ns.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.629961  199956 manager.go:925] ignoring container "/system.slice/run-snapd-ns.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.629964  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/rsyslog.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.629967  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/rsyslog.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.629970  199956 factory.go:255] Factory "raw" can handle container "/system.slice/rsyslog.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.629974  199956 manager.go:925] ignoring container "/system.slice/rsyslog.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.629977  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/blk-availability.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.629980  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/blk-availability.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.629983  199956 factory.go:255] Factory "raw" can handle container "/system.slice/blk-availability.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.629987  199956 manager.go:925] ignoring container "/system.slice/blk-availability.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.629990  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/boot-efi.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.629993  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/boot-efi.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.629997  199956 manager.go:925] ignoring container "/system.slice/boot-efi.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630000  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/accounts-daemon.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630003  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/accounts-daemon.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630006  199956 factory.go:255] Factory "raw" can handle container "/system.slice/accounts-daemon.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630010  199956 manager.go:925] ignoring container "/system.slice/accounts-daemon.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630013  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/upower.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630015  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/upower.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630019  199956 factory.go:255] Factory "raw" can handle container "/system.slice/upower.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630022  199956 manager.go:925] ignoring container "/system.slice/upower.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630025  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-user-sessions.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630028  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-user-sessions.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630031  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-user-sessions.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630035  199956 manager.go:925] ignoring container "/system.slice/systemd-user-sessions.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630038  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-sysctl.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630041  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-sysctl.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630044  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-sysctl.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630048  199956 manager.go:925] ignoring container "/system.slice/systemd-sysctl.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630051  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/alsa-restore.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630053  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/alsa-restore.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630057  199956 factory.go:255] Factory "raw" can handle container "/system.slice/alsa-restore.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630060  199956 manager.go:925] ignoring container "/system.slice/alsa-restore.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630063  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630068  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-rootfs.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630073  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630076  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/acpid.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630079  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/acpid.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630082  199956 factory.go:255] Factory "raw" can handle container "/system.slice/acpid.socket", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630087  199956 manager.go:925] ignoring container "/system.slice/acpid.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630089  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-fs-fuse-connections.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630093  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-fs-fuse-connections.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630097  199956 manager.go:925] ignoring container "/system.slice/sys-fs-fuse-connections.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630099  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/unattended-upgrades.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630102  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/unattended-upgrades.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630105  199956 factory.go:255] Factory "raw" can handle container "/system.slice/unattended-upgrades.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630109  199956 manager.go:925] ignoring container "/system.slice/unattended-upgrades.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630112  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snapd.seeded.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630115  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/snapd.seeded.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630118  199956 factory.go:255] Factory "raw" can handle container "/system.slice/snapd.seeded.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630122  199956 manager.go:925] ignoring container "/system.slice/snapd.seeded.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630124  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/user@0.service/dbus.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630127  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/user@0.service/dbus.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630131  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/user@0.service/dbus.socket", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630135  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/user@0.service/dbus.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630138  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630142  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-rootfs.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630147  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630151  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-gtk\\x2dcommon\\x2dthemes-1535.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630154  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-gtk\\x2dcommon\\x2dthemes-1535.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630158  199956 manager.go:925] ignoring container "/system.slice/snap-gtk\\x2dcommon\\x2dthemes-1535.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630161  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-snap\\x2dstore-558.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630164  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-snap\\x2dstore-558.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630168  199956 manager.go:925] ignoring container "/system.slice/snap-snap\\x2dstore-558.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630171  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-shm.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630175  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-shm.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630180  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-8a888a08b37b70c1ad7739d652b322da5ea43c126008d63ce959264410fe329b-shm.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630184  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630188  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-rootfs.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630193  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630196  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/ufw.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630199  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/ufw.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630202  199956 factory.go:255] Factory "raw" can handle container "/system.slice/ufw.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630206  199956 manager.go:925] ignoring container "/system.slice/ufw.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630208  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/colord.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630211  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/colord.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630214  199956 factory.go:255] Factory "raw" can handle container "/system.slice/colord.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630218  199956 manager.go:925] ignoring container "/system.slice/colord.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630221  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dev-hugepages.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630224  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/dev-hugepages.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630228  199956 manager.go:925] ignoring container "/system.slice/dev-hugepages.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630231  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-modules-load.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630234  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-modules-load.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630239  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-modules-load.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630243  199956 manager.go:925] ignoring container "/system.slice/systemd-modules-load.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630246  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2d58469e7a\\x2dc331\\x2d785c\\x2dc760\\x2d93c7232334bd.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630250  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2d58469e7a\\x2dc331\\x2d785c\\x2dc760\\x2d93c7232334bd.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630255  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2d58469e7a\\x2dc331\\x2d785c\\x2dc760\\x2d93c7232334bd.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630258  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-9b11acac1b891eafc7ff2b244f1c42938f71a575ce81ff917e3b7ebf61d2e045-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630262  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-9b11acac1b891eafc7ff2b244f1c42938f71a575ce81ff917e3b7ebf61d2e045-rootfs.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630267  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-9b11acac1b891eafc7ff2b244f1c42938f71a575ce81ff917e3b7ebf61d2e045-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630271  199956 factory.go:262] Factory "containerd" was unable to handle container "/docker"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630273  199956 factory.go:262] Factory "systemd" was unable to handle container "/docker"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630276  199956 factory.go:255] Factory "raw" can handle container "/docker", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630280  199956 manager.go:925] ignoring container "/docker"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630283  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/gdm.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630286  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/gdm.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630289  199956 factory.go:255] Factory "raw" can handle container "/system.slice/gdm.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630293  199956 manager.go:925] ignoring container "/system.slice/gdm.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630296  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-d94e1927\\x2dd22d\\x2d4831\\x2da764\\x2d0170eae346a1-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d9qh7j.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630300  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-d94e1927\\x2dd22d\\x2d4831\\x2da764\\x2d0170eae346a1-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d9qh7j.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630306  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-d94e1927\\x2dd22d\\x2d4831\\x2da764\\x2d0170eae346a1-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d9qh7j.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630310  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630314  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-rootfs.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630319  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630323  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/user@0.service/dbus.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630326  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/user@0.service/dbus.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630329  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/user@0.service/dbus.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630333  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/user@0.service/dbus.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630336  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/networkd-dispatcher.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630339  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/networkd-dispatcher.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630342  199956 factory.go:255] Factory "raw" can handle container "/system.slice/networkd-dispatcher.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630346  199956 manager.go:925] ignoring container "/system.slice/networkd-dispatcher.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630348  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2d562f9e70\\x2db2b7\\x2d3ac4\\x2d2311\\x2db91510a5a9ac.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630352  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2d562f9e70\\x2db2b7\\x2d3ac4\\x2d2311\\x2db91510a5a9ac.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630358  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2d562f9e70\\x2db2b7\\x2d3ac4\\x2d2311\\x2db91510a5a9ac.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630361  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-timesyncd.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630364  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-timesyncd.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630367  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-timesyncd.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630371  199956 manager.go:925] ignoring container "/system.slice/systemd-timesyncd.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630374  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/cups.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630377  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/cups.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630380  199956 factory.go:255] Factory "raw" can handle container "/system.slice/cups.socket", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630384  199956 manager.go:925] ignoring container "/system.slice/cups.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630386  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-7af065b7\\x2d9095\\x2d4d91\\x2d9b9e\\x2d2644e7b1f4bf-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dx6vjr.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630391  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-7af065b7\\x2d9095\\x2d4d91\\x2d9b9e\\x2d2644e7b1f4bf-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dx6vjr.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630396  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-7af065b7\\x2d9095\\x2d4d91\\x2d9b9e\\x2d2644e7b1f4bf-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dx6vjr.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630400  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/avahi-daemon.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630403  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/avahi-daemon.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630406  199956 factory.go:255] Factory "raw" can handle container "/system.slice/avahi-daemon.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630410  199956 manager.go:925] ignoring container "/system.slice/avahi-daemon.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630413  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-d2688d45\\x2d2487\\x2d46e7\\x2daecb\\x2de3479626909d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d4pnfq.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630417  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-d2688d45\\x2d2487\\x2d46e7\\x2daecb\\x2de3479626909d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d4pnfq.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630423  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-d2688d45\\x2d2487\\x2d46e7\\x2daecb\\x2de3479626909d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d4pnfq.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630427  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/wpa_supplicant.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630429  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/wpa_supplicant.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630432  199956 factory.go:255] Factory "raw" can handle container "/system.slice/wpa_supplicant.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630436  199956 manager.go:925] ignoring container "/system.slice/wpa_supplicant.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630439  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/console-setup.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630442  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/console-setup.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630445  199956 factory.go:255] Factory "raw" can handle container "/system.slice/console-setup.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630449  199956 manager.go:925] ignoring container "/system.slice/console-setup.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630452  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journald.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630455  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journald.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630458  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journald.socket", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630462  199956 manager.go:925] ignoring container "/system.slice/systemd-journald.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630465  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630470  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-rootfs.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630475  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630478  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dm-event.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630481  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/dm-event.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630484  199956 factory.go:255] Factory "raw" can handle container "/system.slice/dm-event.socket", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630488  199956 manager.go:925] ignoring container "/system.slice/dm-event.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630491  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/lvm2-monitor.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630493  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/lvm2-monitor.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630497  199956 factory.go:255] Factory "raw" can handle container "/system.slice/lvm2-monitor.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630501  199956 manager.go:925] ignoring container "/system.slice/lvm2-monitor.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630503  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/cron.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630506  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/cron.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630509  199956 factory.go:255] Factory "raw" can handle container "/system.slice/cron.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630513  199956 manager.go:925] ignoring container "/system.slice/cron.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630516  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-5900ac5c1383a321a6ae837b57d6d29d7a660a102c0806210a9ea57290673062-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630520  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-5900ac5c1383a321a6ae837b57d6d29d7a660a102c0806210a9ea57290673062-rootfs.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630525  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-5900ac5c1383a321a6ae837b57d6d29d7a660a102c0806210a9ea57290673062-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630529  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journald-dev-log.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630532  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journald-dev-log.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630535  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journald-dev-log.socket", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630539  199956 manager.go:925] ignoring container "/system.slice/systemd-journald-dev-log.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630542  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dev-mqueue.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630545  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/dev-mqueue.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630549  199956 manager.go:925] ignoring container "/system.slice/dev-mqueue.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630552  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-lvm2\\x2dpvscan.slice/lvm2-pvscan@8:10.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630555  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-lvm2\\x2dpvscan.slice/lvm2-pvscan@8:10.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630558  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-lvm2\\x2dpvscan.slice/lvm2-pvscan@8:10.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630563  199956 manager.go:925] ignoring container "/system.slice/system-lvm2\\x2dpvscan.slice/lvm2-pvscan@8:10.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630566  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630568  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630571  199956 factory.go:255] Factory "raw" can handle container "/user.slice", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630575  199956 manager.go:925] ignoring container "/user.slice"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630579  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-rfkill.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630581  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-rfkill.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630584  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-rfkill.socket", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630589  199956 manager.go:925] ignoring container "/system.slice/systemd-rfkill.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630591  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-shm.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630595  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-shm.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630600  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-317c1653f9d7a951769a5a23b66c4d8424b379460af166c4251f459c6970a7b4-shm.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630604  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/NetworkManager.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630607  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/NetworkManager.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630610  199956 factory.go:255] Factory "raw" can handle container "/system.slice/NetworkManager.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630614  199956 manager.go:925] ignoring container "/system.slice/NetworkManager.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630617  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-shm.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630621  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-shm.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630626  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-shm.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630629  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-systemd\\x2dfsck.slice/systemd-fsck@dev-disk-by\\x2duuid-4B5A\\x2dDC89.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630633  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-systemd\\x2dfsck.slice/systemd-fsck@dev-disk-by\\x2duuid-4B5A\\x2dDC89.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630637  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-systemd\\x2dfsck.slice/systemd-fsck@dev-disk-by\\x2duuid-4B5A\\x2dDC89.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630641  199956 manager.go:925] ignoring container "/system.slice/system-systemd\\x2dfsck.slice/systemd-fsck@dev-disk-by\\x2duuid-4B5A\\x2dDC89.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630644  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/ssh.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630647  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/ssh.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630650  199956 factory.go:255] Factory "raw" can handle container "/system.slice/ssh.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630654  199956 manager.go:925] ignoring container "/system.slice/ssh.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630657  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dbus.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630660  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/dbus.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630663  199956 factory.go:255] Factory "raw" can handle container "/system.slice/dbus.socket", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630667  199956 manager.go:925] ignoring container "/system.slice/dbus.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630669  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/keyboard-setup.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630672  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/keyboard-setup.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630675  199956 factory.go:255] Factory "raw" can handle container "/system.slice/keyboard-setup.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630679  199956 manager.go:925] ignoring container "/system.slice/keyboard-setup.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630682  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e489181a7f95431b6d92f037dbe3f788b46a5e024df7aaf29e0115dab1168ab1-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630686  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e489181a7f95431b6d92f037dbe3f788b46a5e024df7aaf29e0115dab1168ab1-rootfs.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630692  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e489181a7f95431b6d92f037dbe3f788b46a5e024df7aaf29e0115dab1168ab1-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630695  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/apparmor.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630698  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/apparmor.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630701  199956 factory.go:255] Factory "raw" can handle container "/system.slice/apparmor.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630705  199956 manager.go:925] ignoring container "/system.slice/apparmor.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630708  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-getty.slice"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630711  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-getty.slice"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630714  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-getty.slice", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630718  199956 manager.go:925] ignoring container "/system.slice/system-getty.slice"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630720  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-1000.slice/user-runtime-dir@1000.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630723  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-1000.slice/user-runtime-dir@1000.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630727  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-1000.slice/user-runtime-dir@1000.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630731  199956 manager.go:925] ignoring container "/user.slice/user-1000.slice/user-runtime-dir@1000.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630734  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f8443d2868215ec42d3fa335663976e33df166c5e98369aa3f9d7ddb9235bead-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630738  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f8443d2868215ec42d3fa335663976e33df166c5e98369aa3f9d7ddb9235bead-rootfs.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630743  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f8443d2868215ec42d3fa335663976e33df166c5e98369aa3f9d7ddb9235bead-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630747  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-shm.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630751  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-shm.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630756  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-ff986e087aa3c3bf1159d240c008c7f6bda18768413569f5db68d0639d0c6017-shm.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630759  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/user-runtime-dir@0.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630762  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/user-runtime-dir@0.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630766  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/user-runtime-dir@0.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630770  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/user-runtime-dir@0.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630772  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-0f9ae677fe935afcf43e145281deae0d663ba95fda6759ff24f0dd3e1cee17a9-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630776  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-0f9ae677fe935afcf43e145281deae0d663ba95fda6759ff24f0dd3e1cee17a9-rootfs.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630781  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-0f9ae677fe935afcf43e145281deae0d663ba95fda6759ff24f0dd3e1cee17a9-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630785  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f78ed441ff85d669a403a76c0987f2d86913431bd96d454b1a3605bdcf0474f2-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630789  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f78ed441ff85d669a403a76c0987f2d86913431bd96d454b1a3605bdcf0474f2-rootfs.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630794  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-f78ed441ff85d669a403a76c0987f2d86913431bd96d454b1a3605bdcf0474f2-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630797  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-update-utmp.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630800  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-update-utmp.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630803  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-update-utmp.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630808  199956 manager.go:925] ignoring container "/system.slice/systemd-update-utmp.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630811  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/docker.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630814  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/docker.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630817  199956 factory.go:255] Factory "raw" can handle container "/system.slice/docker.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630821  199956 manager.go:925] ignoring container "/system.slice/docker.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630823  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-tmpfiles-setup.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630826  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-tmpfiles-setup.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630829  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-tmpfiles-setup.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630833  199956 manager.go:925] ignoring container "/system.slice/systemd-tmpfiles-setup.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630836  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/dbus.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630839  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/dbus.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630842  199956 factory.go:255] Factory "raw" can handle container "/system.slice/dbus.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630846  199956 manager.go:925] ignoring container "/system.slice/dbus.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630848  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-1000.slice"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630851  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-1000.slice"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630854  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-1000.slice", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630858  199956 manager.go:925] ignoring container "/user.slice/user-1000.slice"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630861  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-kernel-config.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630864  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-kernel-config.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630868  199956 manager.go:925] ignoring container "/system.slice/sys-kernel-config.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630871  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-04e472cb\\x2db6f9\\x2d4065\\x2db509\\x2dd7e1579fc48d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dhqj8d.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630876  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-04e472cb\\x2db6f9\\x2d4065\\x2db509\\x2dd7e1579fc48d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dhqj8d.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630881  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-04e472cb\\x2db6f9\\x2d4065\\x2db509\\x2dd7e1579fc48d-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dhqj8d.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630885  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journald-audit.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630888  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journald-audit.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630891  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journald-audit.socket", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630895  199956 manager.go:925] ignoring container "/system.slice/systemd-journald-audit.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630898  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-kernel-tracing.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630901  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-kernel-tracing.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630905  199956 manager.go:925] ignoring container "/system.slice/sys-kernel-tracing.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630908  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-tmpfiles-setup-dev.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630911  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-tmpfiles-setup-dev.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630915  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-tmpfiles-setup-dev.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630919  199956 manager.go:925] ignoring container "/system.slice/systemd-tmpfiles-setup-dev.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630922  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/whoopsie.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630925  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/whoopsie.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630928  199956 factory.go:255] Factory "raw" can handle container "/system.slice/whoopsie.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630932  199956 manager.go:925] ignoring container "/system.slice/whoopsie.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630935  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/-.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630938  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/-.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630942  199956 manager.go:925] ignoring container "/system.slice/-.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630945  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-bare-5.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630948  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-bare-5.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630952  199956 manager.go:925] ignoring container "/system.slice/snap-bare-5.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630955  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-snapd-ns-snap\\x2dstore.mnt.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630958  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-snapd-ns-snap\\x2dstore.mnt.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630962  199956 manager.go:925] ignoring container "/system.slice/run-snapd-ns-snap\\x2dstore.mnt.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630965  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-13149330f3752d0ff65bcac86c7b522b2040811e815fbc87e56b40b612875d5a-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630969  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-13149330f3752d0ff65bcac86c7b522b2040811e815fbc87e56b40b612875d5a-rootfs.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630974  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-13149330f3752d0ff65bcac86c7b522b2040811e815fbc87e56b40b612875d5a-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630977  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-resolved.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630980  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-resolved.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630984  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-resolved.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630988  199956 manager.go:925] ignoring container "/system.slice/systemd-resolved.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630990  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2d719b045b\\x2df019\\x2d025c\\x2d185d\\x2da976163f375a.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630994  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2d719b045b\\x2df019\\x2d025c\\x2d185d\\x2da976163f375a.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.630999  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2d719b045b\\x2df019\\x2d025c\\x2d185d\\x2da976163f375a.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631002  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-modprobe.slice"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631005  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-modprobe.slice"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631008  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-modprobe.slice", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631013  199956 manager.go:925] ignoring container "/system.slice/system-modprobe.slice"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631016  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-user-0.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631019  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-user-0.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631023  199956 manager.go:925] ignoring container "/system.slice/run-user-0.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631027  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b3af3b285c950aba4fcd0176473261f7f4700aeaafe9c73ae15b15a7d6304092-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631031  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b3af3b285c950aba4fcd0176473261f7f4700aeaafe9c73ae15b15a7d6304092-rootfs.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631036  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-b3af3b285c950aba4fcd0176473261f7f4700aeaafe9c73ae15b15a7d6304092-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631040  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631044  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-rootfs.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631049  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631052  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631056  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-rootfs.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631061  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631065  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/session-44.scope"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631068  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/session-44.scope"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631071  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/session-44.scope", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631075  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/session-44.scope"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631078  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/thermald.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631080  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/thermald.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631083  199956 factory.go:255] Factory "raw" can handle container "/system.slice/thermald.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631087  199956 manager.go:925] ignoring container "/system.slice/thermald.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631090  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631094  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-rootfs.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631099  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631103  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-shm.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631107  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-shm.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631117  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-shm.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631121  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/avahi-daemon.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631124  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/avahi-daemon.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631127  199956 factory.go:255] Factory "raw" can handle container "/system.slice/avahi-daemon.socket", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631131  199956 manager.go:925] ignoring container "/system.slice/avahi-daemon.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631134  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-b0713fbc\\x2defc5\\x2d4044\\x2d9d08\\x2d2326a0752f87-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dgcgm6.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631138  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-b0713fbc\\x2defc5\\x2d4044\\x2d9d08\\x2d2326a0752f87-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dgcgm6.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631143  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-b0713fbc\\x2defc5\\x2d4044\\x2d9d08\\x2d2326a0752f87-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dgcgm6.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631147  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-shm.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631151  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-shm.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631157  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-e6efb04965fd161f5770f7a34ab31f0695a628f993056e59fe3ae4db06b68f17-shm.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631161  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/switcheroo-control.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631164  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/switcheroo-control.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631167  199956 factory.go:255] Factory "raw" can handle container "/system.slice/switcheroo-control.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631171  199956 manager.go:925] ignoring container "/system.slice/switcheroo-control.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631174  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-fsckd.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631176  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-fsckd.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631179  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-fsckd.socket", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631183  199956 manager.go:925] ignoring container "/system.slice/systemd-fsckd.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631186  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-core20-1611.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631189  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-core20-1611.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631193  199956 manager.go:925] ignoring container "/system.slice/snap-core20-1611.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631196  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-logind.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631199  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-logind.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631202  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-logind.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631206  199956 manager.go:925] ignoring container "/system.slice/systemd-logind.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631208  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-shm.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631212  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-shm.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631217  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-32056e42fc780824dfd2bd91b3a0e7347dea39e94a7a5173144605838646d33c-shm.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631221  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-kernel-debug.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631225  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-kernel-debug.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631229  199956 manager.go:925] ignoring container "/system.slice/sys-kernel-debug.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631232  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-random-seed.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631235  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-random-seed.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631238  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-random-seed.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631242  199956 manager.go:925] ignoring container "/system.slice/systemd-random-seed.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631245  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/uuidd.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631248  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/uuidd.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631251  199956 factory.go:255] Factory "raw" can handle container "/system.slice/uuidd.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631255  199956 manager.go:925] ignoring container "/system.slice/uuidd.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631258  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journal-flush.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631261  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journal-flush.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631265  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journal-flush.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631269  199956 manager.go:925] ignoring container "/system.slice/systemd-journal-flush.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631272  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/apport.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631274  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/apport.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631277  199956 factory.go:255] Factory "raw" can handle container "/system.slice/apport.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631281  199956 manager.go:925] ignoring container "/system.slice/apport.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631284  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-udevd-control.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631287  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-udevd-control.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631290  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-udevd-control.socket", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631294  199956 manager.go:925] ignoring container "/system.slice/systemd-udevd-control.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631297  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-shm.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631301  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-shm.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631306  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-66323c88a73f750d34f74a6517fb4590ea34946b96c694f5b474e06d4fd278d2-shm.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631309  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/ModemManager.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631312  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/ModemManager.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631315  199956 factory.go:255] Factory "raw" can handle container "/system.slice/ModemManager.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631319  199956 manager.go:925] ignoring container "/system.slice/ModemManager.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631322  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2dd678d9bc\\x2d8fcb\\x2d9fbe\\x2d1142\\x2ddede54862bab.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631326  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2dd678d9bc\\x2d8fcb\\x2d9fbe\\x2d1142\\x2ddede54862bab.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631330  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2dd678d9bc\\x2d8fcb\\x2d9fbe\\x2d1142\\x2ddede54862bab.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631333  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/containerd.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631336  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/containerd.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631339  199956 factory.go:255] Factory "raw" can handle container "/system.slice/containerd.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631343  199956 manager.go:925] ignoring container "/system.slice/containerd.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631347  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-user-1000-gvfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631351  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-user-1000-gvfs.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631355  199956 manager.go:925] ignoring container "/system.slice/run-user-1000-gvfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631358  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-initctl.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631360  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-initctl.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631363  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-initctl.socket", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631367  199956 manager.go:925] ignoring container "/system.slice/systemd-initctl.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631370  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-udevd.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631374  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-udevd.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631377  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-udevd.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631381  199956 manager.go:925] ignoring container "/system.slice/systemd-udevd.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631384  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/cups-browsed.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631386  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/cups-browsed.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631390  199956 factory.go:255] Factory "raw" can handle container "/system.slice/cups-browsed.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631393  199956 manager.go:925] ignoring container "/system.slice/cups-browsed.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631396  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/system-lvm2\\x2dpvscan.slice"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631399  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/system-lvm2\\x2dpvscan.slice"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631402  199956 factory.go:255] Factory "raw" can handle container "/system.slice/system-lvm2\\x2dpvscan.slice", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631406  199956 manager.go:925] ignoring container "/system.slice/system-lvm2\\x2dpvscan.slice"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631409  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/session-40.scope"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631412  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/session-40.scope"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631415  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/session-40.scope", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631419  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/session-40.scope"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631422  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/rtkit-daemon.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631425  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/rtkit-daemon.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631428  199956 factory.go:255] Factory "raw" can handle container "/system.slice/rtkit-daemon.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631432  199956 manager.go:925] ignoring container "/system.slice/rtkit-daemon.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631434  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-user-1000.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631437  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-user-1000.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631441  199956 manager.go:925] ignoring container "/system.slice/run-user-1000.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631444  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-shm.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631448  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-shm.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631453  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9-shm.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631457  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/polkit.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631459  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/polkit.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631464  199956 factory.go:255] Factory "raw" can handle container "/system.slice/polkit.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631468  199956 manager.go:925] ignoring container "/system.slice/polkit.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631471  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/lvm2-lvmpolld.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631473  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/lvm2-lvmpolld.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631476  199956 factory.go:255] Factory "raw" can handle container "/system.slice/lvm2-lvmpolld.socket", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631481  199956 manager.go:925] ignoring container "/system.slice/lvm2-lvmpolld.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631484  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-shm.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631488  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-shm.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631493  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8-shm.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631497  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/kmod-static-nodes.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631500  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/kmod-static-nodes.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631503  199956 factory.go:255] Factory "raw" can handle container "/system.slice/kmod-static-nodes.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631507  199956 manager.go:925] ignoring container "/system.slice/kmod-static-nodes.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631510  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-udev-trigger.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631513  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-udev-trigger.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631516  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-udev-trigger.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631520  199956 manager.go:925] ignoring container "/system.slice/systemd-udev-trigger.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631523  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/sys-kernel-debug-tracing.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631526  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/sys-kernel-debug-tracing.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631530  199956 manager.go:925] ignoring container "/system.slice/sys-kernel-debug-tracing.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631533  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/docker.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631535  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/docker.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631538  199956 factory.go:255] Factory "raw" can handle container "/system.slice/docker.socket", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631542  199956 manager.go:925] ignoring container "/system.slice/docker.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631545  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e0c5b991f81d5128566686552e45a9bbc8c584e4f6c94909edb3a42720dacc90-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631549  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e0c5b991f81d5128566686552e45a9bbc8c584e4f6c94909edb3a42720dacc90-rootfs.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631554  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e0c5b991f81d5128566686552e45a9bbc8c584e4f6c94909edb3a42720dacc90-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631558  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-remount-fs.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631561  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-remount-fs.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631564  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-remount-fs.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631568  199956 manager.go:925] ignoring container "/system.slice/systemd-remount-fs.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631571  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snapd.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631573  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/snapd.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631576  199956 factory.go:255] Factory "raw" can handle container "/system.slice/snapd.socket", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631580  199956 manager.go:925] ignoring container "/system.slice/snapd.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631583  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-1000.slice/session-1.scope"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631586  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-1000.slice/session-1.scope"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631590  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-1000.slice/session-1.scope", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631595  199956 manager.go:925] ignoring container "/user.slice/user-1000.slice/session-1.scope"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631598  199956 factory.go:262] Factory "containerd" was unable to handle container "/init.scope"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631600  199956 factory.go:262] Factory "systemd" was unable to handle container "/init.scope"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631603  199956 factory.go:255] Factory "raw" can handle container "/init.scope", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631607  199956 manager.go:925] ignoring container "/init.scope"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631609  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-sysusers.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631612  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-sysusers.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631615  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-sysusers.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631620  199956 manager.go:925] ignoring container "/system.slice/systemd-sysusers.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631623  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snapd.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631625  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/snapd.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631628  199956 factory.go:255] Factory "raw" can handle container "/system.slice/snapd.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631632  199956 manager.go:925] ignoring container "/system.slice/snapd.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631635  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-gnome\\x2d3\\x2d38\\x2d2004-115.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631638  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-gnome\\x2d3\\x2d38\\x2d2004-115.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631643  199956 manager.go:925] ignoring container "/system.slice/snap-gnome\\x2d3\\x2d38\\x2d2004-115.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631646  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-1cb44bdbde0b41852e382d7dab7c184af8dd4e5b25d5cfe6a10a9c8ab601659d-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631650  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-1cb44bdbde0b41852e382d7dab7c184af8dd4e5b25d5cfe6a10a9c8ab601659d-rootfs.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631655  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-1cb44bdbde0b41852e382d7dab7c184af8dd4e5b25d5cfe6a10a9c8ab601659d-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631658  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-shm.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631662  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-shm.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631667  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.grpc.v1.cri-sandboxes-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-shm.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631671  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/acpid.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631673  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/acpid.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631677  199956 factory.go:255] Factory "raw" can handle container "/system.slice/acpid.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631680  199956 manager.go:925] ignoring container "/system.slice/acpid.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631683  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snapd.apparmor.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631686  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/snapd.apparmor.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631689  199956 factory.go:255] Factory "raw" can handle container "/system.slice/snapd.apparmor.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631693  199956 manager.go:925] ignoring container "/system.slice/snapd.apparmor.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631696  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/proc-sys-fs-binfmt_misc.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631700  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/proc-sys-fs-binfmt_misc.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631704  199956 manager.go:925] ignoring container "/system.slice/proc-sys-fs-binfmt_misc.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631707  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/setvtrgb.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631709  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/setvtrgb.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631712  199956 factory.go:255] Factory "raw" can handle container "/system.slice/setvtrgb.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631716  199956 manager.go:925] ignoring container "/system.slice/setvtrgb.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631719  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631723  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-rootfs.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631728  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-47a4512fb308df27df14a7bf1b422a396f52207a782af7a67e3b631e68aec806-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631732  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/user@0.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631736  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/user@0.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631739  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/user@0.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631743  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/user@0.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631746  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice/session-42.scope"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631749  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice/session-42.scope"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631752  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice/session-42.scope", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631756  199956 manager.go:925] ignoring container "/user.slice/user-0.slice/session-42.scope"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631759  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631761  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631764  199956 factory.go:255] Factory "raw" can handle container "/system.slice", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631768  199956 manager.go:925] ignoring container "/system.slice"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631771  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/irqbalance.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631774  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/irqbalance.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631777  199956 factory.go:255] Factory "raw" can handle container "/system.slice/irqbalance.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631781  199956 manager.go:925] ignoring container "/system.slice/irqbalance.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631783  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-udevd-kernel.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631786  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-udevd-kernel.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631789  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-udevd-kernel.socket", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631793  199956 manager.go:925] ignoring container "/system.slice/systemd-udevd-kernel.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631796  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/uuidd.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631798  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/uuidd.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631802  199956 factory.go:255] Factory "raw" can handle container "/system.slice/uuidd.socket", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631806  199956 manager.go:925] ignoring container "/system.slice/uuidd.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631809  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-machine-id-commit.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631812  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-machine-id-commit.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631815  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-machine-id-commit.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631819  199956 manager.go:925] ignoring container "/system.slice/systemd-machine-id-commit.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631822  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-cf25b9ea\\x2d6823\\x2d4eff\\x2db30d\\x2d36a09f9d897e-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dtqzsm.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631826  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-cf25b9ea\\x2d6823\\x2d4eff\\x2db30d\\x2d36a09f9d897e-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dtqzsm.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631832  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-cf25b9ea\\x2d6823\\x2d4eff\\x2db30d\\x2d36a09f9d897e-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2dtqzsm.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631836  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/syslog.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631839  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/syslog.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631842  199956 factory.go:255] Factory "raw" can handle container "/system.slice/syslog.socket", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631846  199956 manager.go:925] ignoring container "/system.slice/syslog.socket"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631849  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/udisks2.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631853  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/udisks2.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631856  199956 factory.go:255] Factory "raw" can handle container "/system.slice/udisks2.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631860  199956 manager.go:925] ignoring container "/system.slice/udisks2.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631863  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/kerneloops.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631866  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/kerneloops.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631869  199956 factory.go:255] Factory "raw" can handle container "/system.slice/kerneloops.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631873  199956 manager.go:925] ignoring container "/system.slice/kerneloops.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631876  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/NetworkManager-wait-online.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631878  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/NetworkManager-wait-online.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631882  199956 factory.go:255] Factory "raw" can handle container "/system.slice/NetworkManager-wait-online.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631886  199956 manager.go:925] ignoring container "/system.slice/NetworkManager-wait-online.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631889  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631893  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-rootfs.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631898  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-7aa4349196381ea2a9dc0d11308bca61f2e03add0058630deb207e1343a1db30-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631901  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-1000.slice/user@1000.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631904  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-1000.slice/user@1000.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631907  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-1000.slice/user@1000.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631911  199956 manager.go:925] ignoring container "/user.slice/user-1000.slice/user@1000.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631914  199956 factory.go:262] Factory "containerd" was unable to handle container "/kata_overhead"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631917  199956 factory.go:262] Factory "systemd" was unable to handle container "/kata_overhead"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631920  199956 factory.go:255] Factory "raw" can handle container "/kata_overhead", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631924  199956 manager.go:925] ignoring container "/kata_overhead"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631927  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/bluetooth.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631929  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/bluetooth.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631932  199956 factory.go:255] Factory "raw" can handle container "/system.slice/bluetooth.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631936  199956 manager.go:925] ignoring container "/system.slice/bluetooth.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631939  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/systemd-journald.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631942  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/systemd-journald.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631945  199956 factory.go:255] Factory "raw" can handle container "/system.slice/systemd-journald.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631949  199956 manager.go:925] ignoring container "/system.slice/systemd-journald.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631952  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e10efaa459a32161059fda75d4bcaf3bbdb896781f772b508e43fe00bc7c9003-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631956  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e10efaa459a32161059fda75d4bcaf3bbdb896781f772b508e43fe00bc7c9003-rootfs.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631961  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-e10efaa459a32161059fda75d4bcaf3bbdb896781f772b508e43fe00bc7c9003-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631964  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/openvpn.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631967  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/openvpn.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631970  199956 factory.go:255] Factory "raw" can handle container "/system.slice/openvpn.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631974  199956 manager.go:925] ignoring container "/system.slice/openvpn.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631977  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/cups.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631981  199956 factory.go:262] Factory "systemd" was unable to handle container "/system.slice/cups.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631984  199956 factory.go:255] Factory "raw" can handle container "/system.slice/cups.service", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631988  199956 manager.go:925] ignoring container "/system.slice/cups.service"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631991  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-186424b47c7b9022ecfe8996dc73cd9101f7539259cd65914279c84c9a02a288-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.631995  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-186424b47c7b9022ecfe8996dc73cd9101f7539259cd65914279c84c9a02a288-rootfs.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.632000  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-186424b47c7b9022ecfe8996dc73cd9101f7539259cd65914279c84c9a02a288-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.632003  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/snap-snapd-16292.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.632006  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/snap-snapd-16292.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.632010  199956 manager.go:925] ignoring container "/system.slice/snap-snapd-16292.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.632014  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.632017  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-rootfs.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.632022  199956 manager.go:925] ignoring container "/system.slice/run-containerd-io.containerd.runtime.v2.task-k8s.io-350218cd31fb726f58629e28733f103c74ed935f37d21a09e9e2205635c7bf6d-rootfs.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.632026  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/run-netns-cni\\x2d8b7da23a\\x2d3511\\x2dfe06\\x2d72d7\\x2d26a549eb607d.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.632031  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/run-netns-cni\\x2d8b7da23a\\x2d3511\\x2dfe06\\x2d72d7\\x2d26a549eb607d.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.632036  199956 manager.go:925] ignoring container "/system.slice/run-netns-cni\\x2d8b7da23a\\x2d3511\\x2dfe06\\x2d72d7\\x2d26a549eb607d.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.632039  199956 factory.go:262] Factory "containerd" was unable to handle container "/system.slice/var-lib-kubelet-pods-1c057a0e\\x2d3ee3\\x2d44f3\\x2db294\\x2d434acc8f9491-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d2sbqp.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.632043  199956 factory.go:255] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-1c057a0e\\x2d3ee3\\x2d44f3\\x2db294\\x2d434acc8f9491-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d2sbqp.mount", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.632049  199956 manager.go:925] ignoring container "/system.slice/var-lib-kubelet-pods-1c057a0e\\x2d3ee3\\x2d44f3\\x2db294\\x2d434acc8f9491-volumes-kubernetes.io\\x7eprojected-kube\\x2dapi\\x2daccess\\x2d2sbqp.mount"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.632053  199956 factory.go:262] Factory "containerd" was unable to handle container "/user.slice/user-0.slice"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.632055  199956 factory.go:262] Factory "systemd" was unable to handle container "/user.slice/user-0.slice"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.632058  199956 factory.go:255] Factory "raw" can handle container "/user.slice/user-0.slice", but ignoring.
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.632062  199956 manager.go:925] ignoring container "/user.slice/user-0.slice"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.650058  199956 qos_container_manager_linux.go:379] "Updated QoS cgroup configuration"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.757862  199956 kubelet.go:1280] "Container garbage collection succeeded"
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.905973  199956 reflector.go:536] object-"confidential-containers-system"/"kube-root-ca.crt": Watch close - *v1.ConfigMap total 0 items received
Jan 20 12:48:27 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:27.922157  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:48:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:28.484016  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:28.484085  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:28.492249  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[40fcee5f-dfb2-4e86-8a92-c3decebb1a80] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:28 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123eba0 2 [] false false map[] 0xc001717000 0xc000f8c370}
Jan 20 12:48:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:28.492279  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:28.879079  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:48:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:28.879150  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:28.880288  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:28 GMT] X-Content-Type-Options:[nosniff]] 0xc00123f040 2 [] true false map[] 0xc001717200 <nil>}
Jan 20 12:48:28 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:28.880405  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:48:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:29.483398  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:29.483468  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:29.491720  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[c32eb4ed-8f65-4c55-8022-655acb3eb52f] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:29 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e8000 2 [] false false map[] 0xc00105ac00 0xc001dec420}
Jan 20 12:48:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:29.491753  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:29.581423  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:48:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:29.587108  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:48:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:29.587120  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:48:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:29.587127  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:48:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:29.587879  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:48:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:29.587925  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:48:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:29.587932  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:48:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:29.587956  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:48:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:29.830472  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:48:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:29.830479  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:48:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:29.830684  199956 interface.go:209] Interface eno2 is up
Jan 20 12:48:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:29.830740  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:48:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:29.830748  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:48:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:29.830753  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:48:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:29.830756  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:48:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:29.830760  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:48:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:29.831196  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:48:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:29.831205  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:48:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:29.831209  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:48:29 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:29.831214  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:48:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:30.484167  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:30.484237  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:30.492350  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[20a0c988-08e6-499d-a391-160e9b14515b] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:30 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123fae0 2 [] false false map[] 0xc001717800 0xc0013fad10}
Jan 20 12:48:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:30.492379  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:30.693513  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:48:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:30.701264  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:48:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:30.710987  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510794" capacity="511757" time="2023-01-20 12:48:30.710668562 -0500 EST m=+1023.266443387"
Jan 20 12:48:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:30.711001  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59491868Ki" capacity="65586124Ki" time="2023-01-20 12:48:30.695699627 -0500 EST m=+1023.251474518"
Jan 20 12:48:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:30.711008  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64400912Ki" capacity="65061836Ki" time="2023-01-20 12:48:30.710927568 -0500 EST m=+1023.266702392"
Jan 20 12:48:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:30.711014  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="901992820Ki" capacity="981310056Ki" time="2023-01-20 12:48:30.695699627 -0500 EST m=+1023.251474518"
Jan 20 12:48:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:30.711020  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:48:30.695699627 -0500 EST m=+1023.251474518"
Jan 20 12:48:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:30.711026  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="901992820Ki" capacity="981310056Ki" time="2023-01-20 12:48:21.262015539 -0500 EST"
Jan 20 12:48:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:30.711031  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:48:21.262015539 -0500 EST"
Jan 20 12:48:30 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:30.711056  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:48:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:31.146495  199956 handler.go:293] error while reading "/proc/199956/fd/34" link: readlink /proc/199956/fd/34: no such file or directory
Jan 20 12:48:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:31.166503  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:48:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:31.166586  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:31.174628  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[4af4754e-425c-4a1b-933b-21547fcf1ccc] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:31 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001821b20 2 [] false false map[] 0xc001717b00 0xc0015f5340}
Jan 20 12:48:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:31.174682  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:31.483476  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:31.483544  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:31.491463  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[583108f3-f541-4013-9a5c-2972317e048f] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:31 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001821de0 2 [] false false map[] 0xc001717d00 0xc0015f54a0}
Jan 20 12:48:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:31.491493  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:31.582058  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:48:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:31.588069  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:48:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:31.588082  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:48:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:31.588555  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:48:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:31.588885  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:48:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:31.588929  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:48:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:31.588936  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:48:31 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:31.588945  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:48:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:32.483425  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:32.483492  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:32.491319  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[4ac5753e-fb28-4ba0-95f0-c945fc77ff19] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:32 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001486bc0 2 [] false false map[] 0xc001a4b700 0xc0015f5600}
Jan 20 12:48:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:32.491365  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:32.689904  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:48:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:32.689983  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:32.691067  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:32 GMT]] 0xc0013005c0 2 [] true false map[] 0xc0016d7100 <nil>}
Jan 20 12:48:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:32.691174  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:48:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:32.696397  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:48:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:32.696461  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:32.697483  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:32 GMT]] 0xc001486d40 2 [] true false map[] 0xc001a4b900 <nil>}
Jan 20 12:48:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:32.697585  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:48:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:32.709806  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:48:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:32.709873  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:32.709873  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:48:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:32.709944  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:32.710941  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:32 GMT]] 0xc001486da0 2 [] true false map[] 0xc001717f00 <nil>}
Jan 20 12:48:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:32.710979  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:32 GMT]] 0xc0020fcae0 2 [] true false map[] 0xc001a4bb00 <nil>}
Jan 20 12:48:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:32.711047  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:48:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:32.711087  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:48:32 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:32.923671  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:48:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:33.483105  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:33.483133  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:33.489401  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[4f153ed8-1d02-4561-85a7-eeaf97716b53] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:33 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ffa580 2 [] false false map[] 0xc0014ec700 0xc001b6a9a0}
Jan 20 12:48:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:33.489467  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:33.582117  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[kube-system/kube-scheduler-zcy-z390-aorus-master]
Jan 20 12:48:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:33.582202  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:48:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:33.582343  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 updateType=0
Jan 20 12:48:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:33.582423  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3
Jan 20 12:48:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:33.582458  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/kube-scheduler-zcy-z390-aorus-master"
Jan 20 12:48:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:33.582557  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" oldPhase=Running phase=Running
Jan 20 12:48:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:33.582844  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:27 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:41 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:41 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:27 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.239.159.53 PodIPs:[{IP:10.239.159.53}] StartTime:2023-01-20 12:31:27 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:kube-scheduler State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:22 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:6 Image:k8s.gcr.io/kube-scheduler:v1.24.0 ImageID:k8s.gcr.io/kube-scheduler@sha256:db842a7c431fd51db7e1911f6d1df27a7b6b6963ceda24852b654d2cd535b776 ContainerID:containerd://13149330f3752d0ff65bcac86c7b522b2040811e815fbc87e56b40b612875d5a Started:0xc002001fc9}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:48:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:33.583201  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/kube-scheduler-zcy-z390-aorus-master"
Jan 20 12:48:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:33.583269  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/kube-scheduler-zcy-z390-aorus-master"
Jan 20 12:48:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:33.583795  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:c96a30c34db72c81c93fc59c942bde753dfbff589dbbc1b9effc7f9f0b875fa8 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/kube-scheduler-zcy-z390-aorus-master"
Jan 20 12:48:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:33.583955  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 isTerminal=false
Jan 20 12:48:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:33.584012  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 updateType=0
Jan 20 12:48:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:33.588094  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:48:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:33.588106  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:48:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:33.588560  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:48:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:33.588931  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:48:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:33.588976  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:48:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:33.588984  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:48:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:33.588992  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:48:33 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:33.671466  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" volumeName="kubeconfig" volumeSpecName="kubeconfig"
Jan 20 12:48:34 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:34.483118  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:34 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:34.483179  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:34 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:34.491350  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[0ff4da3d-32ed-4e73-8601-f2c6d61161f8] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:34 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001300ec0 2 [] false false map[] 0xc0014ecc00 0xc001dbe630}
Jan 20 12:48:34 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:34.491393  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:35.484030  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:35.484096  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:35.492368  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[9c77ce0b-fd19-4914-b678-ec4281c38502] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:35 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000cb2f60 2 [] false false map[] 0xc001d18200 0xc0020ee210}
Jan 20 12:48:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:35.492400  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:35.581250  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:48:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:35.584610  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:48:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:35.584640  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:48:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:35.584655  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:48:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:35.585813  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:48:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:35.585918  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:48:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:35.585933  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:48:35 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:35.585952  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:48:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:36.164630  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:48:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:36.164718  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:36.171798  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:36 GMT] X-Content-Type-Options:[nosniff]] 0xc001300040 2 [] false false map[] 0xc001e50c00 0xc0020ee2c0}
Jan 20 12:48:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:36.171826  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:48:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:36.271902  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:48:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:36.271968  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:36.273197  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:36 GMT]] 0xc0002be5a0 29 [] true false map[] 0xc000338600 <nil>}
Jan 20 12:48:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:36.273303  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:48:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:36.483341  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:36.483368  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:36.489332  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[e6b1a993-57c4-4f82-91bb-866ef1add3a2] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:36 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00129c000 2 [] false false map[] 0xc000338a00 0xc0020ee420}
Jan 20 12:48:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:36.489400  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:36.896089  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:48:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:36.896160  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:36.903831  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:36 GMT] X-Content-Type-Options:[nosniff]] 0xc0008e27e0 2 [] false false map[] 0xc0021a2100 0xc00229a6e0}
Jan 20 12:48:36 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:36.903879  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:48:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:37.483814  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:37.483879  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:37.491680  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[8d574355-b8b6-41ce-bbad-a1b92946b0ad] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:37 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00129c9e0 2 [] false false map[] 0xc0021a2300 0xc000dadc30}
Jan 20 12:48:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:37.491710  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:37.581290  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:48:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:37.587156  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:48:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:37.587170  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:48:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:37.587691  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:48:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:37.588066  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:48:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:37.588108  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:48:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:37.588115  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:48:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:37.588123  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:48:37 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:37.925222  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:48:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:38.483459  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:38.483528  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:38.491160  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[b28c3137-5b70-4d97-aff5-a6685e31e208] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:38 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00129d460 2 [] false false map[] 0xc000fcca00 0xc0010f9e40}
Jan 20 12:48:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:38.491188  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:38.878889  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:48:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:38.878961  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:38.878988  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/healthz" timeout="1s"
Jan 20 12:48:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:38.879054  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:38.880230  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:38 GMT] X-Content-Type-Options:[nosniff]] 0xc001718200 2 [] true false map[] 0xc0014c6e00 <nil>}
Jan 20 12:48:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:38.880341  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:48:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:38.880360  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:38 GMT] X-Content-Type-Options:[nosniff]] 0xc000320ae0 2 [] true false map[] 0xc001e51600 <nil>}
Jan 20 12:48:38 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:38.880532  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:48:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:39.483163  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:39.483231  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:39.491674  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[8105883e-d239-4463-b365-c8a47c7ac489] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:39 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000320b00 2 [] false false map[] 0xc001e51900 0xc001559c30}
Jan 20 12:48:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:39.491702  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:39.582067  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:48:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:39.588046  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:48:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:39.588059  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:48:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:39.588691  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:48:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:39.589125  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:48:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:39.589169  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:48:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:39.589175  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:48:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:39.589183  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:48:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:39.962399  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:48:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:39.962407  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:48:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:39.962561  199956 interface.go:209] Interface eno2 is up
Jan 20 12:48:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:39.962605  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:48:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:39.962612  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:48:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:39.962617  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:48:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:39.962620  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:48:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:39.962624  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:48:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:39.963042  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:48:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:39.963050  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:48:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:39.963054  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:48:39 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:39.963058  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:48:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:40.483416  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:40.483487  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:40.491307  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[84437438-10f3-4206-9d2f-5b417df62203] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:40 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008e2b40 2 [] false false map[] 0xc000338e00 0xc0018dfe40}
Jan 20 12:48:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:40.491338  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:40.711108  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:48:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:40.728948  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:48:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:40.763733  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59486696Ki" capacity="65586124Ki" time="2023-01-20 12:48:40.713345286 -0500 EST m=+1033.269120177"
Jan 20 12:48:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:40.763745  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64401100Ki" capacity="65061836Ki" time="2023-01-20 12:48:40.763683779 -0500 EST m=+1033.319458602"
Jan 20 12:48:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:40.763751  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="901992784Ki" capacity="981310056Ki" time="2023-01-20 12:48:40.713345286 -0500 EST m=+1033.269120177"
Jan 20 12:48:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:40.763756  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:48:40.713345286 -0500 EST m=+1033.269120177"
Jan 20 12:48:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:40.763761  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="901992784Ki" capacity="981310056Ki" time="2023-01-20 12:48:31.262289895 -0500 EST"
Jan 20 12:48:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:40.763766  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:48:31.262289895 -0500 EST"
Jan 20 12:48:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:40.763771  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510794" capacity="511757" time="2023-01-20 12:48:40.763430984 -0500 EST m=+1033.319205807"
Jan 20 12:48:40 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:40.763795  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:48:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:41.166902  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:48:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:41.166974  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:41.175496  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[bad09ba4-2d2c-4de3-8798-64206ff9ffde] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:41 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123e8a0 2 [] false false map[] 0xc001f37c00 0xc001c52dc0}
Jan 20 12:48:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:41.175539  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:41.484107  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:41.484172  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:41.497270  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[0e084678-4de4-4806-8be6-ef6514be991a] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:41 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0017189c0 2 [] false false map[] 0xc000339100 0xc001b629a0}
Jan 20 12:48:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:41.497396  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:41.581900  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:48:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:41.585378  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:48:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:41.585408  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:48:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:41.585424  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:48:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:41.586372  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:48:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:41.586478  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:48:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:41.586494  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:48:41 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:41.586514  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:48:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:42.483769  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:42.483806  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:42.489718  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[691fc6f3-0a11-4736-8fba-aa308784efcd] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:42 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001820f00 2 [] false false map[] 0xc000339300 0xc00208b4a0}
Jan 20 12:48:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:42.489789  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:42.689507  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:48:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:42.689581  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:42.690761  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:42 GMT]] 0xc0008ddbc0 2 [] true false map[] 0xc00105be00 <nil>}
Jan 20 12:48:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:42.690879  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:48:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:42.696062  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:48:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:42.696127  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:42.697211  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:42 GMT]] 0xc0008ddc00 2 [] true false map[] 0xc001775500 <nil>}
Jan 20 12:48:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:42.697313  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:48:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:42.710531  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:48:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:42.710590  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:42.710601  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:48:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:42.710659  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:42.711675  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:42 GMT]] 0xc0009782a0 2 [] true false map[] 0xc000339600 <nil>}
Jan 20 12:48:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:42.711682  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:42 GMT]] 0xc001821580 2 [] true false map[] 0xc001775700 <nil>}
Jan 20 12:48:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:42.711776  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:48:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:42.711804  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:48:42 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:42.926619  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:48:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:43.483221  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:43.483311  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:43.487826  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[94b4893c-b85e-4324-a5ff-1a0d2592a438] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:43 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00123ed00 2 [] false false map[] 0xc001f37f00 0xc00208b6b0}
Jan 20 12:48:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:43.487858  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:43.581874  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:48:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:43.587275  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:48:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:43.587288  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:48:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:43.587951  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:48:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:43.589157  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:48:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:43.589214  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:48:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:43.589221  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:48:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:43.589230  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:48:43 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:43.954180  199956 handler.go:293] error while reading "/proc/199956/fd/34" link: readlink /proc/199956/fd/34: no such file or directory
Jan 20 12:48:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:44.483651  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:44.483687  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:44.490104  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[d34606b0-ec4f-4192-a1fc-a3e2166b78d6] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:44 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ffae00 2 [] false false map[] 0xc000338700 0xc0008ca9a0}
Jan 20 12:48:44 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:44.490181  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:45.483864  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:45.483935  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:45.492337  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[9a896b05-0d10-4704-a7f4-0bfa283654ae] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:45 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ffafa0 2 [] false false map[] 0xc000338d00 0xc0002b6b00}
Jan 20 12:48:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:45.492370  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:45.581858  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:48:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:45.587249  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:48:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:45.587262  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:48:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:45.587268  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:48:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:45.587841  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:48:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:45.587886  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:48:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:45.587893  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:48:45 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:45.587901  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:48:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:46.164853  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:48:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:46.164924  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:46.172659  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:46 GMT] X-Content-Type-Options:[nosniff]] 0xc0020fda80 2 [] false false map[] 0xc000339000 0xc001b63970}
Jan 20 12:48:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:46.172701  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:48:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:46.272581  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:48:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:46.272646  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:46.273875  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:46 GMT]] 0xc000ffb5c0 29 [] true false map[] 0xc00105ac00 <nil>}
Jan 20 12:48:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:46.273984  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:48:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:46.483412  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:46.483490  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:46.491436  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[2b1a5133-fb30-4aea-8175-20d3ea058aa5] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:46 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0020fdb40 2 [] false false map[] 0xc000339200 0xc001b63b80}
Jan 20 12:48:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:46.491491  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:46.895636  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:48:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:46.895703  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:46.902929  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:46 GMT] X-Content-Type-Options:[nosniff]] 0xc000ce96c0 2 [] false false map[] 0xc00105af00 0xc0014b3c30}
Jan 20 12:48:46 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:46.902983  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:48:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:47.483621  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:47.483691  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:47.491672  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[2ef8081f-1e41-44c5-b803-133d9b8c8fd3] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:47 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000ce9760 2 [] false false map[] 0xc001774100 0xc0015bb340}
Jan 20 12:48:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:47.491716  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:47.526245  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/etcd.yaml"
Jan 20 12:48:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:47.528420  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver.yaml"
Jan 20 12:48:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:47.531345  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-controller-manager.yaml"
Jan 20 12:48:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:47.532228  199956 file.go:201] "Reading config file" path="/etc/kubernetes/manifests/kube-scheduler.yaml"
Jan 20 12:48:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:47.532514  199956 config.go:279] "Setting pods for source" source="file"
Jan 20 12:48:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:47.582113  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:48:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:47.588137  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:48:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:47.588150  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:48:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:47.588619  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:48:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:47.588952  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:48:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:47.588996  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:48:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:47.589003  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:48:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:47.589011  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:48:47 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:47.928564  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:48:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:48.484048  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:48.484120  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:48.492361  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[3329f61a-3e0e-4439-90c1-fe82ca6d9750] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:48 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0002be780 2 [] false false map[] 0xc001774500 0xc0017e2d10}
Jan 20 12:48:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:48.492393  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:48.581464  199956 kubelet.go:2117] "SyncLoop (SYNC) pods" total=1 pods=[kube-system/coredns-6d4b75cb6d-48zl2]
Jan 20 12:48:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:48.581555  199956 pod_workers.go:888] "Processing pod event" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 updateType=0
Jan 20 12:48:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:48.581613  199956 kubelet.go:1501] "syncPod enter" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1
Jan 20 12:48:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:48.581648  199956 kubelet_pods.go:1435] "Generating pod status" pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:48:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:48.581743  199956 kubelet_pods.go:1447] "Got phase for pod" pod="kube-system/coredns-6d4b75cb6d-48zl2" oldPhase=Running phase=Running
Jan 20 12:48:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:48.582048  199956 status_manager.go:535] "Ignoring same status for pod" pod="kube-system/coredns-6d4b75cb6d-48zl2" status={Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:45 -0500 EST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:45 -0500 EST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-20 12:31:42 -0500 EST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.239.159.53 PodIP:10.244.0.2 PodIPs:[{IP:10.244.0.2}] StartTime:2023-01-20 12:31:42 -0500 EST InitContainerStatuses:[] ContainerStatuses:[{Name:coredns State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2023-01-20 12:31:43 -0500 EST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:k8s.gcr.io/coredns/coredns:v1.8.6 ImageID:k8s.gcr.io/coredns/coredns@sha256:5b6ec0d6de9baaf3e92d0f66cd96a25b9edbce8716f5f15dcd1a616b3abd590e ContainerID:containerd://b3af3b285c950aba4fcd0176473261f7f4700aeaafe9c73ae15b15a7d6304092 Started:0xc002132c39}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Jan 20 12:48:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:48.582371  199956 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:48:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:48.582439  199956 volume_manager.go:435] "All volumes are attached and mounted for pod" pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:48:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:48.582928  199956 kuberuntime_manager.go:714] "computePodActions got for pod" podActions={KillPod:false CreateSandbox:false SandboxID:b8e40fa4de3fe747240d7f81d0179281c33e6514783d9275178e621ff94e64e9 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[] EphemeralContainersToStart:[]} pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:48:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:48.583094  199956 kubelet.go:1503] "syncPod exit" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 isTerminal=false
Jan 20 12:48:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:48.583146  199956 pod_workers.go:988] "Processing pod event done" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 updateType=0
Jan 20 12:48:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:48.675179  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/coredns-6d4b75cb6d-48zl2" volumeName="config-volume" volumeSpecName="config-volume"
Jan 20 12:48:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:48.675326  199956 desired_state_of_world_populator.go:310] "Added volume to desired state" pod="kube-system/coredns-6d4b75cb6d-48zl2" volumeName="kube-api-access-9qh7j" volumeSpecName="kube-api-access-9qh7j"
Jan 20 12:48:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:48.776490  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/d94e1927-d22d-4831-a764-0170eae346a1-config-volume\") pod \"coredns-6d4b75cb6d-48zl2\" (UID: \"d94e1927-d22d-4831-a764-0170eae346a1\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:48:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:48.776639  199956 reconciler.go:234] "Starting operationExecutor.MountVolume for volume \"kube-api-access-9qh7j\" (UniqueName: \"kubernetes.io/projected/d94e1927-d22d-4831-a764-0170eae346a1-kube-api-access-9qh7j\") pod \"coredns-6d4b75cb6d-48zl2\" (UID: \"d94e1927-d22d-4831-a764-0170eae346a1\") Volume is already mounted to pod, but remount was requested." pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:48:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:48.776808  199956 projected.go:183] Setting up volume kube-api-access-9qh7j for pod d94e1927-d22d-4831-a764-0170eae346a1 at /var/lib/kubelet/pods/d94e1927-d22d-4831-a764-0170eae346a1/volumes/kubernetes.io~projected/kube-api-access-9qh7j
Jan 20 12:48:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:48.776819  199956 configmap.go:181] Setting up volume config-volume for pod d94e1927-d22d-4831-a764-0170eae346a1 at /var/lib/kubelet/pods/d94e1927-d22d-4831-a764-0170eae346a1/volumes/kubernetes.io~configmap/config-volume
Jan 20 12:48:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:48.776908  199956 configmap.go:205] Received configMap kube-system/coredns containing (1) pieces of data, 339 total bytes
Jan 20 12:48:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:48.776990  199956 quota_linux.go:271] SupportsQuotas called, but quotas disabled
Jan 20 12:48:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:48.777228  199956 atomic_writer.go:161] pod kube-system/coredns-6d4b75cb6d-48zl2 volume config-volume: no update required for target directory /var/lib/kubelet/pods/d94e1927-d22d-4831-a764-0170eae346a1/volumes/kubernetes.io~configmap/config-volume
Jan 20 12:48:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:48.777251  199956 atomic_writer.go:161] pod kube-system/coredns-6d4b75cb6d-48zl2 volume kube-api-access-9qh7j: no update required for target directory /var/lib/kubelet/pods/d94e1927-d22d-4831-a764-0170eae346a1/volumes/kubernetes.io~projected/kube-api-access-9qh7j
Jan 20 12:48:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:48.777307  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/d94e1927-d22d-4831-a764-0170eae346a1-config-volume\") pod \"coredns-6d4b75cb6d-48zl2\" (UID: \"d94e1927-d22d-4831-a764-0170eae346a1\") " pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:48:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:48.777322  199956 operation_generator.go:703] "MountVolume.SetUp succeeded for volume \"kube-api-access-9qh7j\" (UniqueName: \"kubernetes.io/projected/d94e1927-d22d-4831-a764-0170eae346a1-kube-api-access-9qh7j\") pod \"coredns-6d4b75cb6d-48zl2\" (UID: \"d94e1927-d22d-4831-a764-0170eae346a1\") " pod="kube-system/coredns-6d4b75cb6d-48zl2"
Jan 20 12:48:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:48.879640  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:48:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:48.879667  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:48.880127  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:48 GMT] X-Content-Type-Options:[nosniff]] 0xc000706340 2 [] true false map[] 0xc001774a00 <nil>}
Jan 20 12:48:48 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:48.880167  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:48:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:49.483614  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:49.483686  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:49.491675  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[eb5a9c89-5306-4d11-b3fe-696f600c7d1c] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:49 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001821a80 2 [] false false map[] 0xc001774c00 0xc001bf24d0}
Jan 20 12:48:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:49.491704  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:49.582409  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:48:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:49.588184  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:48:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:49.588197  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:48:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:49.588638  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:48:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:49.589003  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:48:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:49.589050  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:48:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:49.589056  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:48:49 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:49.589065  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:48:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:50.262897  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:48:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:50.262908  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:48:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:50.263094  199956 interface.go:209] Interface eno2 is up
Jan 20 12:48:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:50.263128  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:48:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:50.263138  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:48:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:50.263144  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:48:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:50.263149  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:48:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:50.263153  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:48:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:50.263462  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:48:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:50.263475  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:48:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:50.263481  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:48:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:50.263485  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:48:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:50.483766  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:50.483835  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:50.491668  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[c240b764-9fcb-4cdc-9661-37904e02f315] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:50 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc000707860 2 [] false false map[] 0xc001775500 0xc001edf550}
Jan 20 12:48:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:50.491736  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:50.764526  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:48:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:50.772263  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:48:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:50.781769  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="901992744Ki" capacity="981310056Ki" time="2023-01-20 12:48:41.262456071 -0500 EST"
Jan 20 12:48:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:50.781787  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:48:41.262456071 -0500 EST"
Jan 20 12:48:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:50.781795  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510791" capacity="511757" time="2023-01-20 12:48:50.781279139 -0500 EST m=+1043.337053963"
Jan 20 12:48:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:50.781803  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59473784Ki" capacity="65586124Ki" time="2023-01-20 12:48:50.766658637 -0500 EST m=+1043.322433528"
Jan 20 12:48:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:50.781813  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64400840Ki" capacity="65061836Ki" time="2023-01-20 12:48:50.781667134 -0500 EST m=+1043.337441958"
Jan 20 12:48:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:50.781821  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="901992744Ki" capacity="981310056Ki" time="2023-01-20 12:48:50.766658637 -0500 EST m=+1043.322433528"
Jan 20 12:48:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:50.781827  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:48:50.766658637 -0500 EST m=+1043.322433528"
Jan 20 12:48:50 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:50.781864  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:48:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:51.166281  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:48:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:51.166306  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:51.170283  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[7b332051-d83a-4426-bcec-61938c4fd248] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:51 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e8000 2 [] false false map[] 0xc0014c6100 0xc0013b0160}
Jan 20 12:48:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:51.170329  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:51.483739  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:51.483799  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:51.491649  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[dcf79614-498c-482d-a57f-09edf1786884] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:51 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e8060 2 [] false false map[] 0xc0014c6400 0xc0008caa50}
Jan 20 12:48:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:51.491677  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:51.582256  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:48:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:51.588141  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:48:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:51.588154  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:48:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:51.588160  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:48:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:51.588892  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:48:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:51.588984  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:48:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:51.588991  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:48:51 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:51.588998  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:48:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:52.483687  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:52.483763  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:52.491503  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[ef527e28-afb1-4275-ba8e-b31a627ce484] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:52 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e88e0 2 [] false false map[] 0xc001775800 0xc001ededc0}
Jan 20 12:48:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:52.491534  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:52.689302  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:48:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:52.689370  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:52.690525  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:52 GMT]] 0xc001719680 2 [] true false map[] 0xc001e50400 <nil>}
Jan 20 12:48:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:52.690629  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:48:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:52.695800  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:48:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:52.695862  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:52.696916  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:52 GMT]] 0xc0017196c0 2 [] true false map[] 0xc001e50600 <nil>}
Jan 20 12:48:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:52.697024  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:48:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:52.710311  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:48:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:52.710328  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:52.710336  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:48:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:52.710349  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:52.710647  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:52 GMT]] 0xc0012e90e0 2 [] true false map[] 0xc001e50800 <nil>}
Jan 20 12:48:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:52.710642  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:52 GMT]] 0xc00129c020 2 [] true false map[] 0xc001775b00 <nil>}
Jan 20 12:48:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:52.710670  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:48:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:52.710674  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:48:52 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:52.930210  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:48:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:53.483418  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:53.483450  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:53.489877  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[c9610bd6-0442-4629-b409-1473bee92d76] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:53 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00129c7a0 2 [] false false map[] 0xc001716100 0xc001051a20}
Jan 20 12:48:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:53.489956  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:53.581805  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:48:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:53.587234  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:48:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:53.587247  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:48:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:53.587749  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:48:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:53.588124  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:48:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:53.588171  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:48:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:53.588177  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:48:53 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:53.588185  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:48:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:54.483392  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:54.483465  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:54.491242  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[a9eec0f9-1589-409b-86a8-c3e91f125901] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:54 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00129d260 2 [] false false map[] 0xc000339c00 0xc00207efd0}
Jan 20 12:48:54 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:54.491273  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:55.483413  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:55.483485  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:55.491727  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[5071a8f2-331d-484c-9cef-02ae58d87f90] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:55 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00129d860 2 [] false false map[] 0xc001716500 0xc00141dce0}
Jan 20 12:48:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:55.491783  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:55.581838  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:48:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:55.587276  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:48:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:55.587289  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:48:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:55.587932  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:48:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:55.588470  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:48:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:55.588518  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:48:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:55.588525  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:48:55 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:55.588534  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:48:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:56.164601  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10259 path="/healthz" timeout="15s"
Jan 20 12:48:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:56.164625  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:56.167612  199956 http.go:134] Probe succeeded for https://127.0.0.1:10259/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:56 GMT] X-Content-Type-Options:[nosniff]] 0xc00129dfe0 2 [] false false map[] 0xc001775e00 0xc0017b9c30}
Jan 20 12:48:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:56.167655  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-scheduler-zcy-z390-aorus-master" podUID=85104da468beff6c82d5dd9e33408bb3 containerName="kube-scheduler"
Jan 20 12:48:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:56.271581  199956 prober.go:178] "HTTP-Probe" scheme="http" host="127.0.0.1" port=2381 path="/health" timeout="15s"
Jan 20 12:48:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:56.271654  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:56.272954  199956 http.go:134] Probe succeeded for http://127.0.0.1:2381/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[29] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:56 GMT]] 0xc000ffa0c0 29 [] true false map[] 0xc001846000 <nil>}
Jan 20 12:48:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:56.273065  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/etcd-zcy-z390-aorus-master" podUID=238c3ede68aedd86877a0bea7d596a6d containerName="etcd"
Jan 20 12:48:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:56.345320  199956 handler.go:293] error while reading "/proc/199956/fd/34" link: readlink /proc/199956/fd/34: no such file or directory
Jan 20 12:48:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:56.483655  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:56.483724  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:56.491558  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[8e27733c-7321-4395-bd25-c32b07d46b24] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:56 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0002bf020 2 [] false false map[] 0xc001716b00 0xc001763ad0}
Jan 20 12:48:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:56.491587  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:56.895019  199956 prober.go:178] "HTTP-Probe" scheme="https" host="127.0.0.1" port=10257 path="/healthz" timeout="15s"
Jan 20 12:48:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:56.895097  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:56.902603  199956 http.go:134] Probe succeeded for https://127.0.0.1:10257/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:56 GMT] X-Content-Type-Options:[nosniff]] 0xc0012e9560 2 [] false false map[] 0xc0021a2800 0xc001a5b600}
Jan 20 12:48:56 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:56.902632  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-controller-manager-zcy-z390-aorus-master" podUID=bb966e6bae3e8a20cd2e19cf8eeeb8ce containerName="kube-controller-manager"
Jan 20 12:48:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:57.483405  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:57.483491  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:57.497737  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[4defff97-5b1c-4553-ace7-12cc7e3226f8] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:57 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001486620 2 [] false false map[] 0xc001846400 0xc0019eae70}
Jan 20 12:48:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:57.497875  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:57.581942  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:48:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:57.588116  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:48:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:57.588129  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:48:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:57.588716  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:48:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:57.589165  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:48:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:57.589209  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:48:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:57.589216  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:48:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:57.589225  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:48:57 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:57.931507  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
Jan 20 12:48:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:58.483500  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:58.483575  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:58.491320  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[b91a6c2d-5f79-49e9-a2b2-a97a15eb6565] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:58 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0012e9f20 2 [] false false map[] 0xc00105b000 0xc001cdeb00}
Jan 20 12:48:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:58.491351  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:58.878954  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/healthz" timeout="1s"
Jan 20 12:48:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:58.878987  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.4" port=8081 path="/readyz" timeout="1s"
Jan 20 12:48:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:58.879027  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:58.879074  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:58.880116  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:58 GMT] X-Content-Type-Options:[nosniff]] 0xc000ffbca0 2 [] true false map[] 0xc001e50d00 <nil>}
Jan 20 12:48:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:58.880175  199956 http.go:134] Probe succeeded for http://10.244.0.4:8081/readyz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:58 GMT] X-Content-Type-Options:[nosniff]] 0xc001301e60 2 [] true false map[] 0xc00105b300 <nil>}
Jan 20 12:48:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:58.880234  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:48:58 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:58.880280  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="confidential-containers-system/cc-operator-controller-manager-79797456f6-m6znr" podUID=d2688d45-2487-46e7-aecb-e3479626909d containerName="manager"
Jan 20 12:48:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:59.483882  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:48:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:59.483910  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:48:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:59.489775  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[bfb4a4eb-df7c-439a-a869-e256a9ac64d9] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:48:59 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00013cba0 2 [] false false map[] 0xc001717400 0xc001e056b0}
Jan 20 12:48:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:59.489843  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:48:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:59.581424  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:48:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:59.587183  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:48:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:59.587195  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:48:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:59.587202  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:48:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:59.587784  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:48:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:59.587830  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:48:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:59.587837  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:48:59 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:48:59.587845  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:49:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:00.315665  199956 interface.go:432] Looking for default routes with IPv4 addresses
Jan 20 12:49:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:00.315702  199956 interface.go:437] Default route transits interface "eno2"
Jan 20 12:49:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:00.316273  199956 interface.go:209] Interface eno2 is up
Jan 20 12:49:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:00.316397  199956 interface.go:257] Interface "eno2" has 2 addresses :[10.239.159.53/24 fe80::a639:3505:7aba:6753/64].
Jan 20 12:49:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:00.316431  199956 interface.go:224] Checking addr  10.239.159.53/24.
Jan 20 12:49:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:00.316457  199956 interface.go:231] IP found 10.239.159.53
Jan 20 12:49:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:00.316473  199956 interface.go:263] Found valid IPv4 address 10.239.159.53 for interface "eno2".
Jan 20 12:49:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:00.316492  199956 interface.go:443] Found active IP 10.239.159.53
Jan 20 12:49:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:00.317547  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/aws-ebs"
Jan 20 12:49:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:00.317584  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/gce-pd"
Jan 20 12:49:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:00.317606  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/cinder"
Jan 20 12:49:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:00.317627  199956 setters.go:750] "Skipping volume limits for volume plugin" plugin="kubernetes.io/azure-disk"
Jan 20 12:49:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:00.483841  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:49:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:00.483911  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:49:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:00.496769  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[4685f6fa-ba24-4e3f-b288-f4714857de1f] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:49:00 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc0008e20c0 2 [] false false map[] 0xc001e51500 0xc002346e70}
Jan 20 12:49:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:00.496914  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:49:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:00.782082  199956 eviction_manager.go:237] "Eviction manager: synchronize housekeeping"
Jan 20 12:49:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:00.790980  199956 fs.go:419] unable to determine file system type, partition mountpoint does not exist: /var/lib/docker/overlay2/984ac9c767ea943f8b13b37f8b452fbc5ba8c3b85650c338d80a3214e3ab34aa/merged
Jan 20 12:49:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:00.800928  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:48:51.26194107 -0500 EST"
Jan 20 12:49:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:00.800941  199956 helpers.go:775] "Eviction manager:" log="observations" signal=pid.available resourceName="510794" capacity="511757" time="2023-01-20 12:49:00.800596791 -0500 EST m=+1053.356371615"
Jan 20 12:49:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:00.800948  199956 helpers.go:775] "Eviction manager:" log="observations" signal=memory.available resourceName="59465244Ki" capacity="65586124Ki" time="2023-01-20 12:49:00.784280711 -0500 EST m=+1053.340055603"
Jan 20 12:49:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:00.800954  199956 helpers.go:775] "Eviction manager:" log="observations" signal=allocatableMemory.available resourceName="64400660Ki" capacity="65061836Ki" time="2023-01-20 12:49:00.800872805 -0500 EST m=+1053.356647629"
Jan 20 12:49:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:00.800961  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.available resourceName="901992712Ki" capacity="981310056Ki" time="2023-01-20 12:49:00.784280711 -0500 EST m=+1053.340055603"
Jan 20 12:49:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:00.800967  199956 helpers.go:775] "Eviction manager:" log="observations" signal=nodefs.inodesFree resourceName="61945644" capacity="62382080" time="2023-01-20 12:49:00.784280711 -0500 EST m=+1053.340055603"
Jan 20 12:49:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:00.800972  199956 helpers.go:775] "Eviction manager:" log="observations" signal=imagefs.available resourceName="901992712Ki" capacity="981310056Ki" time="2023-01-20 12:48:51.26194107 -0500 EST"
Jan 20 12:49:00 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:00.800998  199956 eviction_manager.go:328] "Eviction manager: no resources are starved"
Jan 20 12:49:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:01.166252  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/livez" timeout="15s"
Jan 20 12:49:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:01.166317  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:49:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:01.174542  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/livez, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[9c7c023c-4fbe-4952-8470-719de1a3d987] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:49:01 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00013ca80 2 [] false false map[] 0xc001e51c00 0xc000fd8fd0}
Jan 20 12:49:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:01.174583  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:49:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:01.483952  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:49:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:01.484018  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:49:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:01.492425  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[683e40e4-a0b7-423b-a8ea-2fe477a82580] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:49:01 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc00013cb60 2 [] false false map[] 0xc001f36100 0xc001983d90}
Jan 20 12:49:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:01.492457  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:49:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:01.581218  199956 kubelet.go:2148] "SyncLoop (housekeeping)"
Jan 20 12:49:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:01.587147  199956 kubelet_pods.go:1079] "Clean up pod workers for terminated pods"
Jan 20 12:49:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:01.587160  199956 kubelet_pods.go:1108] "Clean up probes for terminating and terminated pods"
Jan 20 12:49:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:01.587773  199956 kubelet_pods.go:1145] "Clean up orphaned pod statuses"
Jan 20 12:49:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:01.588306  199956 kubelet_pods.go:1164] "Clean up orphaned pod directories"
Jan 20 12:49:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:01.588352  199956 kubelet_pods.go:1175] "Clean up orphaned mirror pods"
Jan 20 12:49:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:01.588358  199956 kubelet_pods.go:1182] "Clean up orphaned pod cgroups"
Jan 20 12:49:01 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:01.588366  199956 kubelet.go:2156] "SyncLoop (housekeeping) end"
Jan 20 12:49:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:02.484153  199956 prober.go:178] "HTTP-Probe" scheme="https" host="10.239.159.53" port=6443 path="/readyz" timeout="15s"
Jan 20 12:49:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:02.484222  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:49:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:02.492374  199956 http.go:134] Probe succeeded for https://10.239.159.53:6443/readyz, Response: {200 OK 200 HTTP/2.0 2 0 map[Audit-Id:[3516b15a-3277-4205-ab06-51de79862244] Cache-Control:[no-cache, private] Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:49:02 GMT] X-Content-Type-Options:[nosniff] X-Kubernetes-Pf-Flowschema-Uid:[d4490f90-faa6-4794-8538-1fa046dc1f83] X-Kubernetes-Pf-Prioritylevel-Uid:[1d478448-9340-4d83-92ec-ba80835ca624]] 0xc001301da0 2 [] false false map[] 0xc001716500 0xc00112f4a0}
Jan 20 12:49:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:02.492404  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/kube-apiserver-zcy-z390-aorus-master" podUID=a522c3124739125cc5fd78c92c51c5fd containerName="kube-apiserver"
Jan 20 12:49:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:02.689951  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8080 path="/health" timeout="5s"
Jan 20 12:49:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:02.690014  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:49:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:02.691089  199956 http.go:134] Probe succeeded for http://10.244.0.2:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:49:02 GMT]] 0xc0008dd2c0 2 [] true false map[] 0xc00105b300 <nil>}
Jan 20 12:49:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:02.691192  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:49:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:02.696403  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.2" port=8181 path="/ready" timeout="1s"
Jan 20 12:49:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:02.696467  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:49:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:02.697420  199956 http.go:134] Probe succeeded for http://10.244.0.2:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:49:02 GMT]] 0xc0008dd300 2 [] true false map[] 0xc0011c4900 <nil>}
Jan 20 12:49:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:02.697522  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-48zl2" podUID=d94e1927-d22d-4831-a764-0170eae346a1 containerName="coredns"
Jan 20 12:49:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:02.709628  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8181 path="/ready" timeout="1s"
Jan 20 12:49:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:02.709661  199956 prober.go:178] "HTTP-Probe" scheme="http" host="10.244.0.3" port=8080 path="/health" timeout="5s"
Jan 20 12:49:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:02.709704  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:49:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:02.709724  199956 prober.go:181] "HTTP-Probe Headers" headers=map[]
Jan 20 12:49:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:02.710720  199956 http.go:134] Probe succeeded for http://10.244.0.3:8181/ready, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:49:02 GMT]] 0xc0020fc020 2 [] true false map[] 0xc00105b500 <nil>}
Jan 20 12:49:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:02.710843  199956 prober.go:130] "Probe succeeded" probeType="Readiness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:49:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:02.710899  199956 http.go:134] Probe succeeded for http://10.244.0.3:8080/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Fri, 20 Jan 2023 17:49:02 GMT]] 0xc0009780c0 2 [] true false map[] 0xc0011c5c00 <nil>}
Jan 20 12:49:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:02.710955  199956 prober.go:130] "Probe succeeded" probeType="Liveness" pod="kube-system/coredns-6d4b75cb6d-zdl2m" podUID=cf25b9ea-6823-4eff-b30d-36a09f9d897e containerName="coredns"
Jan 20 12:49:02 zcy-Z390-AORUS-MASTER kubelet[199956]: I0120 12:49:02.932989  199956 kubelet.go:2341] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:"
